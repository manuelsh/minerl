{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1\n",
    "* samples from human dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Rainbow\n",
    "\n",
    "[M. Hessel et al., \"Rainbow: Combining Improvements in Deep Reinforcement Learning.\" arXiv preprint arXiv:1710.02298, 2017.](https://arxiv.org/pdf/1710.02298.pdf)\n",
    "\n",
    "We will integrate all the following seven components into a single integrated agent, which is called Rainbow!\n",
    "\n",
    "1. DQN\n",
    "2. Double DQN\n",
    "3. Prioritized Experience Replay\n",
    "4. Dueling Network\n",
    "5. Noisy Network\n",
    "6. Categorical DQN\n",
    "7. N-step Learning\n",
    "\n",
    "This method shows an impressive performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. \n",
    "\n",
    "![rainbow](https://user-images.githubusercontent.com/14961526/60591412-61748100-9dd9-11e9-84fb-076c7a61fbab.png)\n",
    "\n",
    "However, the integration is not so simple because some of components are not independent each other, so we will look into a number of points that people especailly feel confused.\n",
    "\n",
    "1. Noisy Network <-> Dueling Network\n",
    "2. Dueling Network <-> Categorical DQN\n",
    "3. Categorical DQN <-> Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import deque, OrderedDict\n",
    "from typing import Deque, Dict, List, Tuple\n",
    "\n",
    "import minerl\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from segment_tree import MinSegmentTree, SumSegmentTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'MineRLNavigateDense-v0'\n",
    "SEED = 777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 40.2 ms, total: 40.2 ms\n",
      "Wall time: 108 ms\n"
     ]
    }
   ],
   "source": [
    "%time human_data = minerl.data.make(ENV_NAME, data_dir='/app/code/minerl-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_human = human_data.sarsd_iter(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('attack', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       "             ('back', array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])),\n",
       "             ('camera', array([[ -5.9995675 , -12.92215   ],\n",
       "                     [-15.22967   , -18.921715  ],\n",
       "                     [ -6.614908  ,  -1.538353  ],\n",
       "                     [ -7.8455887 ,   1.8460236 ],\n",
       "                     [ -7.076412  ,   2.6151962 ],\n",
       "                     [ -5.076557  ,   0.9230118 ],\n",
       "                     [ -0.9230118 ,  -0.        ],\n",
       "                     [ -1.6921844 ,  -0.30766296],\n",
       "                     [ -2.3075256 ,  -1.2306747 ],\n",
       "                     [  0.        ,   0.        ]], dtype=float32)),\n",
       "             ('forward', array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       "             ('jump', array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])),\n",
       "             ('left', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       "             ('place', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       "             ('right', array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])),\n",
       "             ('sneak', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
       "             ('sprint', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Same as the basic N-step buffer. \n",
    "\n",
    "(Please see *01.dqn.ipynb*, *07.n_step_learning.ipynb* for detailed description about the basic (n-step) replay buffer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim: int, \n",
    "        size: int, \n",
    "        batch_size: int = 32, \n",
    "        n_step: int = 1, \n",
    "        gamma: float = 0.99\n",
    "    ):\n",
    "        \n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "        \n",
    "        # for N-step Learning\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def store(\n",
    "        self, \n",
    "        obs: np.ndarray, \n",
    "        act: np.ndarray, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
    "        \n",
    "        transition = (obs, act, rew, next_obs, done)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "        # single step transition is not ready\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return ()\n",
    "        \n",
    "        # make a n-step transition\n",
    "        rew, next_obs, done = self._get_n_step_info(\n",
    "            self.n_step_buffer, self.gamma\n",
    "        )\n",
    "        obs, act = self.n_step_buffer[0][:2]\n",
    "        \n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "        return self.n_step_buffer[0]\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "            # for N-step Learning\n",
    "            indices=indices,\n",
    "        )\n",
    "    \n",
    "    def sample_batch_from_idxs(\n",
    "        self, idxs: np.ndarray\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        # for N-step Learning\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "        )\n",
    "    \n",
    "    def _get_n_step_info(\n",
    "        self, n_step_buffer: Deque, gamma: float\n",
    "    ) -> Tuple[np.int64, np.ndarray, bool]:\n",
    "        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n",
    "        # info of the last transition\n",
    "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
    "\n",
    "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
    "            r, n_o, d = transition[-3:]\n",
    "\n",
    "            rew = r + gamma * rew * (1 - d)\n",
    "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
    "\n",
    "        return rew, next_obs, done\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized replay Buffer\n",
    "\n",
    "`store` method returns boolean in order to inform if a N-step transition has been generated.\n",
    "\n",
    "(Please see *02.per.ipynb* for detailed description about PER.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    \"\"\"Prioritized Replay buffer.\n",
    "    \n",
    "    Attributes:\n",
    "        max_priority (float): max priority\n",
    "        tree_ptr (int): next index of tree\n",
    "        alpha (float): alpha parameter for prioritized replay buffer\n",
    "        sum_tree (SumSegmentTree): sum tree for prior\n",
    "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim: int, \n",
    "        size: int, \n",
    "        batch_size: int = 32, \n",
    "        alpha: float = 0.6,\n",
    "        n_step: int = 1, \n",
    "        gamma: float = 0.99,\n",
    "        human_data = human_data,\n",
    "        sample_human_prob = 1. # probability of sampling from human data, instead of from history\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        assert alpha >= 0\n",
    "        \n",
    "        super(PrioritizedReplayBuffer, self).__init__(\n",
    "            obs_dim, size, batch_size, n_step, gamma\n",
    "        )\n",
    "        self.max_priority, self.tree_ptr = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "        self.human_data_iter = human_data.sarsd_iter(num_epochs=-1, \n",
    "                                               max_sequence_len=batch_size)\n",
    "        self.sample_human_prob = sample_human_prob\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def store(\n",
    "        self, \n",
    "        obs: np.ndarray, \n",
    "        act: int, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
    "        \"\"\"Store experience and priority.\"\"\"\n",
    "        transition = super().store(obs, act, rew, next_obs, done)\n",
    "        \n",
    "        if transition:\n",
    "            self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "            self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
    "        \n",
    "        return transition\n",
    "\n",
    "    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n",
    "        if np.random.rand()<self.sample_human_prob:\n",
    "            return self.sample_batch_hum()\n",
    "        else:\n",
    "            return self.sample_batch_hist(beta)\n",
    "    \n",
    "    def sample_batch_hum(self) -> Dict[str, np.ndarray]:    \n",
    "        \"\"\"Sample a batch of experiences from human replay.\"\"\"\n",
    "        obs, acts, rews, next_obs, done = next(self.human_data_iter)\n",
    "        return dict(\n",
    "            obs=obs,\n",
    "            next_obs=next_obs,\n",
    "            acts=acts,\n",
    "            rews=rews,\n",
    "            done=done,\n",
    "            indices=None,\n",
    "            weights = np.array([1/self.batch_size]*self.batch_size)          \n",
    "        )\n",
    "        \n",
    "    def sample_batch_hist(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Sample a batch of experiences from historic memory.\"\"\"\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "        \n",
    "        indices = self._sample_proportional()\n",
    "        \n",
    "        obs = self.obs_buf[indices]\n",
    "        next_obs = self.next_obs_buf[indices]\n",
    "        acts = self.acts_buf[indices]\n",
    "        rews = self.rews_buf[indices]\n",
    "        done = self.done_buf[indices]\n",
    "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "        \n",
    "        return dict(\n",
    "            obs=obs,\n",
    "            next_obs=next_obs,\n",
    "            acts=acts,\n",
    "            rews=rews,\n",
    "            done=done,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "        \n",
    "    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n",
    "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority ** self.alpha\n",
    "            self.min_tree[idx] = priority ** self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "            \n",
    "    def _sample_proportional(self) -> List[int]:\n",
    "        \"\"\"Sample indices based on proportions.\"\"\"\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
    "        segment = p_total / self.batch_size\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def _calculate_weight(self, idx: int, beta: float):\n",
    "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "        \n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Layer\n",
    "\n",
    "Please see *05.noisy_net.ipynb* for detailed description.\n",
    "\n",
    "**References:**\n",
    "\n",
    "- https://github.com/higgsfield/RL-Adventure/blob/master/5.noisy%20dqn.ipynb\n",
    "- https://github.com/Kaixhin/Rainbow/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy linear module for NoisyNet.\n",
    "           \n",
    "    Attributes:\n",
    "        in_features (int): input size of linear module\n",
    "        out_features (int): output size of linear module\n",
    "        std_init (float): initial std value\n",
    "        weight_mu (nn.Parameter): mean value weight parameter\n",
    "        weight_sigma (nn.Parameter): std value weight parameter\n",
    "        bias_mu (nn.Parameter): mean value bias parameter\n",
    "        bias_sigma (nn.Parameter): std value bias parameter\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        std_init: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(\n",
    "            self.std_init / math.sqrt(self.in_features)\n",
    "        )\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(\n",
    "            self.std_init / math.sqrt(self.out_features)\n",
    "        )\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Make new noise.\"\"\"\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "\n",
    "        # outer product\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\n",
    "        \n",
    "        We don't use separate statements on train / eval mode.\n",
    "        It doesn't show remarkable difference of performance.\n",
    "        \"\"\"\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
    "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def scale_noise(size: int) -> torch.Tensor:\n",
    "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
    "        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=1.0, size=size))\n",
    "\n",
    "        return x.sign().mul(x.abs().sqrt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NoisyNet + DuelingNet + Categorical DQN\n",
    "\n",
    "#### NoisyNet + DuelingNet\n",
    "\n",
    "NoisyLinear is employed for the last two layers of advantage and value layers. The noise should be reset at evey update step.\n",
    "\n",
    "#### DuelingNet + Categorical DQN\n",
    "\n",
    "The dueling network architecture is adapted for use with return distributions. The network has a shared representation, which is then fed into a value stream with atom_size outputs, and into an advantage stream with atom_size × out_dim outputs. For each atom, the value and advantage streams are aggregated, as in dueling DQN, and then passed through a softmax layer to obtain the normalized parametric distributions used to estimate the returns’ distributions.\n",
    "\n",
    "```\n",
    "        advantage = self.advantage_layer(adv_hid).view(-1, self.out_dim, self.atom_size)\n",
    "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
    "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        dist = F.softmax(q_atoms, dim=-1)\n",
    "```\n",
    "\n",
    "(Please see *04.dueling.ipynb*, *05.noisy_net.ipynb*, *06.categorical_dqn.ipynb* for detailed description of each component's network architecture.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        out_dim: int, \n",
    "        atom_size: int,\n",
    "        non_visual_state_dim: int,\n",
    "        support: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.support = support\n",
    "        self.out_dim = out_dim\n",
    "        self.atom_size = atom_size\n",
    "\n",
    "        # set common feature layer\n",
    "        self.feature_layer_1 = nn.Sequential(\n",
    "#             nn.Linear(in_dim, 128), \n",
    "#             nn.ReLU(),\n",
    "              nn.Conv2d(3,32, kernel_size=8, stride=4),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(32,64, kernel_size=4, stride=2),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(64,64, kernel_size=3, stride=1),\n",
    "              nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.feature_layer_2 = nn.Sequential( \n",
    "              nn.Linear(1024+non_visual_state_dim, 512),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(512, 128),\n",
    "              nn.ReLU()\n",
    "        )\n",
    "\n",
    "        \n",
    "        # set advantage layer\n",
    "        self.advantage_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.advantage_layer = NoisyLinear(128, out_dim * atom_size)\n",
    "\n",
    "        # set value layer\n",
    "        self.value_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.value_layer = NoisyLinear(128, atom_size)\n",
    "\n",
    "    def forward(self, x_visual: torch.Tensor, x_not_visual: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        dist = self.dist(x_visual, x_not_visual)\n",
    "        q = torch.sum(dist * self.support, dim=2)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def dist(self,  x_visual: torch.Tensor, x_not_visual: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get distribution for atoms.\"\"\"\n",
    "        r = self.feature_layer_1(x_visual.permute(0,3,1,2))\n",
    "        feature = self.feature_layer_2(torch.cat([ r.view(r.shape[0], -1),  x_not_visual.view(-1,1) ], dim=1 ))\n",
    "        adv_hid = F.relu(self.advantage_hidden_layer(feature))\n",
    "        val_hid = F.relu(self.value_hidden_layer(feature))\n",
    "        \n",
    "        advantage = self.advantage_layer(adv_hid).view(\n",
    "            -1, self.out_dim, self.atom_size\n",
    "        )\n",
    "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
    "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        dist = F.softmax(q_atoms, dim=-1)\n",
    "        \n",
    "        return dist\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset all noisy layers.\"\"\"\n",
    "        self.advantage_hidden_layer.reset_noise()\n",
    "        self.advantage_layer.reset_noise()\n",
    "        self.value_hidden_layer.reset_noise()\n",
    "        self.value_layer.reset_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow Agent\n",
    "\n",
    "Here is a summary of DQNAgent class.\n",
    "\n",
    "| Method           | Note                                                 |\n",
    "| ---              | ---                                                  |\n",
    "|select_action     | select an action from the input state.               |\n",
    "|step              | take an action and return the response of the env.   |\n",
    "|compute_dqn_loss  | return dqn loss.                                     |\n",
    "|update_model      | update the model by gradient descent.                |\n",
    "|target_hard_update| hard update from the local model to the target model.|\n",
    "|train             | train the agent during num_frames.                   |\n",
    "|test              | test the agent (1 episode).                          |\n",
    "|plot              | plot the training progresses.                        |\n",
    "\n",
    "#### Categorical DQN + Double DQN\n",
    "\n",
    "The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Here, we use `self.dqn` instead of `self.dqn_target` to obtain the target actions.\n",
    "\n",
    "```\n",
    "        # Categorical DQN + Double DQN\n",
    "        # target_dqn is used when we don't employ double DQN\n",
    "        next_action = self.dqn(next_state).argmax(1)\n",
    "        next_dist = self.dqn_target.dist(next_state)\n",
    "        next_dist = next_dist[range(self.batch_size), next_action]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "    \n",
    "    Attribute:\n",
    "        env (gym.Env): openAI Gym environment\n",
    "        memory (PrioritizedReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        target_update (int): period for target model's hard update\n",
    "        gamma (float): discount factor\n",
    "        dqn (Network): model to train and select actions\n",
    "        dqn_target (Network): target model to update\n",
    "        optimizer (torch.optim): optimizer for training dqn\n",
    "        transition (list): transition information including \n",
    "                           state, action, reward, next_state, done\n",
    "        v_min (float): min value of support\n",
    "        v_max (float): max value of support\n",
    "        atom_size (int): the unit number of support\n",
    "        support (torch.Tensor): support for categorical dqn\n",
    "        use_n_step (bool): whether to use n_step memory\n",
    "        n_step (int): step number to calculate n-step td error\n",
    "        memory_n (ReplayBuffer): n-step replay buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        obs_dim: int,\n",
    "        non_visual_state_dim: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "        # PER parameters\n",
    "        alpha: float = 0.6,\n",
    "        beta: float = 0.4,\n",
    "        prior_eps: float = 1e-6,\n",
    "        # Categorical DQN parameters\n",
    "        v_min: float = -30.0,\n",
    "        v_max: float = 180.0,\n",
    "        atom_size: int = 51,\n",
    "        # N-step Learning\n",
    "        n_step: int = 3,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "        \n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            target_update (int): period for target model's hard update\n",
    "            lr (float): learning rate\n",
    "            gamma (float): discount factor\n",
    "            alpha (float): determines how much prioritization is used\n",
    "            beta (float): determines how much importance sampling is used\n",
    "            prior_eps (float): guarantees every transition can be sampled\n",
    "            v_min (float): min value of support\n",
    "            v_max (float): max value of support\n",
    "            atom_size (int): the unit number of support\n",
    "            n_step (int): step number to calculate n-step td error\n",
    "        \"\"\"\n",
    "        obs_dim = obs_dim\n",
    "        # action_dim = env.action_space.n\n",
    "        \n",
    "        actions_dict = OrderedDict({#'attack', \n",
    "           #'back', \n",
    "           'camera_left' : {'camera': [ 0, -20]},\n",
    "           'camera_right' : {'camera': [ 0, 20]},\n",
    "           'camera_left_little' : {'camera': [ 0, -5]}, \n",
    "           'camera_right_little' : {'camera': [ 0, 5]},\n",
    "           'forward': {'forward': 1}, \n",
    "           #'jump', \n",
    "           'jump+forward' : {'forward': 1, 'jump':1},\n",
    "           'left' : {'left':1},\n",
    "           #'place',\n",
    "           'right' : {'right':1}, \n",
    "           'sneak+forward' : {'sneak': 1, 'jump':1},\n",
    "           'wait': {}})\n",
    "           #'sprint'\n",
    "            \n",
    "        self.actions_items = list(actions_dict.items())\n",
    "        action_dim = len(actions_dict) # we interpret camera yaw as discrete\n",
    "        \n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.gamma = gamma\n",
    "        # NoisyNet: All attributes related to epsilon are removed\n",
    "        \n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "        \n",
    "        # PER\n",
    "        # memory for 1-step Learning\n",
    "        self.beta = beta\n",
    "        self.prior_eps = prior_eps\n",
    "        self.memory = PrioritizedReplayBuffer(\n",
    "            obs_dim, memory_size, batch_size, alpha=alpha\n",
    "        )\n",
    "        \n",
    "        # memory for N-step Learning\n",
    "        self.use_n_step = True if n_step > 1 else False\n",
    "        if self.use_n_step:\n",
    "            self.n_step = n_step\n",
    "            self.memory_n = ReplayBuffer(\n",
    "                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma\n",
    "            )\n",
    "            \n",
    "        # Categorical DQN parameters\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.atom_size = atom_size\n",
    "        self.support = torch.linspace(\n",
    "            self.v_min, self.v_max, self.atom_size\n",
    "        ).to(self.device)\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network(\n",
    "            action_dim, self.atom_size,non_visual_state_dim, self.support\n",
    "        ).to(self.device)\n",
    "        self.dqn_target = Network(\n",
    "            action_dim, self.atom_size,non_visual_state_dim, self.support\n",
    "        ).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "        \n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "        self.scores = None\n",
    "        \n",
    "\n",
    "    def flatten_state(self, state):\n",
    "        return np.append(state['pov'].reshape(-1), state['compassAngle'])\n",
    "\n",
    "    def unflatten_state(self, flat_state):\n",
    "        return (flat_state[:,:-1].reshape(-1,64,64,3), flat_state[:,-1])\n",
    "    \n",
    "    def convert_action(self, action_number):\n",
    "        return self.actions_items[action_number][1]\n",
    "        \n",
    "    def select_action(self, raw_state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # NoisyNet: no epsilon greedy action selection\n",
    "        selected_action = self.dqn(\n",
    "            self.float_tensor( [raw_state['pov'].astype(float)] ),\n",
    "            self.float_tensor( [raw_state['compassAngle']] )\n",
    "        ).argmax()\n",
    "        selected_action = selected_action.detach().cpu().numpy()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition = [ self.flatten_state(raw_state), selected_action ]\n",
    "        \n",
    "        return selected_action\n",
    "    \n",
    "\n",
    "    \n",
    "    def step(self, action_number: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        action_dict = self.convert_action(action_number)\n",
    "        next_state, reward, done, _ = self.env.step(action_dict)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, self.flatten_state(next_state), done]\n",
    "            \n",
    "            # N-step transition\n",
    "            if self.use_n_step:\n",
    "                one_step_transition = self.memory_n.store(*self.transition)\n",
    "            # 1-step transition\n",
    "            else:\n",
    "                one_step_transition = self.transition\n",
    "\n",
    "            # add a single step transition\n",
    "            if one_step_transition:\n",
    "                self.memory.store(*one_step_transition)\n",
    "    \n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        # PER needs beta to calculate weights\n",
    "        samples = self.memory.sample_batch(self.beta)\n",
    "        weights = self.float_tensor( samples[\"weights\"].reshape(-1, 1) )\n",
    "        indices = samples[\"indices\"]\n",
    "        \n",
    "        # 1-step Learning loss\n",
    "        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n",
    "        \n",
    "        # PER: importance sampling before average\n",
    "        loss = torch.mean(elementwise_loss * weights)\n",
    "        \n",
    "        # N-step Learning loss\n",
    "        # we are gonna combine 1-step loss and n-step loss so as to\n",
    "        # prevent high-variance. The original rainbow employs n-step loss only.\n",
    "        if self.use_n_step & indices:\n",
    "            gamma = self.gamma ** self.n_step\n",
    "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
    "            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n",
    "            elementwise_loss += elementwise_loss_n_loss\n",
    "            \n",
    "            # PER: importance sampling before average\n",
    "            loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        # https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_\n",
    "        clip_grad_norm_(self.dqn.parameters(), 1.0, norm_type=1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # PER: update priorities\n",
    "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
    "        new_priorities = loss_for_prior + self.prior_eps\n",
    "        self.memory.update_priorities(indices, new_priorities)\n",
    "        \n",
    "        # NoisyNet: reset noise\n",
    "        self.dqn.reset_noise()\n",
    "        self.dqn_target.reset_noise()\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        update_cnt = 0\n",
    "        losses = []\n",
    "        self.scores = []\n",
    "        self.current_score = []\n",
    "        score = 0\n",
    "\n",
    "        for frame_idx in range(1, num_frames + 1):\n",
    "            action = self.select_action(state)\n",
    "            \n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            self.current_score.append(score)\n",
    "            \n",
    "            # NoisyNet: removed decrease of epsilon\n",
    "            \n",
    "            # PER: increase beta\n",
    "            fraction = min(frame_idx / num_frames, 1.0)\n",
    "            self.beta = self.beta + fraction * (1.0 - self.beta)\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                self.current_score = []\n",
    "                self.scores.append(score)\n",
    "                score = 0\n",
    "\n",
    "            # if training is ready\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "                \n",
    "                # if hard update is needed\n",
    "                if update_cnt % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "\n",
    "            # plotting\n",
    "            if frame_idx % plotting_interval == 0:\n",
    "                self._plot(frame_idx, self.scores,self.current_score, losses)\n",
    "                \n",
    "        self.env.close()\n",
    "                \n",
    "    def test(self) -> List[np.ndarray]:\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "        \n",
    "        raw_state = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        frames = []\n",
    "        while not done:\n",
    "            frames.append(self.env.render(mode=\"rgb_array\"))\n",
    "            action = self.select_action(raw_state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            raw_state = next_state\n",
    "            score += reward\n",
    "        \n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def float_tensor(self, numpy_array):\n",
    "        return torch.FloatTensor(numpy_array).to(self.device)\n",
    "\n",
    "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray], gamma: float) -> torch.Tensor:\n",
    "        \"\"\"Return categorical dqn loss.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "        state_visual, state_not_visual = self.unflatten_state( samples[\"obs\"] )\n",
    "        next_state_visual, next_state_not_visual = self.unflatten_state( samples[\"next_obs\"] )\n",
    "        next_state_visual = self.float_tensor( next_state_visual )\n",
    "        next_state_not_visual = self.float_tensor( next_state_not_visual )\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(device)\n",
    "        reward = self.float_tensor( samples[\"rews\"].reshape(-1, 1) )\n",
    "        done = self.float_tensor( samples[\"done\"].reshape(-1, 1) )\n",
    "        \n",
    "        # Categorical DQN algorithm\n",
    "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_action = self.dqn(next_state_visual, \n",
    "                                   next_state_not_visual).argmax(1)\n",
    "            next_dist = self.dqn_target.dist(next_state_visual, \n",
    "                                   next_state_not_visual)\n",
    "            next_dist = next_dist[range(self.batch_size), \n",
    "                                  next_action]\n",
    "\n",
    "            t_z = reward + (1 - done) * gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    0, (batch_size - 1) * self.atom_size, self.batch_size\n",
    "                ).long()\n",
    "                .unsqueeze(1)\n",
    "                .expand(self.batch_size, self.atom_size)\n",
    "                .to(self.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.dqn.dist(self.float_tensor(state_visual),\n",
    "                             self.float_tensor(state_not_visual))\n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
    "\n",
    "        return elementwise_loss\n",
    "\n",
    "    def _target_hard_update(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "                \n",
    "    def _plot(\n",
    "        self, \n",
    "        frame_idx: int, \n",
    "        scores: List[float],\n",
    "        current_score: List[float],\n",
    "        losses: List[float],\n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores)\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        plt.subplot(133)\n",
    "        plt.title('current_score')\n",
    "        plt.plot(current_score)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "You can see the [code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) and [configurations](https://github.com/openai/gym/blob/master/gym/envs/__init__.py#L53) of CartPole-v0 from OpenAI's repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 359 ms, sys: 345 ms, total: 704 ms\n",
      "Wall time: 1min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict(compassAngle:Box(), inventory:Dict(dirt:Box()), pov:Box(64, 64, 3))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time env = gym.make(ENV_NAME)\n",
    "obs_dim = 64*64*3 + 1\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "seed_torch(SEED)\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "number of trainable parameters: 877698\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "num_frames = 10000000\n",
    "memory_size = 100000\n",
    "batch_size = 32\n",
    "target_update = 100\n",
    "\n",
    "# train\n",
    "agent = DQNAgent(env, memory_size, batch_size, target_update, obs_dim)\n",
    "\n",
    "# number of trainable parameters of model\n",
    "print('number of trainable parameters:', sum(p.numel() for p in agent.dqn.parameters()  if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the agent.\"\"\"\n",
    "\n",
    "state = env.reset()\n",
    "update_cnt = 0\n",
    "losses = []\n",
    "scores = []\n",
    "current_score = []\n",
    "current_scores = []\n",
    "max_parameters = []\n",
    "plotting_interval = 200\n",
    "score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10000000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/10000000 [00:00<1069:33:10,  2.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 2/10000000 [00:00<1059:02:59,  2.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 3/10000000 [00:01<1108:42:07,  2.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 4/10000000 [00:01<1265:55:32,  2.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 5/10000000 [00:02<1235:54:57,  2.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 6/10000000 [00:02<1228:53:12,  2.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 7/10000000 [00:03<1207:53:42,  2.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 8/10000000 [00:03<1177:18:25,  2.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 9/10000000 [00:03<1177:44:29,  2.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 10/10000000 [00:04<1117:31:33,  2.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 11/10000000 [00:04<1087:55:53,  2.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 12/10000000 [00:04<982:20:22,  2.83it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 13/10000000 [00:05<952:16:11,  2.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 14/10000000 [00:05<959:21:58,  2.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 15/10000000 [00:05<947:44:16,  2.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 16/10000000 [00:06<902:59:58,  3.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 17/10000000 [00:06<941:36:05,  2.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 18/10000000 [00:06<972:59:33,  2.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 19/10000000 [00:07<1001:55:30,  2.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 20/10000000 [00:07<1033:42:20,  2.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 21/10000000 [00:08<1003:57:27,  2.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 22/10000000 [00:08<1028:56:46,  2.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 23/10000000 [00:08<999:57:08,  2.78it/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 24/10000000 [00:09<979:17:03,  2.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 25/10000000 [00:09<984:19:40,  2.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 26/10000000 [00:09<1014:47:47,  2.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 27/10000000 [00:10<1056:58:44,  2.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 28/10000000 [00:10<1042:58:55,  2.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 29/10000000 [00:10<1026:44:47,  2.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 30/10000000 [00:11<1075:43:50,  2.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 31/10000000 [00:11<1057:45:02,  2.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 32/10000000 [00:12<1067:46:55,  2.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 33/10000000 [00:12<1063:49:43,  2.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-91ceb350723f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# if training is ready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mupdate_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-147-703c4bd4d5f4>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# 1-step Learning loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0melementwise_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dqn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# PER: importance sampling before average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-147-703c4bd4d5f4>\u001b[0m in \u001b[0;36m_compute_dqn_loss\u001b[0;34m(self, samples, gamma)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;34m\"\"\"Return categorical dqn loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m  \u001b[0;31m# for shortening the following lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mstate_visual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_not_visual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten_state\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"obs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mnext_state_visual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_not_visual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten_state\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"next_obs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mnext_state_visual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_tensor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnext_state_visual\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-147-703c4bd4d5f4>\u001b[0m in \u001b[0;36munflatten_state\u001b[0;34m(self, flat_state)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munflatten_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflat_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "for frame_idx in tqdm(range(1, num_frames + 1)):\n",
    "    action = agent.select_action(state)\n",
    "\n",
    "    next_state, reward, done = agent.step(action)\n",
    "\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    current_score.append(score)\n",
    "\n",
    "    # NoisyNet: removed decrease of epsilon\n",
    "\n",
    "    # PER: increase beta\n",
    "    fraction = min(frame_idx / num_frames, 1.0)\n",
    "    agent.beta = agent.beta + fraction * (1.0 - agent.beta)\n",
    "\n",
    "    # if episode ends\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        current_scores.append(current_score)\n",
    "        scores.append(score)\n",
    "        pickle.dump(current_scores, open('current_score.pkl', 'wb'))\n",
    "        pickle.dump(scores, open('scores.pkl', 'wb'))\n",
    "        current_score = []\n",
    "        score = 0\n",
    "\n",
    "    # if training is ready\n",
    "    if len(agent.memory) >= agent.batch_size:\n",
    "        loss = agent.update_model()\n",
    "        losses.append(loss)\n",
    "        update_cnt += 1\n",
    "\n",
    "        # if hard update is needed\n",
    "        if update_cnt % agent.target_update == 0:\n",
    "            agent._target_hard_update()\n",
    "\n",
    "    # plotting\n",
    "    if frame_idx % plotting_interval == 0:\n",
    "        agent._plot(frame_idx, scores, current_score, losses)\n",
    "        torch.save(agent.dqn.state_dict(), 'rainbow_model.torch')\n",
    "        torch.save(agent.dqn_target.state_dict(), 'rainbow_model.torch')\n",
    "        # monitor parameters\n",
    "        max_parameters.append( [float(i.max()) for i in agent.dqn.parameters()])\n",
    "        pickle.dump(max_parameters, open('max_parameters.pkl', 'wb'))\n",
    "        \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-147-703c4bd4d5f4>\u001b[0m(144)\u001b[0;36munflatten_state\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    142 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    143 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0munflatten_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 144 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflat_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    145 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    146 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mconvert_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> flat_state\n",
      "OrderedDict([('compassAngle', array([-168.44076  ,  179.240652 ,  179.240652 ,  173.3590224,\n",
      "        173.34675  ,  178.27137  , -173.08908  , -173.08908  ,\n",
      "       -161.994096 , -161.994096 , -149.747868 , -137.5418772,\n",
      "       -126.3332016, -116.812512 , -109.400472 , -104.267124 ,\n",
      "       -101.367468 , -100.487916 , -101.2966056, -103.35582  ,\n",
      "       -105.628266 , -106.35048  , -104.187204 , -104.187204 ,\n",
      "        -98.126964 ,  -98.126964 ,  -88.653492 ,  -77.0003424,\n",
      "        -64.3607424,  -51.2271864,  -37.447164 ,  -22.6345824])), ('inventory', OrderedDict([('dirt', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))])), ('pov', array([[[[ 19,  26, 149],\n",
      "         [ 18,  25, 148],\n",
      "         [ 19,  26, 149],\n",
      "         ...,\n",
      "         [ 20,  27, 152],\n",
      "         [ 21,  28, 153],\n",
      "         [ 21,  28, 153]],\n",
      "\n",
      "        [[ 19,  26, 149],\n",
      "         [ 19,  26, 149],\n",
      "         [ 18,  25, 148],\n",
      "         ...,\n",
      "         [ 20,  27, 152],\n",
      "         [ 21,  28, 153],\n",
      "         [ 21,  28, 153]],\n",
      "\n",
      "        [[ 20,  29, 151],\n",
      "         [ 20,  29, 151],\n",
      "         [ 18,  25, 148],\n",
      "         ...,\n",
      "         [ 19,  26, 149],\n",
      "         [ 21,  28, 151],\n",
      "         [ 19,  26, 149]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  2,   2,  28],\n",
      "         [  2,   2,  28],\n",
      "         [  1,   1,  27],\n",
      "         ...,\n",
      "         [  4,   5,  28],\n",
      "         [  2,   3,  26],\n",
      "         [  1,   2,  25]],\n",
      "\n",
      "        [[  2,   2,  28],\n",
      "         [  2,   2,  28],\n",
      "         [  2,   2,  28],\n",
      "         ...,\n",
      "         [  4,   5,  28],\n",
      "         [  4,   5,  28],\n",
      "         [  2,   3,  26]],\n",
      "\n",
      "        [[  2,   2,  28],\n",
      "         [  1,   1,  27],\n",
      "         [  2,   2,  28],\n",
      "         ...,\n",
      "         [  4,   5,  28],\n",
      "         [  4,   5,  28],\n",
      "         [  4,   5,  28]]],\n",
      "\n",
      "\n",
      "       [[[ 19,  26, 149],\n",
      "         [ 18,  25, 148],\n",
      "         [ 18,  27, 151],\n",
      "         ...,\n",
      "         [ 20,  29, 153],\n",
      "         [ 21,  28, 151],\n",
      "         [ 19,  26, 149]],\n",
      "\n",
      "        [[ 23,  30, 153],\n",
      "         [ 18,  25, 148],\n",
      "         [ 17,  26, 150],\n",
      "         ...,\n",
      "         [ 20,  29, 153],\n",
      "         [ 20,  27, 150],\n",
      "         [ 18,  25, 148]],\n",
      "\n",
      "        [[ 23,  32, 154],\n",
      "         [ 22,  31, 153],\n",
      "         [ 22,  31, 153],\n",
      "         ...,\n",
      "         [ 19,  28, 152],\n",
      "         [ 20,  27, 150],\n",
      "         [ 20,  27, 150]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         ...,\n",
      "         [  4,   5,  28],\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26]],\n",
      "\n",
      "        [[  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         ...,\n",
      "         [  4,   5,  28],\n",
      "         [  4,   5,  28],\n",
      "         [  2,   3,  26]],\n",
      "\n",
      "        [[  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         ...,\n",
      "         [  2,   3,  26],\n",
      "         [  4,   5,  28],\n",
      "         [  4,   5,  28]]],\n",
      "\n",
      "\n",
      "       [[[ 15,  27, 148],\n",
      "         [ 16,  28, 149],\n",
      "         [ 20,  27, 150],\n",
      "         ...,\n",
      "         [ 24,  33, 157],\n",
      "         [ 22,  31, 153],\n",
      "         [ 20,  29, 151]],\n",
      "\n",
      "        [[ 30,  42, 163],\n",
      "         [ 30,  42, 163],\n",
      "         [ 20,  27, 150],\n",
      "         ...,\n",
      "         [ 19,  28, 152],\n",
      "         [ 19,  28, 150],\n",
      "         [ 20,  29, 151]],\n",
      "\n",
      "        [[ 25,  37, 160],\n",
      "         [ 30,  42, 165],\n",
      "         [ 30,  42, 163],\n",
      "         ...,\n",
      "         [ 20,  27, 148],\n",
      "         [ 19,  27, 145],\n",
      "         [ 20,  28, 146]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  1,   2,  25],\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         ...,\n",
      "         [  4,   2,  26],\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26]],\n",
      "\n",
      "        [[  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         ...,\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26]],\n",
      "\n",
      "        [[  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         [  1,   2,  25],\n",
      "         ...,\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26],\n",
      "         [  2,   3,  26]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[  6,   4,  12],\n",
      "         [  5,   3,  11],\n",
      "         [  4,   4,  12],\n",
      "         ...,\n",
      "         [  3,   2,   7],\n",
      "         [  3,   2,   7],\n",
      "         [  3,   2,   7]],\n",
      "\n",
      "        [[  6,   4,  12],\n",
      "         [  6,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         ...,\n",
      "         [  3,   2,   7],\n",
      "         [  3,   2,   7],\n",
      "         [  3,   2,   7]],\n",
      "\n",
      "        [[  4,   5,  10],\n",
      "         [  4,   5,  10],\n",
      "         [  4,   5,  10],\n",
      "         ...,\n",
      "         [  3,   2,   5],\n",
      "         [  3,   2,   5],\n",
      "         [  3,   2,   5]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 32,  46, 185],\n",
      "         [ 36,  50, 189],\n",
      "         [ 36,  50, 189],\n",
      "         ...,\n",
      "         [ 24,  38, 175],\n",
      "         [ 25,  35, 169],\n",
      "         [ 27,  37, 171]],\n",
      "\n",
      "        [[ 35,  49, 188],\n",
      "         [ 35,  49, 188],\n",
      "         [ 31,  42, 180],\n",
      "         ...,\n",
      "         [ 44,  66, 207],\n",
      "         [ 27,  39, 174],\n",
      "         [ 26,  38, 173]],\n",
      "\n",
      "        [[ 35,  49, 188],\n",
      "         [ 29,  43, 182],\n",
      "         [ 30,  41, 179],\n",
      "         ...,\n",
      "         [ 44,  66, 207],\n",
      "         [ 52,  64, 199],\n",
      "         [ 28,  40, 175]]],\n",
      "\n",
      "\n",
      "       [[[  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         ...,\n",
      "         [  6,   5,  10],\n",
      "         [  3,   3,  11],\n",
      "         [  4,   4,  12]],\n",
      "\n",
      "        [[  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         ...,\n",
      "         [  6,   5,  10],\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12]],\n",
      "\n",
      "        [[  4,   5,  10],\n",
      "         [  4,   5,  10],\n",
      "         [  4,   5,  10],\n",
      "         ...,\n",
      "         [  4,   5,  10],\n",
      "         [  4,   5,  10],\n",
      "         [  4,   5,  10]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 35,  45, 181],\n",
      "         [ 29,  39, 175],\n",
      "         [ 29,  39, 173],\n",
      "         ...,\n",
      "         [ 29,  36, 169],\n",
      "         [ 30,  37, 170],\n",
      "         [ 30,  37, 170]],\n",
      "\n",
      "        [[ 29,  39, 173],\n",
      "         [ 29,  39, 173],\n",
      "         [ 34,  48, 185],\n",
      "         ...,\n",
      "         [ 22,  36, 173],\n",
      "         [ 30,  37, 172],\n",
      "         [ 30,  37, 172]],\n",
      "\n",
      "        [[ 29,  39, 173],\n",
      "         [ 37,  47, 181],\n",
      "         [ 33,  47, 184],\n",
      "         ...,\n",
      "         [ 35,  49, 186],\n",
      "         [ 29,  36, 171],\n",
      "         [ 27,  34, 169]]],\n",
      "\n",
      "\n",
      "       [[[  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         ...,\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12]],\n",
      "\n",
      "        [[  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         ...,\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12]],\n",
      "\n",
      "        [[  4,   4,  12],\n",
      "         [  4,   4,  12],\n",
      "         [  4,   5,  10],\n",
      "         ...,\n",
      "         [  4,   5,  10],\n",
      "         [  4,   4,  12],\n",
      "         [  4,   4,  12]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 32,  39, 172],\n",
      "         [ 27,  34, 167],\n",
      "         [ 26,  33, 166],\n",
      "         ...,\n",
      "         [ 29,  41, 176],\n",
      "         [ 31,  43, 178],\n",
      "         [ 31,  43, 178]],\n",
      "\n",
      "        [[ 26,  33, 168],\n",
      "         [ 26,  33, 168],\n",
      "         [ 27,  37, 171],\n",
      "         ...,\n",
      "         [ 29,  38, 177],\n",
      "         [ 28,  39, 177],\n",
      "         [ 30,  41, 179]],\n",
      "\n",
      "        [[ 27,  34, 169],\n",
      "         [ 29,  36, 171],\n",
      "         [ 30,  40, 174],\n",
      "         ...,\n",
      "         [ 30,  39, 178],\n",
      "         [ 36,  47, 185],\n",
      "         [ 29,  40, 178]]]], dtype=uint8))])\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-e34b7a9ac00d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch_hum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-c30154fab826>\u001b[0m in \u001b[0;36msample_batch_hum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_batch_hum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;34m\"\"\"Sample a batch of experiences from human replay.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_data_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         return dict(\n\u001b[1;32m     72\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.memory.sample_batch_hum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# start_time: 0:02\n",
    "# agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Run the trained agent (1 episode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = agent.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imports specifically so we can render outputs in Colab.\n",
    "# from matplotlib import animation\n",
    "# from JSAnimation.IPython_display import display_animation\n",
    "# from IPython.display import display\n",
    "\n",
    "\n",
    "# def display_frames_as_gif(frames):\n",
    "#     \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
    "#     patch = plt.imshow(frames[0])\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     def animate(i \n",
    "#         patch.set_data(frames[i])\n",
    "\n",
    "#     anim = animation.FuncAnimation(\n",
    "#         plt.gcf(), animate, frames = len(frames), interval=50\n",
    "#     )\n",
    "#     display(display_animation(anim, default_mode='loop'))\n",
    "    \n",
    "        \n",
    "# # display \n",
    "# display_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
