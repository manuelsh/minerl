{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: this version does not have fixed alpha possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'sac,tree_chop'\n",
    "\n",
    "from importlib import reload\n",
    "import models_sac_treechop\n",
    "import utils\n",
    "reload(models_sac_treechop)\n",
    "reload(utils)\n",
    "\n",
    "from models_sac_treechop import CriticNetwork, ActorNetwork\n",
    "from utils import ReplayBuffer, seed_everything, Monitor\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import minerl\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "logging.basicConfig(filename='logs/'+VERSION+'-'+time.strftime(\"%Y%m%d-%H%M%S\")+'.log', \n",
    "                    filemode='w', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = ['attack', 'jump', 'forward', 'back', 'left', 'right', 'sprint', 'sneak', 'camera_hor', 'camera_ver']\n",
    "\n",
    "SEED = 584\n",
    "OBS_DIM = int(64*64*3) # pov\n",
    "BUFFER_SIZE = int(1E5)\n",
    "\n",
    "MAX_NUM_FRAMES = int(1E9)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "POV_SCALING = 255\n",
    "TANH_FACTOR = 1\n",
    "CAMERA_FACTOR = 1\n",
    "NUM_FILTERS = 128\n",
    "\n",
    "ACTOR_LR = 1E-4\n",
    "CRITIC_LR = 1E-4\n",
    "ALPHA_LR = 1E-4\n",
    "\n",
    "GAMMA = 0.9\n",
    "BUFFER_SIZE = int(1E5)\n",
    "\n",
    "TAU = 0.001\n",
    "USE_BN = True\n",
    "\n",
    "INITIAL_ALPHA = 0.1\n",
    "\n",
    "CHECKS = True\n",
    "\n",
    "monitor = Monitor('monitor_'+VERSION+'.pkl', 'monitoring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_acts=len(ACTIONS),\n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 gamma=GAMMA,\n",
    "                 actor_learning_rate=ACTOR_LR, \n",
    "                 critic_learning_rate=CRITIC_LR,\n",
    "                 alpha_lr = ALPHA_LR,\n",
    "                 tau=TAU,\n",
    "                 buffer_capacity=BUFFER_SIZE,\n",
    "                 initial_alpha=INITIAL_ALPHA\n",
    "                 ):\n",
    "\n",
    "        self.num_acts = num_acts\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(\"Device is: \"+str(self.device))\n",
    "        print(\"Device is: \", self.device)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.actor_lr = actor_learning_rate\n",
    "        self.critic_lr = critic_learning_rate\n",
    "        self.tau = tau\n",
    "        self.buffer = ReplayBuffer( \n",
    "            obs_dim=OBS_DIM,\n",
    "            size=BUFFER_SIZE,\n",
    "            act_dim=len(ACTIONS),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        \n",
    "        params_nn = {'acts_dim':len(ACTIONS), 'num_filters':NUM_FILTERS, 'use_bn':USE_BN,  \n",
    "            'pov_scaling':POV_SCALING}\n",
    "        self.policy = ActorNetwork(**params_nn).to(self.device)\n",
    "        self.q1 = CriticNetwork(**params_nn).to(self.device)\n",
    "        self.q2 = CriticNetwork(**params_nn).to(self.device)\n",
    "        self.target_q1 = CriticNetwork(**params_nn).to(self.device)\n",
    "        self.target_q2 = CriticNetwork(**params_nn).to(self.device)\n",
    "        \n",
    "        \n",
    "        self.target_q1.load_state_dict(self.q1.state_dict())\n",
    "        self.target_q2.load_state_dict(self.q2.state_dict())\n",
    "        \n",
    "        self.q1_optim = optim.Adam(self.q1.parameters(), lr=CRITIC_LR)\n",
    "        self.q2_optim = optim.Adam(self.q2.parameters(), lr=CRITIC_LR)\n",
    "        self.policy_optim = optim.Adam(self.policy.parameters(), lr=ACTOR_LR)\n",
    "        self.alpha_optim = optim.Adam([self.log_alpha], lr=ALPHA_LR)\n",
    "        \n",
    "        self.target_entropy = -torch.prod(torch.Tensor(num_acts).to(self.device)).item()\n",
    "        self.alpha = initial_alpha\n",
    "\n",
    "\n",
    "    def get_act(self, obs):\n",
    "        \n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            act = self.policy.get_action(obs)\n",
    "        self.policy.train()\n",
    "        \n",
    "        return act.cpu().numpy()\n",
    "        \n",
    "    def unflatten_obs(self, flat_obs):\n",
    "        return (flat_obs.reshape(-1,64,64,3))\n",
    "        \n",
    "    def flatten_obs(self, obs):\n",
    "        return obs.reshape(-1)\n",
    "\n",
    "    # Store the transition into the replay buffer\n",
    "    def store_transition(self, obs, next_obs, act, rew, done):\n",
    "        obs = self.flatten_obs(obs)\n",
    "        next_obs = self.flatten_obs(next_obs)\n",
    "        self.buffer.store(obs=obs, act=act, rew=rew, \n",
    "                          next_obs=next_obs, done=done)\n",
    "\n",
    "    def float_tensor(self, numpy_array):\n",
    "        return torch.FloatTensor(numpy_array).to(self.device)\n",
    "        \n",
    "    def fit_batch(self):\n",
    "        # Sample frorm buffer\n",
    "        transitions = self.buffer.sample_batch()\n",
    "        \n",
    "        obss = self.unflatten_obs(self.float_tensor(transitions['obs']))\n",
    "        next_obss = self.unflatten_obs(self.float_tensor(transitions['next_obs']))\n",
    "        acts = self.float_tensor(transitions['acts'])\n",
    "        rews = self.float_tensor(transitions['rews'])\n",
    "        dones = self.float_tensor(transitions['dones'])\n",
    "        \n",
    "        # Q function loss\n",
    "        with torch.no_grad():\n",
    "            next_log_prob, next_action = self.policy.get_log_probs(next_obss)\n",
    "            target_q1_next = self.target_q1(next_obss, next_action).view(-1)\n",
    "            target_q2_next = self.target_q2(next_obss, next_action).view(-1)\n",
    "            min_q_target_hat = torch.min(target_q1_next, target_q2_next) - self.alpha * next_log_prob.view(-1) # UNDERSTAND\n",
    "            y = rews + (1 - dones) * self.gamma * min_q_target_hat\n",
    "            monitor.add(['next_log_prob_mean','min_q_target_hat_mean','alpha','y_mean'],\n",
    "                       [float(next_log_prob.mean()), float(min_q_target_hat.mean()), \n",
    "                        float(self.alpha), float(y.mean())])\n",
    "        \n",
    "        q1_hat = self.q1( obss, acts ).view(-1)  \n",
    "        q2_hat = self.q2( obss, acts ).view(-1)  \n",
    "        q1_loss = F.mse_loss(q1_hat, y) \n",
    "        q2_loss = F.mse_loss(q2_hat, y)\n",
    "        #assert (float(q1_loss)<100) or (float(q2_loss)<100)\n",
    "        \n",
    "        # Policy loss\n",
    "        log_pi, pi = self.policy.get_log_probs(obss)\n",
    "        \n",
    "        q1_hat_policy = self.q1(obss, pi)\n",
    "        q2_hat_policy = self.q2(obss, pi)\n",
    "        min_q_pi = torch.min(q1_hat_policy, q2_hat_policy)\n",
    "\n",
    "        policy_loss = ((self.alpha * log_pi) - min_q_pi).mean() # JÏ€ = ð”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.q1_optim.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0, norm_type=1)\n",
    "        self.q1_optim.step()\n",
    "        \n",
    "        self.q2_optim.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0, norm_type=1)\n",
    "        self.q2_optim.step()\n",
    "        \n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0, norm_type=1)\n",
    "        self.policy_optim.step()\n",
    "        \n",
    "        # Alpha parameter tuning      \n",
    "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "\n",
    "        self.alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.log_alpha.parameters(), 1.0, norm_type=1)\n",
    "        self.alpha_optim.step()\n",
    "\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        return q1_loss, q2_loss, policy_loss, q1_hat, q2_hat\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        self.polyak_averaging(self.target_q1, self.q1)\n",
    "        self.polyak_averaging(self.target_q2, self.q2)\n",
    "        \n",
    "    def polyak_averaging(self, target, original):\n",
    "        for target_param, param in zip(target.parameters(), original.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + target_param.data * (1.0 - self.tau))\n",
    "            \n",
    "    def get_env_act(self, model_act):\n",
    "        '''\n",
    "        Gets environment act from model act\n",
    "        '''\n",
    "        env_act = {act: int(value>0) for act, value in zip(ACTIONS[0:8], model_act[0:8])}\n",
    "        env_act['camera'] = [model_act[8]*180, model_act[9]*180]\n",
    "        return env_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 177 ms, sys: 428 ms, total: 604 ms\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "if 'environment_name' not in locals():\n",
    "    environment_name = 'MineRLTreechop-v0'\n",
    "    %time env = gym.make(environment_name)\n",
    "\n",
    "seed_everything(seed=SEED, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "\n",
    "# number of trainable parameters of model\n",
    "message_1 = 'Number of trainable parameters actor:' + str(sum(p.numel() for p in agent.policy.parameters()  if p.requires_grad))\n",
    "message_2 = 'Number of trainable parameters critic:'+str(sum(p.numel() for p in agent.q1.parameters()  if p.requires_grad)*2)\n",
    "message_3 = 'Number of trainable parameters alpha:'+' Unknown'\n",
    "print(message_1)\n",
    "print(message_2)\n",
    "print(message_3)\n",
    "\n",
    "logging.info(message_1)\n",
    "logging.info(message_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent\n",
    "# agent.actor.load_state_dict(torch.load('trained_models/ddpg_actor_'+VERSION+'.pkl'))\n",
    "# agent.critic.load_state_dict(torch.load('trained_models/ddpg_critic_'+VERSION+'.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time obs = env.reset()\n",
    "assert (agent.unflatten_obs( np.array([agent.flatten_obs(obs['pov'])]) )[0].astype(int) == obs['pov']).sum() == 3*64*64\n",
    "trajectory_count = 1\n",
    "losses_agent = []\n",
    "losses_critic = []\n",
    "scores = []\n",
    "current_score = []\n",
    "current_scores = []\n",
    "max_parameters = []\n",
    "plotting_interval = 100\n",
    "score = 0\n",
    "logging.info('Environment reset')\n",
    "\n",
    "q1_values_mean=[]\n",
    "q2_values_mean=[]\n",
    "losses_agent=[]\n",
    "losses_q1=[]\n",
    "losses_q2=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relus_are_alive():\n",
    "    for network in [agent.q1, agent.q2, agent.policy]:\n",
    "        if float(network.non_lin_1.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_1 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "        if float(network.non_lin_2.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_2 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "        if float(network.non_lin_3.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_3 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "            \n",
    "            \n",
    "# test\n",
    "# agent.actor.x_relu_1[agent.actor.x_relu_1!=0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_idx in tqdm(range(MAX_NUM_FRAMES), desc='frame'):\n",
    "    obs_tensor = agent.float_tensor([obs['pov'].astype(float)])\n",
    "    model_act = agent.get_act(obs_tensor)\n",
    "    env_act = agent.get_env_act(model_act[0])\n",
    "    next_obs, rew, done, info = env.step(env_act)\n",
    "    \n",
    "    # Monitor behaviour of agent for checks\n",
    "    if CHECKS:\n",
    "        with torch.no_grad():\n",
    "            new_q = agent.q1(obs_tensor, agent.float_tensor(model_act))\n",
    "        monitor.add([ 'camera_hor_agent','q_agent','rew_agent'], \n",
    "                    [ env_act['camera'][1], float(new_q), rew])\n",
    "    \n",
    "    # Store the transition in the replay buffer of the agent\n",
    "    agent.store_transition(obs=obs['pov'], next_obs=next_obs['pov'],\n",
    "                               act=model_act, done=done, rew=rew)\n",
    "    \n",
    "    # Prepare for next step and store scores\n",
    "    obs = next_obs\n",
    "    score += rew\n",
    "    monitor.add(f'score_{trajectory_count}', score)\n",
    "    monitor.add(ACTIONS, model_act[0])\n",
    "    monitor.add(['agent_mean', 'agent_log_std'], [float(agent.policy.mean.mean()), float(agent.policy.log_std.mean())])\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        score = 0\n",
    "        last_score = monitor.data[f'score_{trajectory_count}'][-1]\n",
    "        trajectory_count += 1\n",
    "        \n",
    "        # Save model and log\n",
    "        torch.save(agent.policy.state_dict(), 'trained_models/sac_policy'+VERSION+'.pkl')\n",
    "        torch.save(agent.q1.state_dict(), 'trained_models/sac_q1'+VERSION+'.pkl')\n",
    "        torch.save(agent.q2.state_dict(), 'trained_models/sac_q2'+VERSION+'.pkl')\n",
    "        logging.info(f'Trajectory {len(current_scores)} done, with final score {last_score}')\n",
    "        \n",
    "    # TRAIN\n",
    "    if len(agent.buffer) >= agent.batch_size:\n",
    "        q1_loss, q2_loss, policy_loss, q1_hat, q2_hat = agent.fit_batch()\n",
    "        # agent.update_target_networks()\n",
    "        q1_hat_mean = float(q1_hat.mean())\n",
    "        q2_hat_mean = float(q2_hat.mean())\n",
    "        monitor.add(['q1_hat.mean', 'q2_hat.mean', 'loss_agent', 'loss_q1', 'loss_q2'],\n",
    "                    [q1_hat_mean, q2_hat_mean, float(policy_loss),\n",
    "                    float(q1_loss), float(q2_loss) ])\n",
    "        \n",
    "        #assert (q1_hat_mean==q1_hat_mean) or (q2_hat_mean==q2_hat_mean), \"At least one q function returns NaN!\"\n",
    "\n",
    "    if (frame_idx+1) % plotting_interval == 0:\n",
    "        #check_relus_are_alive()\n",
    "        #clear_output(True)\n",
    "        #monitor.plot_all()\n",
    "        monitor.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "net_reward = 0\n",
    "actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    obs = agent.float_tensor([obs['pov'].astype(float)])\n",
    "    model_act = agent.get_act(obs)\n",
    "    env_act = get_env_act(model_act[0])\n",
    "    next_obs, rew, done, info = env.step(env_act)\n",
    "\n",
    "    # Prepare for next step and store scores\n",
    "    obs = next_obs\n",
    "    score += rew\n",
    "    current_score.append(score)\n",
    "    \n",
    "#     if i%10==0:\n",
    "    plt.imshow(env.render(mode='rgb_array')) \n",
    "    display.display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "    net_reward += rew\n",
    "    actions.append((env_act, net_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
