{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: understand log_prob of evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'sac,lr=1E-4,gamma=0.9,sq_log_corrected'\n",
    "\n",
    "from importlib import reload\n",
    "import models_sac\n",
    "import utils\n",
    "reload(models_sac)\n",
    "reload(utils)\n",
    "\n",
    "from models_sac import CriticNetwork, ActorNetwork\n",
    "from utils import ReplayBuffer, seed_everything, Monitor\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import minerl\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "logging.basicConfig(filename='logs/'+VERSION+'-'+time.strftime(\"%Y%m%d-%H%M%S\")+'.log', \n",
    "                    filemode='w', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = ['attack', 'jump', 'forward', 'back', 'left', 'right', 'sprint', 'sneak', 'camera_hor', 'camera_ver']\n",
    "\n",
    "SEED = 584\n",
    "OBS_DIM = int(64*64*3+1) # pov + compassAngle\n",
    "BUFFER_SIZE = int(1E5)\n",
    "\n",
    "MAX_NUM_FRAMES = int(1E9)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "POV_SCALING = 255\n",
    "COMPASS_SCALING = 180\n",
    "MAX_CAMERA_TURN = 15\n",
    "TANH_FACTOR = 1\n",
    "CAMERA_FACTOR = 1\n",
    "NUM_FILTERS = 128\n",
    "\n",
    "ACTOR_LR = 1E-4\n",
    "CRITIC_LR = 1E-4\n",
    "ALPHA_LR = 1E-4\n",
    "\n",
    "GAMMA = 0.9\n",
    "BUFFER_SIZE = int(1E5)\n",
    "\n",
    "TAU = 0.001\n",
    "USE_BN = True\n",
    "\n",
    "INITIAL_ALPHA = 0.1\n",
    "\n",
    "monitor = Monitor('monitor_'+VERSION+'.pkl', 'monitoring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_acts=len(ACTIONS),\n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 gamma=GAMMA,\n",
    "                 actor_learning_rate=ACTOR_LR, \n",
    "                 critic_learning_rate=CRITIC_LR,\n",
    "                 alpha_lr = ALPHA_LR,\n",
    "                 tau=TAU,\n",
    "                 buffer_capacity=BUFFER_SIZE,\n",
    "                 initial_alpha=INITIAL_ALPHA\n",
    "                 ):\n",
    "\n",
    "        self.num_acts = num_acts\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(\"Device is: \"+str(self.device))\n",
    "        print(\"Device is: \", self.device)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.actor_lr = actor_learning_rate\n",
    "        self.critic_lr = critic_learning_rate\n",
    "        self.tau = tau\n",
    "        self.buffer = ReplayBuffer( \n",
    "            obs_dim=OBS_DIM,\n",
    "            size=BUFFER_SIZE,\n",
    "            act_dim=len(ACTIONS),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        \n",
    "        params_nn = {'acts_dim':len(ACTIONS), 'num_filters':NUM_FILTERS, 'use_bn':USE_BN,  \n",
    "            'pov_scaling':POV_SCALING, 'compass_scaling':COMPASS_SCALING}\n",
    "        self.policy = ActorNetwork(**params_nn).to(self.device)\n",
    "        self.q1 = CriticNetwork(**params_nn).to(self.device)\n",
    "        self.q2 = CriticNetwork(**params_nn).to(self.device)\n",
    "        self.target_q1 = CriticNetwork(**params_nn).to(self.device)\n",
    "        self.target_q2 = CriticNetwork(**params_nn).to(self.device)\n",
    "        self.target_q1.load_state_dict(self.q1.state_dict())\n",
    "        self.target_q2.load_state_dict(self.q2.state_dict())\n",
    "        self.target_entropy = -torch.prod(torch.Tensor(num_acts).to(self.device)).item()\n",
    "        self.alpha = initial_alpha\n",
    "        self.log_alpha = nn.Parameter( torch.Tensor( [np.log(self.alpha)] ).to(self.device) )\n",
    "        \n",
    "        self.q1_optim = optim.Adam(self.q1.parameters(), lr=CRITIC_LR)\n",
    "        self.q2_optim = optim.Adam(self.q2.parameters(), lr=CRITIC_LR)\n",
    "        self.policy_optim = optim.Adam(self.policy.parameters(), lr=ACTOR_LR)\n",
    "        self.alpha_optim = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "\n",
    "    def get_act(self, obs_pov, obs_rest):\n",
    "        \n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            act = self.policy.get_action(obs_pov, obs_rest)\n",
    "        self.policy.train()\n",
    "        \n",
    "        return act.cpu().numpy()\n",
    "        \n",
    "    def unflatten_obs(self, flat_obs):\n",
    "        return (flat_obs[:,:-1].reshape(-1,64,64,3), flat_obs[:,-1].reshape(-1,1))\n",
    "        \n",
    "    def flatten_obs(self, obs):\n",
    "        return np.append(obs['pov'].reshape(-1), obs['compassAngle'])\n",
    "\n",
    "    # Store the transition into the replay buffer\n",
    "    def store_transition(self, obs, next_obs, act, rew, done):\n",
    "        obs = self.flatten_obs(obs)\n",
    "        next_obs = self.flatten_obs(next_obs)\n",
    "        self.buffer.store(obs=obs, act=act, rew=rew, \n",
    "                          next_obs=next_obs, done=done)\n",
    "\n",
    "    def float_tensor(self, numpy_array):\n",
    "        return torch.FloatTensor(numpy_array).to(self.device)\n",
    "        \n",
    "    def fit_batch(self):\n",
    "        # Sample frorm buffer\n",
    "        transitions = self.buffer.sample_batch()\n",
    "        \n",
    "        obss_pov, obss_rest = self.unflatten_obs(self.float_tensor(transitions['obs']))\n",
    "        next_obss_pov, next_obss_rest = self.unflatten_obs(self.float_tensor(transitions['next_obs']))\n",
    "        acts = self.float_tensor(transitions['acts'])\n",
    "        rews = self.float_tensor(transitions['rews'])\n",
    "        dones = self.float_tensor(transitions['dones'])\n",
    "        \n",
    "        # Q function loss\n",
    "        with torch.no_grad():\n",
    "            next_log_prob, next_action = self.policy.get_log_probs(next_obss_pov, next_obss_rest)\n",
    "            target_q1_next = self.target_q1(next_obss_pov, next_obss_rest, next_action).view(-1)\n",
    "            target_q2_next = self.target_q2(next_obss_pov, next_obss_rest, next_action).view(-1)\n",
    "            min_q_target_hat = torch.min(target_q1_next, target_q2_next) - self.alpha * next_log_prob # UNDERSTAND\n",
    "            y = rews + (1 - dones) * self.gamma * min_q_target_hat\n",
    "            monitor.add(['next_log_prob_mean','min_q_target_hat_mean','alpha','y_mean'],\n",
    "                       [float(next_log_prob.mean()), float(min_q_target_hat.mean()), \n",
    "                        float(self.alpha), float(y.mean())])\n",
    "        \n",
    "        q1_hat = self.q1( obss_pov, obss_rest , acts ).view(-1)  \n",
    "        q2_hat = self.q2( obss_pov, obss_rest , acts ).view(-1)  \n",
    "        q1_loss = F.mse_loss(q1_hat, y) \n",
    "        q2_loss = F.mse_loss(q2_hat, y)\n",
    "        #assert (float(q1_loss)<100) or (float(q2_loss)<100)\n",
    "        \n",
    "        # Policy loss\n",
    "        log_pi, pi = self.policy.get_log_probs(obss_pov, obss_rest)\n",
    "        \n",
    "        q1_hat_policy = self.q1(obss_pov, obss_rest, pi)\n",
    "        q2_hat_policy = self.q2(obss_pov, obss_rest, pi)\n",
    "        min_q_pi = torch.min(q1_hat_policy, q2_hat_policy)\n",
    "\n",
    "        policy_loss = ((self.alpha * log_pi) - min_q_pi).mean() # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.q1_optim.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0, norm_type=1)\n",
    "        self.q1_optim.step()\n",
    "        \n",
    "        self.q2_optim.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0, norm_type=1)\n",
    "        self.q2_optim.step()\n",
    "        \n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0, norm_type=1)\n",
    "        self.policy_optim.step()\n",
    "        \n",
    "        # Alpha parameter tuning      \n",
    "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "\n",
    "        self.alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.log_alpha.parameters(), 1.0, norm_type=1)\n",
    "        self.alpha_optim.step()\n",
    "\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        return q1_loss, q2_loss, policy_loss, q1_hat, q2_hat\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        self.polyak_averaging(self.target_q1, self.q1)\n",
    "        self.polyak_averaging(self.target_q2, self.q2)\n",
    "        \n",
    "    def polyak_averaging(self, target, original):\n",
    "        for target_param, param in zip(target.parameters(), original.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + target_param.data * (1.0 - self.tau))\n",
    "            \n",
    "    def get_env_act(self, model_act):\n",
    "        '''\n",
    "        Gets environment act from model act\n",
    "        '''\n",
    "        env_act = {act: int(value>0) for act, value in zip(ACTIONS[0:8], model_act[0:8])}\n",
    "        env_act['camera'] = [model_act[8]*MAX_CAMERA_TURN, model_act[9]*MAX_CAMERA_TURN]\n",
    "        return env_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'environment_name' not in locals():\n",
    "    environment_name = 'MineRLTreechop-v0'\n",
    "    %time env = gym.make(environment_name)\n",
    "\n",
    "seed_everything(seed=SEED, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:  cuda\n",
      "Number of trainable parameters actor:1106388\n",
      "Number of trainable parameters critic:2212866\n",
      "Number of trainable parameters alpha: Unknown\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "# number of trainable parameters of model\n",
    "message_1 = 'Number of trainable parameters actor:' + str(sum(p.numel() for p in agent.policy.parameters()  if p.requires_grad))\n",
    "message_2 = 'Number of trainable parameters critic:'+str(sum(p.numel() for p in agent.q1.parameters()  if p.requires_grad)*2)\n",
    "message_3 = 'Number of trainable parameters alpha:'+' Unknown'\n",
    "print(message_1)\n",
    "print(message_2)\n",
    "print(message_3)\n",
    "\n",
    "logging.info(message_1)\n",
    "logging.info(message_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent\n",
    "# agent.actor.load_state_dict(torch.load('trained_models/ddpg_actor_'+VERSION+'.pkl'))\n",
    "# agent.critic.load_state_dict(torch.load('trained_models/ddpg_critic_'+VERSION+'.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 ms, sys: 5.42 ms, total: 8.44 ms\n",
      "Wall time: 7.45 s\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'compassAngle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-727354a6ab87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'obs = env.reset()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten_obs\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pov'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrajectory_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlosses_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlosses_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a7efdf929823>\u001b[0m in \u001b[0;36mflatten_obs\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflatten_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pov'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compassAngle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Store the transition into the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'compassAngle'"
     ]
    }
   ],
   "source": [
    "%time obs = env.reset()\n",
    "assert (agent.unflatten_obs( np.array([agent.flatten_obs(obs)]) )[0][0].astype(int) == obs['pov']).sum() == 3*64*64\n",
    "trajectory_count = 1\n",
    "losses_agent = []\n",
    "losses_critic = []\n",
    "scores = []\n",
    "current_score = []\n",
    "current_scores = []\n",
    "max_parameters = []\n",
    "plotting_interval = 100\n",
    "score = 0\n",
    "logging.info('Environment reset')\n",
    "\n",
    "q1_values_mean=[]\n",
    "q2_values_mean=[]\n",
    "losses_agent=[]\n",
    "losses_q1=[]\n",
    "losses_q2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relus_are_alive():\n",
    "    for network in [agent.q1, agent.q2, agent.policy]:\n",
    "        if float(network.non_lin_1.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_1 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "        if float(network.non_lin_2.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_2 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "        if float(network.non_lin_3.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_3 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "            \n",
    "    \n",
    "from typing import List    \n",
    "def plot_stats(\n",
    "        frame_idx: int, \n",
    "        scores: List[float],\n",
    "        current_score: List[float],\n",
    "        losses: List[float],\n",
    "    ):\n",
    "    \"\"\"Plot the training progresses.\"\"\"\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('q_values')\n",
    "    plt.plot(scores[-10000:])\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.subplot(133)\n",
    "    plt.title('current_score')\n",
    "    plt.plot(current_score)\n",
    "    plt.show()\n",
    "            \n",
    "# test\n",
    "# agent.actor.x_relu_1[agent.actor.x_relu_1!=0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_idx in tqdm(range(MAX_NUM_FRAMES), desc='frame'):\n",
    "    obs_pov = agent.float_tensor([obs['pov'].astype(float)])\n",
    "    obs_rest = agent.float_tensor([[obs['compassAngle']]])\n",
    "    model_act = agent.get_act(obs_pov, obs_rest)\n",
    "    env_act = agent.get_env_act(model_act[0])\n",
    "    next_obs, rew, done, info = env.step(env_act)\n",
    "\n",
    "    # Store the transition in the replay buffer of the agent\n",
    "    agent.store_transition(obs=obs, next_obs=next_obs,\n",
    "                               act=model_act, done=done, rew=rew)\n",
    "    \n",
    "    # Prepare for next step and store scores\n",
    "    obs = next_obs\n",
    "    score += rew\n",
    "    \n",
    "    monitor.add(f'score_{trajectory_count}', score)\n",
    "    #current_score.append(score)\n",
    "\n",
    "    # if episode ends\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        current_scores.append(current_score)\n",
    "        scores.append(score)\n",
    "        pickle.dump(current_scores, open('scores/current_score_'+VERSION+'.pkl', 'wb'))\n",
    "        pickle.dump(scores, open('scores/scores_'+VERSION+'.pkl', 'wb'))\n",
    "        current_score = []\n",
    "        score = 0\n",
    "        torch.save(agent.policy.state_dict(), 'trained_models/sac_policy'+VERSION+'.pkl')\n",
    "        torch.save(agent.q1.state_dict(), 'trained_models/sac_q1'+VERSION+'.pkl')\n",
    "        torch.save(agent.q2.state_dict(), 'trained_models/sac_q2'+VERSION+'.pkl')\n",
    "        logging.info(f'Trajectory {len(current_scores)} done, with final score {current_scores[-1][-1]}')\n",
    "        trajectory_count += 1\n",
    "\n",
    "    # TRAIN\n",
    "    if len(agent.buffer) >= agent.batch_size:\n",
    "        q1_loss, q2_loss, policy_loss, q1_hat, q2_hat = agent.fit_batch()\n",
    "        agent.update_target_networks()\n",
    "        q1_hat_mean = float(q1_hat.mean())\n",
    "        q2_hat_mean = float(q2_hat.mean())\n",
    "        \n",
    "        assert (q1_hat_mean==q1_hat_mean) or (q2_hat_mean==q2_hat_mean), \"At least one q function returns NaN!\"\n",
    "        \n",
    "        q1_values_mean.append(q1_hat_mean)\n",
    "        q2_values_mean.append(q2_hat_mean)\n",
    "        \n",
    "        losses_agent.append(float(policy_loss))\n",
    "        losses_q1.append(float(q1_loss))\n",
    "        losses_q2.append(float(q2_loss))\n",
    "\n",
    "    if (frame_idx+1) % plotting_interval == 0:\n",
    "        \n",
    "        pickle.dump([q1_values_mean, q2_values_mean], open('logs/q_values_'+VERSION+'.pkl','wb'))\n",
    "        check_relus_are_alive()\n",
    "        \n",
    "        plot_stats(frame_idx, \n",
    "                   np.array([q1_values_mean, q2_values_mean]).T, \n",
    "                   current_score,\n",
    "                   np.array([losses_agent, losses_q1, losses_q2]).T)\n",
    "        monitor.plot_all()\n",
    "        monitor.save()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "net_reward = 0\n",
    "actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    obs_pov = agent.float_tensor([obs['pov'].astype(float)])\n",
    "    obs_rest = agent.float_tensor([[obs['compassAngle']]])\n",
    "    model_act = agent.get_act(obs_pov,obs_rest)\n",
    "    env_act = get_env_act(model_act[0])\n",
    "    next_obs, rew, done, info = env.step(env_act)\n",
    "\n",
    "    # Prepare for next step and store scores\n",
    "    obs = next_obs\n",
    "    score += rew\n",
    "    current_score.append(score)\n",
    "    \n",
    "#     if i%10==0:\n",
    "    plt.imshow(env.render(mode='rgb_array')) \n",
    "    display.display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "    net_reward += rew\n",
    "    actions.append((env_act, net_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
