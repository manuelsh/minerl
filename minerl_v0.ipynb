{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v0 tries DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvllIixz8EAo"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import minerl\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import double_dqn_agent\n",
    "import importlib\n",
    "importlib.reload(double_dqn_agent)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wlgij7v8EAp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 188 ms, sys: 316 ms, total: 504 ms\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'MineRLNavigateDense-v0'\n",
    "%time env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eALEFcDb8EAr"
   },
   "outputs": [],
   "source": [
    "SEED = 924\n",
    "NORM_FACTOR = 255\n",
    "\n",
    "def seed_everything(seed=SEED): \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wimb_9fQ8EAt",
    "outputId": "70251ded-a6bd-4a91-d11b-a088f300f5f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "Dict(attack:Discrete(2), back:Discrete(2), camera:Box(2,), forward:Discrete(2), jump:Discrete(2), left:Discrete(2), place:Enum(none,dirt), right:Discrete(2), sneak:Discrete(2), sprint:Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "ACTIONS = ['attack', \n",
    "           'back', \n",
    "           'camera_left', \n",
    "           'camera_right',\n",
    "           'forward', \n",
    "           'jump', \n",
    "           'left',\n",
    "           'place',\n",
    "           'right', \n",
    "           'sneak',\n",
    "           'sprint']\n",
    "actions_dim = 2**len(ACTIONS) # we interpret camera yaw as discrete\n",
    "print(actions_dim)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5nmOCBQ8EAv",
    "outputId": "336d835d-816c-4ba5-e4b7-b5e7f1e7672a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict(compassAngle:Box(), inventory:Dict(dirt:Box()), pov:Box(64, 64, 3))\n"
     ]
    }
   ],
   "source": [
    "state_dim = 1+64*64*3 # we ignore inventory\n",
    "non_visual_state_dim = 1\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_state(state):\n",
    "#     flatten_state = state['pov'].reshape(-1)\n",
    "#     return np.append(flatten_state, state['compassAngle']) / NORM_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_reward = 0\n",
    "# for i in range(100):\n",
    "#     action = env.action_space.noop()\n",
    "\n",
    "#     action['camera'] = [0, 0.03*obs[\"compassAngle\"]]\n",
    "#     action['back'] = 0\n",
    "#     action['forward'] = 1\n",
    "#     action['jump'] = 1\n",
    "#     action['attack'] = 1\n",
    "\n",
    "#     obs, reward, done, info = env.step(\n",
    "#         action)  \n",
    "    \n",
    "#     if i%10==0:\n",
    "#         plt.imshow(env.render(mode='rgb_array')) \n",
    "#         display.display(plt.gcf())\n",
    "#         clear_output(wait=True)\n",
    "#     net_reward += reward\n",
    "# print(\"Total reward: \", net_reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_action(action_dec):\n",
    "    bin_action = np.binary_repr(action_dec, width=len(ACTIONS))\n",
    "    action = {}\n",
    "    for n, act in enumerate(ACTIONS):\n",
    "        action[act] = int(bin_action[n])\n",
    "    if action['camera_left']==action['camera_right']:\n",
    "        action['camera'] = [ 0, 0]\n",
    "    else:\n",
    "        if action['camera_left']==1:\n",
    "            action['camera'] = [ 0, -20]\n",
    "        if action['camera_right']==1:\n",
    "            action['camera'] = [ 0, 20]\n",
    "    del action['camera_left']\n",
    "    del action['camera_right']\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the agent\n",
    "agent = double_dqn_agent.Agent(non_visual_state_dim, actions_dim, SEED)\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "> /app/code/minerl/double_dqn_agent.py(93)learn()\n",
      "-> actions_local = torch.gather(self.qnetwork_local(states['visual'], states['not_visual']), 1, actions)\n",
      "(Pdb) actions\n",
      "tensor([[ 392],\n",
      "        [1637],\n",
      "        [  64],\n",
      "        [1209],\n",
      "        [ 862],\n",
      "        [ 409],\n",
      "        [ 226],\n",
      "        [ 332],\n",
      "        [1721],\n",
      "        [1734],\n",
      "        [ 308],\n",
      "        [1626],\n",
      "        [1450],\n",
      "        [ 391],\n",
      "        [1815],\n",
      "        [ 633],\n",
      "        [ 178],\n",
      "        [ 259],\n",
      "        [1998],\n",
      "        [ 674],\n",
      "        [ 213],\n",
      "        [1441],\n",
      "        [1670],\n",
      "        [ 810],\n",
      "        [1111],\n",
      "        [ 557],\n",
      "        [2022],\n",
      "        [ 379],\n",
      "        [ 186],\n",
      "        [ 940],\n",
      "        [ 149],\n",
      "        [ 773],\n",
      "        [  43],\n",
      "        [1033],\n",
      "        [ 756],\n",
      "        [1494],\n",
      "        [ 228],\n",
      "        [ 243],\n",
      "        [1635],\n",
      "        [  69],\n",
      "        [1720],\n",
      "        [ 502],\n",
      "        [1983],\n",
      "        [  84],\n",
      "        [2043],\n",
      "        [ 992],\n",
      "        [ 947],\n",
      "        [ 312],\n",
      "        [1626],\n",
      "        [ 531],\n",
      "        [1272],\n",
      "        [1942],\n",
      "        [ 451],\n",
      "        [1128],\n",
      "        [1230],\n",
      "        [ 297],\n",
      "        [ 429],\n",
      "        [1382],\n",
      "        [1961],\n",
      "        [ 239],\n",
      "        [1902],\n",
      "        [1270],\n",
      "        [  83],\n",
      "        [ 898]])\n",
      "(Pdb) ll\n",
      " 83  \t    def learn(self, experiences, gamma):\n",
      " 84  \t        \"\"\"Update value parameters using given batch of experience tuples.\n",
      " 85  \t        Params\n",
      " 86  \t        ======\n",
      " 87  \t            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
      " 88  \t            gamma (float): discount factor\n",
      " 89  \t        \"\"\"\n",
      " 90  \t        states, actions, rewards, next_states, dones = experiences\n",
      " 91  \t        self.qnetwork_local.train()\n",
      " 92  \t        import pdb;pdb.set_trace()\n",
      " 93  ->\t        actions_local = torch.gather(self.qnetwork_local(states['visual'], states['not_visual']), 1, actions)\n",
      " 94  \t        best_action_local = self.qnetwork_local(next_states['visual'], next_states['not_visual']).max(dim=1)[1]\n",
      " 95  \t        actions_target = rewards + gamma * torch.gather( self.qnetwork_target(next_states['visual'], next_states['not_visual']),\n",
      " 96  \t                                                        1,\n",
      " 97  \t                                                        best_action_local.unsqueeze(1)) * (dones==0).float()\n",
      " 98  \t        self.loss = self.loss_function(actions_local, actions_target.detach())\n",
      " 99  \t        self.optimizer.zero_grad()\n",
      "100  \t        self.loss.backward()\n",
      "101  \t        self.optimizer.step()\n",
      "102  \t\n",
      "103  \t        # ------------------- update target network ------------------- #\n",
      "104  \t        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
      "(Pdb) self.qnetwork_local(states['visual'], states['not_visual'])\n",
      "tensor([[ 2.2643,  0.5502,  2.3059,  ..., -4.0200,  6.2181,  4.3971],\n",
      "        [ 1.4837, -0.4547,  2.3988,  ..., -2.6768,  4.7297,  4.1544],\n",
      "        [ 1.6503,  0.9665,  3.2109,  ..., -4.5408,  5.0817,  4.4662],\n",
      "        ...,\n",
      "        [ 1.8867, -0.1386,  3.0452,  ..., -3.8225,  6.7805,  3.9064],\n",
      "        [ 1.2376,  0.8807,  3.3932,  ..., -4.2727,  5.0232,  4.2387],\n",
      "        [ 2.1845,  0.2853,  2.3850,  ..., -3.9689,  5.8117,  4.6966]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "(Pdb) torch.gather(self.qnetwork_local(states['visual'], states['not_visual']), 1, actions)\n",
      "tensor([[ 3.6245e+00],\n",
      "        [ 5.4433e-01],\n",
      "        [ 2.5774e+00],\n",
      "        [ 1.3248e+00],\n",
      "        [ 2.2768e+00],\n",
      "        [ 2.0136e+00],\n",
      "        [ 3.0680e-01],\n",
      "        [-1.2438e+00],\n",
      "        [ 1.0428e+00],\n",
      "        [ 7.9738e-01],\n",
      "        [ 3.9865e-01],\n",
      "        [ 3.5624e-01],\n",
      "        [ 2.0215e+00],\n",
      "        [ 3.7775e-01],\n",
      "        [ 1.9530e+00],\n",
      "        [ 8.1679e+00],\n",
      "        [ 4.0448e+00],\n",
      "        [ 2.3082e+00],\n",
      "        [ 6.5556e+00],\n",
      "        [-3.6908e-01],\n",
      "        [ 3.8172e-01],\n",
      "        [-4.8839e+00],\n",
      "        [ 1.3691e+00],\n",
      "        [-9.0500e-02],\n",
      "        [ 2.5504e+00],\n",
      "        [ 3.6205e+00],\n",
      "        [-2.4846e+00],\n",
      "        [-1.6108e+00],\n",
      "        [-3.1293e+00],\n",
      "        [-2.5234e+00],\n",
      "        [ 4.9746e+00],\n",
      "        [-1.7918e+00],\n",
      "        [ 3.8015e-01],\n",
      "        [ 3.6721e+00],\n",
      "        [ 5.3469e+00],\n",
      "        [-5.0865e+00],\n",
      "        [ 3.8727e+00],\n",
      "        [ 2.0146e+00],\n",
      "        [-1.5629e+00],\n",
      "        [-2.0358e-01],\n",
      "        [-7.0311e-03],\n",
      "        [ 2.0519e+00],\n",
      "        [ 3.1053e+00],\n",
      "        [ 1.9326e-01],\n",
      "        [ 7.7603e-01],\n",
      "        [-5.6248e+00],\n",
      "        [ 3.2371e+00],\n",
      "        [-1.5229e+00],\n",
      "        [-1.5993e-01],\n",
      "        [ 8.2974e+00],\n",
      "        [ 4.4358e-01],\n",
      "        [ 1.3384e+00],\n",
      "        [ 5.3654e-01],\n",
      "        [-9.7369e-01],\n",
      "        [ 4.3267e+00],\n",
      "        [-1.1403e+00],\n",
      "        [ 7.9726e-01],\n",
      "        [-2.0670e-01],\n",
      "        [ 4.1040e+00],\n",
      "        [ 1.3691e+00],\n",
      "        [-2.3466e+00],\n",
      "        [ 3.0781e+00],\n",
      "        [-8.7476e-01],\n",
      "        [ 1.3365e+00]], grad_fn=<GatherBackward>)\n",
      "(Pdb) ll\n",
      " 83  \t    def learn(self, experiences, gamma):\n",
      " 84  \t        \"\"\"Update value parameters using given batch of experience tuples.\n",
      " 85  \t        Params\n",
      " 86  \t        ======\n",
      " 87  \t            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
      " 88  \t            gamma (float): discount factor\n",
      " 89  \t        \"\"\"\n",
      " 90  \t        states, actions, rewards, next_states, dones = experiences\n",
      " 91  \t        self.qnetwork_local.train()\n",
      " 92  \t        import pdb;pdb.set_trace()\n",
      " 93  ->\t        actions_local = torch.gather(self.qnetwork_local(states['visual'], states['not_visual']), 1, actions)\n",
      " 94  \t        best_action_local = self.qnetwork_local(next_states['visual'], next_states['not_visual']).max(dim=1)[1]\n",
      " 95  \t        actions_target = rewards + gamma * torch.gather( self.qnetwork_target(next_states['visual'], next_states['not_visual']),\n",
      " 96  \t                                                        1,\n",
      " 97  \t                                                        best_action_local.unsqueeze(1)) * (dones==0).float()\n",
      " 98  \t        self.loss = self.loss_function(actions_local, actions_target.detach())\n",
      " 99  \t        self.optimizer.zero_grad()\n",
      "100  \t        self.loss.backward()\n",
      "101  \t        self.optimizer.step()\n",
      "102  \t\n",
      "103  \t        # ------------------- update target network ------------------- #\n",
      "104  \t        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
      "(Pdb) n\n",
      "> /app/code/minerl/double_dqn_agent.py(94)learn()\n",
      "-> best_action_local = self.qnetwork_local(next_states['visual'], next_states['not_visual']).max(dim=1)[1]\n",
      "(Pdb) n\n",
      "> /app/code/minerl/double_dqn_agent.py(95)learn()\n",
      "-> actions_target = rewards + gamma * torch.gather( self.qnetwork_target(next_states['visual'], next_states['not_visual']),\n",
      "(Pdb) gamma * torch.gather( self.qnetwork_target(next_states['visual'], next_states['not_visual']),                                                         1,                                                         best_action_local.unsqueeze(1)) * (dones==0)\n",
      "*** RuntimeError: expected backend CPU and dtype Float but got backend CPU and dtype Byte\n",
      "(Pdb) gamma * torch.gather( self.qnetwork_target(next_states['visual'], next_states['not_visual']),                                                         1,                                                         best_action_local.unsqueeze(1)) * (dones==0).float()\n",
      "tensor([[-2.4991],\n",
      "        [-1.6893],\n",
      "        [-0.7402],\n",
      "        [-3.1279],\n",
      "        [-1.3121],\n",
      "        [-2.0493],\n",
      "        [-2.1067],\n",
      "        [-1.4801],\n",
      "        [-2.5689],\n",
      "        [-1.6172],\n",
      "        [ 0.3755],\n",
      "        [ 0.2277],\n",
      "        [ 1.4177],\n",
      "        [-1.4711],\n",
      "        [ 0.3168],\n",
      "        [ 0.2779],\n",
      "        [-3.1324],\n",
      "        [-1.8122],\n",
      "        [-2.7802],\n",
      "        [-0.1608],\n",
      "        [ 2.6685],\n",
      "        [-3.1233],\n",
      "        [-2.0308],\n",
      "        [-1.5590],\n",
      "        [-1.0516],\n",
      "        [-2.2464],\n",
      "        [ 0.5482],\n",
      "        [-1.8281],\n",
      "        [-2.4553],\n",
      "        [ 1.6156],\n",
      "        [-1.4616],\n",
      "        [-2.0947],\n",
      "        [-1.3686],\n",
      "        [-2.7664],\n",
      "        [-1.7554],\n",
      "        [-1.5675],\n",
      "        [-2.3280],\n",
      "        [-3.0379],\n",
      "        [-1.3793],\n",
      "        [-1.7503],\n",
      "        [-2.1683],\n",
      "        [-1.2868],\n",
      "        [-2.8439],\n",
      "        [-1.3209],\n",
      "        [-0.9237],\n",
      "        [-3.1648],\n",
      "        [-3.0080],\n",
      "        [-2.7800],\n",
      "        [-2.4845],\n",
      "        [ 0.3194],\n",
      "        [ 1.7432],\n",
      "        [-1.2474],\n",
      "        [-1.6140],\n",
      "        [-3.1925],\n",
      "        [-1.5539],\n",
      "        [ 1.3818],\n",
      "        [ 0.0957],\n",
      "        [-1.2481],\n",
      "        [ 1.4903],\n",
      "        [-1.2170],\n",
      "        [-2.4650],\n",
      "        [-2.0989],\n",
      "        [-1.0660],\n",
      "        [-2.2453]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) gamma * torch.gather( self.qnetwork_target(next_states['visual'], next_states['not_visual']),                                                         1,                                                         best_action_local.unsqueeze(1)) * ((dones==0).float())\n",
      "tensor([[-2.4991],\n",
      "        [-1.6893],\n",
      "        [-0.7402],\n",
      "        [-3.1279],\n",
      "        [-1.3121],\n",
      "        [-2.0493],\n",
      "        [-2.1067],\n",
      "        [-1.4801],\n",
      "        [-2.5689],\n",
      "        [-1.6172],\n",
      "        [ 0.3755],\n",
      "        [ 0.2277],\n",
      "        [ 1.4177],\n",
      "        [-1.4711],\n",
      "        [ 0.3168],\n",
      "        [ 0.2779],\n",
      "        [-3.1324],\n",
      "        [-1.8122],\n",
      "        [-2.7802],\n",
      "        [-0.1608],\n",
      "        [ 2.6685],\n",
      "        [-3.1233],\n",
      "        [-2.0308],\n",
      "        [-1.5590],\n",
      "        [-1.0516],\n",
      "        [-2.2464],\n",
      "        [ 0.5482],\n",
      "        [-1.8281],\n",
      "        [-2.4553],\n",
      "        [ 1.6156],\n",
      "        [-1.4616],\n",
      "        [-2.0947],\n",
      "        [-1.3686],\n",
      "        [-2.7664],\n",
      "        [-1.7554],\n",
      "        [-1.5675],\n",
      "        [-2.3280],\n",
      "        [-3.0379],\n",
      "        [-1.3793],\n",
      "        [-1.7503],\n",
      "        [-2.1683],\n",
      "        [-1.2868],\n",
      "        [-2.8439],\n",
      "        [-1.3209],\n",
      "        [-0.9237],\n",
      "        [-3.1648],\n",
      "        [-3.0080],\n",
      "        [-2.7800],\n",
      "        [-2.4845],\n",
      "        [ 0.3194],\n",
      "        [ 1.7432],\n",
      "        [-1.2474],\n",
      "        [-1.6140],\n",
      "        [-3.1925],\n",
      "        [-1.5539],\n",
      "        [ 1.3818],\n",
      "        [ 0.0957],\n",
      "        [-1.2481],\n",
      "        [ 1.4903],\n",
      "        [-1.2170],\n",
      "        [-2.4650],\n",
      "        [-2.0989],\n",
      "        [-1.0660],\n",
      "        [-2.2453]], grad_fn=<MulBackward0>)\n",
      "(Pdb) l\n",
      " 90  \t        states, actions, rewards, next_states, dones = experiences\n",
      " 91  \t        self.qnetwork_local.train()\n",
      " 92  \t        import pdb;pdb.set_trace()\n",
      " 93  \t        actions_local = torch.gather(self.qnetwork_local(states['visual'], states['not_visual']), 1, actions)\n",
      " 94  \t        best_action_local = self.qnetwork_local(next_states['visual'], next_states['not_visual']).max(dim=1)[1]\n",
      " 95  ->\t        actions_target = rewards + gamma * torch.gather( self.qnetwork_target(next_states['visual'], next_states['not_visual']),\n",
      " 96  \t                                                        1,\n",
      " 97  \t                                                        best_action_local.unsqueeze(1)) * (dones==0).float()\n",
      " 98  \t        self.loss = self.loss_function(actions_local, actions_target.detach())\n",
      " 99  \t        self.optimizer.zero_grad()\n",
      "100  \t        self.loss.backward()\n"
     ]
    }
   ],
   "source": [
    "n_episodes=2000 # maximum number of training episodes\n",
    "max_t=1000 # maximum number of timesteps per episode\n",
    "eps_start=1.0 #  starting value of epsilon, for epsilon-greedy action selection\n",
    "eps_end=0.01 # minimum value of epsilon\n",
    "eps_decay=0.995 # multiplicative factor (per episode) for decreasing epsilon\n",
    "\n",
    "\n",
    "eps = eps_start                    # initialize epsilon\n",
    "\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    print(i_episode)\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    for t in range(max_t):\n",
    "        action_dec = agent.act([state['pov'].astype(float)], state['compassAngle'], eps)\n",
    "        action = convert_action(action_dec)   \n",
    "        next_state, reward, done, info = env.step(action)     \n",
    "        agent.step(state, action_dec, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward     \n",
    "        if done:\n",
    "            break\n",
    "    scores_window.append(score)       # save most recent score\n",
    "    scores.append(score)              # save most recent score\n",
    "    eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "    print('\\rEpisode {}\\tMean Score: {:.2f}\\tMax score: {:.2f}'.format(i_episode,\n",
    "                                                                      np.mean(scores_window),\n",
    "                                                                      np.max(scores_window)))\n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tLoss: {:.2f}'.format(i_episode, \n",
    "                                                                   np.mean(scores_window),\n",
    "                                                                   agent.loss))\n",
    "    if np.mean(scores_window)>=200.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tMean score: {:.2f}\\tMax score: {:.2f}'.format(i_episode-100, np.mean(scores_window), np.max(scores_window)))\n",
    "        torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "        break\n",
    "    \n",
    "    pickle.dump(scores,open('scores.pickle','wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG (not implemented yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfdUy_YP8EAx"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \n",
    "                                                                \"action\", \n",
    "                                                                \"reward\", \n",
    "                                                                \"next_state\", \n",
    "                                                                \"done\",\n",
    "                                                                \"td_error\"])\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def add(self, states, actions, rewards, next_states, dones, td_errors):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(states, actions, rewards, next_states, dones, td_errors)\n",
    "        self.memory.append(e)\n",
    "        self.td_errors.append(max(td_errors))\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \n",
    "        td_error_probs = self.get_td_errors_probs(self.td_errors)\n",
    "        elements = np.random.choice(range(len(self.memory)), size=BATCH_SIZE, \n",
    "                                    p=td_error_probs, replace=False)\n",
    "        experiences = [self.memory[element] for element in elements]\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.stack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.stack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.stack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        td_errors = torch.from_numpy(np.stack([e.td_error for e in experiences if e is not None])).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones, td_errors)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "        \n",
    "    def get_td_errors_probs(self, td_errors):\n",
    "        td_error_probs_scaled = np.abs(td_errors)**PE_ALPHA\n",
    "        return td_error_probs_scaled / (np.abs(self.td_errors)**PE_ALPHA).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdMthHRe8EAy"
   },
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, mu=0., theta=0.2, sigma=1):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = copy.copy(self.mu)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.stat\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CShe4EU18EA0"
   },
   "outputs": [],
   "source": [
    "def soft_update(local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VXQAGY08EA1"
   },
   "outputs": [],
   "source": [
    "def layer_init(layer, w_scale=1):\n",
    "    nn.init.uniform_(layer.weight.data, a=-LAYER_INIT_RANGE, b=LAYER_INIT_RANGE)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.uniform_(layer.bias.data, a=-LAYER_INIT_RANGE, b=LAYER_INIT_RANGE)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwz4hkKK8EA3"
   },
   "outputs": [],
   "source": [
    "class QFunction(nn.Module):\n",
    "    def __init__(self, state_dim, actions_dim, hidden_units=(256, 128), \n",
    "                 activation=F.leaky_relu):\n",
    "        super(QFunction, self).__init__()\n",
    "        dims = (state_dim + actions_dim, ) + hidden_units + (1, )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.activation = activation      \n",
    "        \n",
    "    def forward(self, state, action, no_grad=False):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qguft8u98EA4"
   },
   "outputs": [],
   "source": [
    "class DeterministicPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, actions_dim, hidden_units=(256, 128) , \n",
    "                 activation=F.leaky_relu):\n",
    "        super(DeterministicPolicy, self).__init__()\n",
    "        dims = (state_dim,) + hidden_units +(actions_dim,)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.activation = activation      \n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return F.tanh( self.layers[-1](x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptJuiRHe8EA6"
   },
   "outputs": [],
   "source": [
    "def perform_an_action(state, noise):\n",
    "    state_tensor = torch.FloatTensor(state).to(device)\n",
    "    actor.eval() \n",
    "    with torch.no_grad():            \n",
    "        action = actor(state_tensor.unsqueeze(0)).squeeze().detach().cpu().numpy() + noise.sample()\n",
    "        if ACTION_CLIPPING:\n",
    "            action = np.clip(action, -1, 1)\n",
    "    actor.train()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    return state, action, reward, next_state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X63Pp-H88EA8"
   },
   "outputs": [],
   "source": [
    "# def watch_env(t_max=600, ou_sigma=0):\n",
    "#     time.sleep(0.1)\n",
    "#     env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "#     states = env_info.vector_observations / STATES_DIVISOR                # get the current state (for each agent)\n",
    "#     scores = np.zeros(num_actors)                          # initialize the score (for each agent)\n",
    "#     noise = OUNoise(size=action_size, theta=ou_sigma, sigma=OU_SIGMA_END)\n",
    "#     noise_sample = noise.sample()\n",
    "#     for i in range(t_max):\n",
    "#         states, actions, rewards, next_states, dones = perform_an_action(states, noise)\n",
    "#         scores += rewards                         # update the score (for each agent)\n",
    "#         states = next_states                               # roll over states to next time step\n",
    "#         time.sleep(0.01)\n",
    "#         if np.any(dones):                                  # exit loop if episode finished\n",
    "#             break\n",
    "#     print('Total score (averaged over actors) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYg6WRuQ8EA9"
   },
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    states_r, actions_r, rewards_r, next_states_r, dones_r, td_errors_r = replay_buffer.sample()\n",
    "\n",
    "    weights = 1/ ( replay_buffer.get_td_errors_probs(td_errors_r) * len(replay_buffer))**beta\n",
    "    for i in range(num_actors):\n",
    "        #from IPython.core.debugger import Tracer; Tracer()()\n",
    "        next_actions_all_r = torch.cat([actor_targets[j](next_states_r[:,j,:]) \\\n",
    "                                          for j in range(num_actors)], dim=1)\n",
    "        critic_target_result = critic_targets[i](adapt_c(next_states_r), \n",
    "                                                 next_actions_all_r).squeeze() * (1-dones_r[:,i])\n",
    "\n",
    "        y = (rewards_r[:,i] + critic_target_result * GAMMA) * weights[:,i].to(device)\n",
    "        y = y.detach()\n",
    "\n",
    "        # Train critic\n",
    "        critic_result = critics[i](adapt_c(states_r), adapt_c(actions_r)).squeeze()\n",
    "        critic_loss_value = F.mse_loss(critic_result, y)\n",
    "        critic_optimizers[i].zero_grad()\n",
    "        critic_loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(critics[i].parameters(), GRAD_CLIPPING)\n",
    "        critic_optimizers[i].step()\n",
    "\n",
    "        # Train the actor\n",
    "        actions_all_r = torch.cat([actors[j](states_r[:,j,:]) if (j==i) else\\\n",
    "                                   actors[j](states_r[:,j,:]).detach() for j in range(num_actors)], dim=1)\n",
    "        change_grad(critics[i], flag_set=False)\n",
    "\n",
    "        actor_loss_value = -critics[i](adapt_c(states_r), actions_all_r, no_grad=False).mean()\n",
    "        actor_optimizers[i].zero_grad()\n",
    "        actor_loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(actors[i].parameters(), GRAD_CLIPPING)\n",
    "        actor_optimizers[i].step()\n",
    "\n",
    "        change_grad(critics[i], flag_set=True)\n",
    "\n",
    "        # Update parameters\n",
    "        soft_update(actors[i], actor_targets[i], TAU)\n",
    "        soft_update(critics[i], critic_targets[i], TAU)\n",
    "\n",
    "        # Monitoring\n",
    "        actions_actor = actors[i](states_r[:,i,:])\n",
    "        actions_actor_target = actor_targets[i](states_r[:,i,:])\n",
    "#         store_monitoring(critic_target_result,critic_result, actions_actor, actions_actor_target, y, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZQGzUQQ8EA_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfYH-Hkx8EBA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpBKqgqB8EBC"
   },
   "outputs": [],
   "source": [
    "action = env.action_space.noop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attack': 0,\n",
       " 'back': 0,\n",
       " 'camera': array([0., 0.], dtype=float32),\n",
       " 'forward': 0,\n",
       " 'jump': 0,\n",
       " 'left': 0,\n",
       " 'place': 0,\n",
       " 'right': 0,\n",
       " 'sneak': 0,\n",
       " 'sprint': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QRmUYVhE8EBD",
    "outputId": "21b2cc8f-6c1e-4dec-b32d-66ee61da5fce",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward:  14.878982543945312\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCTC162y8EBG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkkCS5rP8EBJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twF67lVR8EBL",
    "outputId": "cdadaefd-a194-40cb-d4d7-ea0ebf0a2bbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(MineRLNavigateDense-v0)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkQUMFrg8EBO",
    "outputId": "3d5db205-e3f9-4ebc-bc52-a63814af049e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(compassAngle:Box(1,), inventory:Dict(dirt:Box(1,)), pov:Box(64, 64, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jv-_UY0V8EBR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "minerl-v0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
