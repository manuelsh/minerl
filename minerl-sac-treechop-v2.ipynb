{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: understand log_prob of evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'sac,treechop_v2,max_camera_turn=10,sample_hum_ratio=0.5'\n",
    "\n",
    "from importlib import reload\n",
    "import models_sac\n",
    "import utils\n",
    "reload(models_sac)\n",
    "reload(utils)\n",
    "\n",
    "from models_sac_treechop import CriticNetwork, ActorNetwork\n",
    "from utils import ReplayBuffer, seed_everything, Monitor\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import minerl\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import shutil\n",
    "\n",
    "logging.basicConfig(filename='logs/'+VERSION+'-'+time.strftime(\"%Y%m%d-%H%M%S\")+'.log', \n",
    "                    filemode='w', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = ['attack', 'jump', 'forward', 'back', 'left', 'right', 'sprint', 'sneak', 'camera_hor', 'camera_ver']\n",
    "\n",
    "SEED = 584\n",
    "ENV_NAME = 'MineRLTreechop-v0'\n",
    "OBS_DIM = 64*64*3 # pov + compassAngle\n",
    "OBS_REST = False # if there are other observations beyond POV\n",
    "BUFFER_SIZE = int(1E5)\n",
    "\n",
    "MAX_NUM_FRAMES = int(1E9)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "POV_SCALING = 255\n",
    "\n",
    "TANH_FACTOR = 1\n",
    "CAMERA_FACTOR = 1\n",
    "NUM_FILTERS = 128\n",
    "LIN_1_DIM = 512\n",
    "LIN_2_DIM = 256\n",
    "\n",
    "ACTOR_LR = 1E-5\n",
    "CRITIC_LR = 1E-5\n",
    "ALPHA_LR = 1E-5\n",
    "MAX_CAMERA_TURN = 10\n",
    "\n",
    "DYNAMIC_ALPHA = False\n",
    "Q_PREDICTS_REW = False\n",
    "\n",
    "TARGET_ENTROPY = -1\n",
    "\n",
    "GAMMA = 0.9\n",
    "BUFFER_SIZE = int(1E5)\n",
    "\n",
    "TAU = 0.001\n",
    "USE_BN = True\n",
    "\n",
    "INITIAL_ALPHA = 0.1\n",
    "\n",
    "CHECKS = True\n",
    "CHECKS_AGENT = False # calculates Q value of the agent at each step\n",
    "\n",
    "SAMPLE_HUMAN = True\n",
    "SAMPLE_HUMAN_RATIO = 0.5\n",
    "\n",
    "monitor = Monitor('monitor_'+VERSION+'.pkl', 'monitoring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLE_HUMAN:\n",
    "    human_data = minerl.data.make(ENV_NAME, data_dir='/app/code/minerl-data')\n",
    "else:\n",
    "    human_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_acts=len(ACTIONS),\n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 gamma=GAMMA,\n",
    "                 actor_learning_rate=ACTOR_LR, \n",
    "                 critic_learning_rate=CRITIC_LR,\n",
    "                 alpha_lr = ALPHA_LR,\n",
    "                 tau=TAU,\n",
    "                 initial_alpha=INITIAL_ALPHA,\n",
    "                 target_entropy=TARGET_ENTROPY,\n",
    "                 dynamic_alpha = DYNAMIC_ALPHA,\n",
    "                 sample_human = SAMPLE_HUMAN,\n",
    "                 human_data = human_data\n",
    "                 ):\n",
    "\n",
    "        self.num_acts = num_acts\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(\"Device is: \"+str(self.device))\n",
    "        print(\"Device is: \", self.device)\n",
    "        \n",
    "        self.dynamic_alpha = dynamic_alpha\n",
    "        self.gamma = gamma\n",
    "        self.actor_lr = actor_learning_rate\n",
    "        self.critic_lr = critic_learning_rate\n",
    "        self.tau = tau\n",
    "        self.buffer = ReplayBuffer( \n",
    "            obs_dim=OBS_DIM,\n",
    "            size=BUFFER_SIZE,\n",
    "            act_dim=len(ACTIONS),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "     \n",
    "        params_nn = {'acts_dim':len(ACTIONS), 'num_filters':NUM_FILTERS, \n",
    "                     'use_bn':USE_BN, 'lin_1_dim': LIN_1_DIM, 'lin_2_dim': LIN_2_DIM}\n",
    "        self.policy = ActorNetwork(**params_nn).to(self.device)\n",
    "        self.q1 = CriticNetwork(**params_nn).to(self.device)\n",
    "        self.q2 = CriticNetwork(**params_nn).to(self.device)\n",
    "        \n",
    "        if not Q_PREDICTS_REW:\n",
    "            self.target_q1 = CriticNetwork(**params_nn).to(self.device)\n",
    "            self.target_q2 = CriticNetwork(**params_nn).to(self.device)\n",
    "            self.target_q1.load_state_dict(self.q1.state_dict())\n",
    "            self.target_q2.load_state_dict(self.q2.state_dict())\n",
    "            \n",
    "        self.alpha = initial_alpha\n",
    "        if self.dynamic_alpha: self.log_alpha = nn.Parameter( torch.Tensor( [np.log(self.alpha)] ).to(self.device) )\n",
    "        \n",
    "        self.q1_optim = optim.Adam(self.q1.parameters(), lr=CRITIC_LR)\n",
    "        self.q2_optim = optim.Adam(self.q2.parameters(), lr=CRITIC_LR)\n",
    "        self.policy_optim = optim.Adam(self.policy.parameters(), lr=ACTOR_LR)\n",
    "        if self.dynamic_alpha: self.alpha_optim = optim.Adam([self.log_alpha], lr=ALPHA_LR)\n",
    "        self.target_entropy = target_entropy\n",
    "        self.sample_human = sample_human\n",
    "        if self.sample_human:\n",
    "            self.human_data_iter = human_data.sarsd_iter( max_sequence_len=batch_size, seed = SEED )\n",
    "\n",
    "    def get_act(self, obs):\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            act = self.policy.get_action(obs)\n",
    "        self.policy.train()    \n",
    "        return act#.cpu().numpy()\n",
    "        \n",
    "    def unflatten_obs(self, flat_obs):\n",
    "        if OBS_REST:\n",
    "            return (flat_obs[:,:-1].reshape(-1,64,64,3), flat_obs[:,-1].reshape(-1,1))\n",
    "        else:\n",
    "            return flat_obs.reshape(-1,64,64,3)\n",
    "    \n",
    "        \n",
    "    def flatten_obs(self, obs):\n",
    "        if OBS_REST:\n",
    "            return np.append(obs['pov'].reshape(-1), obs['compassAngle'])\n",
    "        else:\n",
    "            return obs.reshape(-1)\n",
    "\n",
    "    # Store the transition into the replay buffer\n",
    "    def store_transition(self, obs, next_obs, act, rew, done):\n",
    "        obs = self.flatten_obs(obs)\n",
    "        next_obs = self.flatten_obs(next_obs)\n",
    "        self.buffer.store(obs=obs, act=act, rew=rew, \n",
    "                          next_obs=next_obs, done=done)\n",
    "\n",
    "    def float_tensor(self, numpy_array):\n",
    "        return torch.FloatTensor(numpy_array).to(self.device)\n",
    "        \n",
    "    def sample_batch(self):\n",
    "        \n",
    "        if self.sample_human & (random.random() < SAMPLE_HUMAN_RATIO):\n",
    "            obss_h, acts_h, rews_h, next_obss_h, dones_h = next(self.human_data_iter)\n",
    "            obss = self.float_tensor( obss_h['pov'] )\n",
    "            acts_ex = np.stack([acts_h[action] for action in ACTIONS[0:8] ], axis = 1)*2-1 # all acts except the camera ones. Transformation is due to the tanh.\n",
    "            act_cam = np.clip(acts_h['camera']/MAX_CAMERA_TURN, -1, +1)\n",
    "            acts =self.float_tensor( np.append( acts_ex, act_cam, axis=1) )\n",
    "            rews = self.float_tensor( rews_h )\n",
    "            next_obss = self.float_tensor( next_obss_h['pov'] )\n",
    "            dones = self.float_tensor(dones_h[:-1])\n",
    "        else:\n",
    "            transitions = self.buffer.sample_batch()      \n",
    "            obss = self.unflatten_obs( self.float_tensor(transitions['obs']) )\n",
    "            next_obss = self.unflatten_obs( self.float_tensor(transitions['next_obs']) )\n",
    "            acts = self.float_tensor(transitions['acts'])\n",
    "            rews = self.float_tensor(transitions['rews'])\n",
    "            dones = self.float_tensor(transitions['dones'])\n",
    "        return obss, next_obss, acts, rews, dones\n",
    "    \n",
    "    def fit_batch(self):\n",
    "        # Sample\n",
    "        obss, next_obss, acts, rews, dones = self.sample_batch()\n",
    "\n",
    "        # Q function loss\n",
    "        with torch.no_grad():\n",
    "            next_log_prob, next_action = self.policy.get_log_probs(next_obss)\n",
    "            if not Q_PREDICTS_REW:\n",
    "                target_q1_next = self.target_q1(next_obss, next_action).view(-1)\n",
    "                target_q2_next = self.target_q2(next_obss, next_action).view(-1)\n",
    "                min_q_target_hat = torch.min(target_q1_next, target_q2_next) - self.alpha * next_log_prob.view(-1) \n",
    "                y = rews + (1 - dones) * self.gamma * min_q_target_hat\n",
    "            else:\n",
    "                y = rews\n",
    " \n",
    "        q1_hat = self.q1( obss , acts ).view(-1)  \n",
    "        q2_hat = self.q2( obss , acts ).view(-1)  \n",
    "        q1_loss = F.mse_loss(q1_hat, y) \n",
    "        q2_loss = F.mse_loss(q2_hat, y)\n",
    "        #assert (float(q1_loss)<100) or (float(q2_loss)<100)\n",
    "        \n",
    "        # Policy loss\n",
    "        log_pi, pi = self.policy.get_log_probs(obss)\n",
    "        \n",
    "        q1_hat_policy = self.q1( obss, pi)\n",
    "        q2_hat_policy = self.q2( obss, pi)\n",
    "        min_q_pi = torch.min(q1_hat_policy, q2_hat_policy)\n",
    "        policy_loss = (self.alpha * log_pi - min_q_pi).mean()\n",
    "        \n",
    "        if CHECKS:\n",
    "            #policy_prev_params = copy.deepcopy( list(self.policy.parameters()))\n",
    "            y_mean = float( y.mean() )\n",
    "            monitor.add(['next_log_prob_mean', 'min_q_target_hat_mean',\n",
    "                         'alpha','y_mean', \n",
    "                         'log_pi', 'policy_mean_mean','policy_log_std_mean', 'num_rewards_training'],\n",
    "           [float(next_log_prob.mean()),  float(min_q_target_hat.mean()), \n",
    "            float(self.alpha), y_mean, float(log_pi.mean()),\n",
    "            float(self.policy.mean.mean()), float(self.policy.log_std.mean()), float(sum(rews))])\n",
    "            assert y_mean == y_mean, \"Y is NaN!\"\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.q1_optim.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0, norm_type=1)\n",
    "        self.q1_optim.step()\n",
    "        \n",
    "        self.q2_optim.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0, norm_type=1)\n",
    "        self.q2_optim.step()\n",
    "        \n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0, norm_type=1)\n",
    "        self.policy_optim.step()\n",
    "        \n",
    "        if CHECKS:\n",
    "            check_nan_policy = np.array([float(i.mean()) for i in self.policy.parameters()]).mean()\n",
    "            assert check_nan_policy == check_nan_policy\n",
    "            check_nan_q1 = np.array([float(i.mean()) for i in self.q1.parameters()]).mean()\n",
    "            assert check_nan_q1 == check_nan_q1\n",
    "            check_nan_q2 = np.array([float(i.mean()) for i in self.q2.parameters()]).mean()\n",
    "            assert check_nan_q2 == check_nan_q2\n",
    "        \n",
    "        # Alpha parameter tuning\n",
    "        if self.dynamic_alpha:\n",
    "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "\n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(self.log_alpha.parameters(), 1.0, norm_type=1)\n",
    "            self.alpha_optim.step()\n",
    "\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        return q1_loss, q2_loss, policy_loss, q1_hat, q2_hat\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        self.polyak_averaging(self.target_q1, self.q1)\n",
    "        self.polyak_averaging(self.target_q2, self.q2)\n",
    "        \n",
    "    def polyak_averaging(self, target, original):\n",
    "        for target_param, param in zip(target.parameters(), original.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + target_param.data * (1.0 - self.tau))\n",
    "            \n",
    "    def get_env_act(self, model_act):\n",
    "        '''\n",
    "        Gets environment act from model act\n",
    "        '''\n",
    "#         env_act = {'attack':0, 'jump':1, 'forward':1, 'back':0, 'left':0, 'right':0, 'sprint':0, 'sneak':0, 'camera':[0, model_act[0]*180]}\n",
    "        env_act = {act: int(value>0) for act, value in zip(ACTIONS[0:8], model_act[0:8])}\n",
    "        env_act['camera'] = [model_act[8]*MAX_CAMERA_TURN, model_act[9]*MAX_CAMERA_TURN]\n",
    "        return env_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 255 ms, sys: 328 ms, total: 584 ms\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "if 'env' not in locals():\n",
    "    %time env = gym.make(ENV_NAME)\n",
    "\n",
    "seed_everything(seed=SEED, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:  cuda\n",
      "Number of trainable parameters actor: 3642004\n",
      "Number of trainable parameters critic: 7284482\n",
      "Number of trainable parameters alpha: Unknown\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "# number of trainable parameters of model\n",
    "message_1 = f'Number of trainable parameters actor: {str(sum(p.numel() for p in agent.policy.parameters()  if p.requires_grad))}'\n",
    "message_2 = f'Number of trainable parameters critic: {str(sum(p.numel() for p in agent.q1.parameters()  if p.requires_grad)*2)}'\n",
    "message_3 = 'Number of trainable parameters alpha:'+' Unknown'\n",
    "print(message_1)\n",
    "print(message_2)\n",
    "print(message_3)\n",
    "\n",
    "logging.info(message_1)\n",
    "logging.info(message_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent\n",
    "# agent.policy.load_state_dict(torch.load('trained_models/sac_policy'+VERSION+'.pkl'))\n",
    "#agent.critic.load_state_dict(torch.load('trained_models/ddpg_critic_'+VERSION+'.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 ms, sys: 4.61 ms, total: 18.1 ms\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%time obs = env.reset()\n",
    "assert (agent.unflatten_obs( np.array([agent.flatten_obs(obs['pov'])]) )[0].astype(int) == obs['pov']).sum() == 3*64*64\n",
    "trajectory_count = 1\n",
    "losses_agent = []\n",
    "losses_critic = []\n",
    "scores = []\n",
    "current_score = []\n",
    "current_scores = []\n",
    "max_parameters = []\n",
    "plotting_interval = 100\n",
    "score = 0\n",
    "logging.info('Environment reset')\n",
    "\n",
    "q1_values_mean=[]\n",
    "q2_values_mean=[]\n",
    "losses_agent=[]\n",
    "losses_q1=[]\n",
    "losses_q2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relus_are_alive():\n",
    "    for network in [agent.q1, agent.q2, agent.policy]:\n",
    "        if float(network.non_lin_1.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_1 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "        if float(network.non_lin_2.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_2 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "        if float(network.non_lin_3.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_3 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "                       \n",
    "# test\n",
    "# agent.actor.x_relu_1[agent.actor.x_relu_1!=0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame:   0%|          | 1187/1000000000 [07:00<69007:47:28,  4.03it/s] "
     ]
    }
   ],
   "source": [
    "for frame_idx in tqdm(range(MAX_NUM_FRAMES), desc='frame'):\n",
    "    obs_tensor = agent.float_tensor([obs['pov'].astype(float)])\n",
    "    model_act = agent.get_act(obs_tensor)\n",
    "    model_act_np = model_act.cpu().numpy()[0] # act returned by the model in numpy\n",
    "    env_act = agent.get_env_act(model_act_np)\n",
    "    next_obs, rew, done, info = env.step(env_act)\n",
    "    \n",
    "    # Monitor behaviour of agent for checks\n",
    "    if CHECKS_AGENT:\n",
    "        with torch.no_grad():\n",
    "            new_q = agent.q1(obs_tensor, model_act)\n",
    "            monitor.add('q_agent', float(new_q))\n",
    "    monitor.add([ 'camera_hor_agent','rew_agent'], \n",
    "                [env_act['camera'][1], rew])\n",
    "    \n",
    "    # Store the transition in the replay buffer of the agent\n",
    "    agent.store_transition(obs=obs['pov'], next_obs=next_obs['pov'],\n",
    "                               act=model_act_np, done=done, rew=rew)\n",
    "    \n",
    "    # Prepare for next step and store scores\n",
    "    obs = next_obs\n",
    "    score += rew\n",
    "    monitor.add(f'score_{trajectory_count}', score)\n",
    "    monitor.add(ACTIONS, model_act[0])\n",
    "    monitor.add(['agent_mean', 'agent_log_std'], [float(agent.policy.mean.mean()), float(agent.policy.log_std.mean())])\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        score = 0\n",
    "        last_score = monitor.data[f'score_{trajectory_count}'][-1]\n",
    "        trajectory_count += 1\n",
    "        \n",
    "        # Save model and log\n",
    "        torch.save(agent.policy.state_dict(), 'trained_models/sac_policy'+VERSION+'.pkl')\n",
    "        torch.save(agent.q1.state_dict(), 'trained_models/sac_q1'+VERSION+'.pkl')\n",
    "        torch.save(agent.q2.state_dict(), 'trained_models/sac_q2'+VERSION+'.pkl')\n",
    "        logging.info(f'Trajectory {len(current_scores)} done, with final score {last_score}')\n",
    "        \n",
    "    # TRAIN\n",
    "    if len(agent.buffer) >= agent.batch_size:\n",
    "        q1_loss, q2_loss, policy_loss, q1_hat, q2_hat = agent.fit_batch()\n",
    "        # agent.update_target_networks()\n",
    "        q1_hat_mean = float(q1_hat.mean())\n",
    "        q2_hat_mean = float(q2_hat.mean())\n",
    "        monitor.add(['q1_hat.mean', 'q2_hat.mean', 'loss_agent', 'loss_q1', 'loss_q2'],\n",
    "                    [q1_hat_mean, q2_hat_mean, float(policy_loss),\n",
    "                    float(q1_loss), float(q2_loss) ])\n",
    "        \n",
    "        #assert (q1_hat_mean==q1_hat_mean) or (q2_hat_mean==q2_hat_mean), \"At least one q function returns NaN!\"\n",
    "\n",
    "    if (frame_idx+1) % plotting_interval == 0:\n",
    "        #check_relus_are_alive()\n",
    "        #clear_output(True)\n",
    "        #monitor.plot_all()\n",
    "        monitor.save()\n",
    "        shutil.copyfile(f'monitoring/monitor_{VERSION}.pkl', f'monitoring/monitor_{VERSION}_copy.pkl')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = agent\n",
    "obss_h, acts_h, rews_h, next_obss_h, dones_h = next(self.human_data_iter)\n",
    "obss = self.float_tensor( obss_h['pov'] )\n",
    "acts_ex = np.stack([acts_h[action] for action in ACTIONS[0:8] ], axis = 1)*2 -1 # all acts except the camera ones\n",
    "acts =self.float_tensor( np.append( acts_ex, acts_h['camera']/POV_SCALING, axis=1) )\n",
    "rews = self.float_tensor( rews_h )\n",
    "next_obss = self.float_tensor( next_obss_h['pov'] )\n",
    "dones = self.float_tensor(dones_h)\n",
    "sum(rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "net_reward = 0\n",
    "actions = []\n",
    "score = 0\n",
    "current_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    obs_pov = agent.float_tensor([obs['pov'].astype(float)])\n",
    "    #obs_rest = agent.float_tensor([[obs['compassAngle']]])\n",
    "    model_act = agent.get_act(obs_pov).cpu().numpy()\n",
    "    env_act = agent.get_env_act(model_act[0])\n",
    "    next_obs, rew, done, info = env.step(env_act)\n",
    "\n",
    "    # Prepare for next step and store scores\n",
    "    obs = next_obs\n",
    "    score += rew\n",
    "    current_score.append(score)\n",
    "    \n",
    "#     if i%10==0:\n",
    "    plt.imshow(env.render(mode='rgb_array')) \n",
    "    display.display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "    net_reward += rew\n",
    "    actions.append((env_act, net_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    env_act= {'attack':0, \n",
    "               'jump':1, \n",
    "               'forward':1, \n",
    "               'back':0, \n",
    "               'left':0, \n",
    "               'right':0, \n",
    "               'sprint':0, \n",
    "               'sneak':0, \n",
    "               'camera':[0,  0.03*obs[\"compassAngle\"]]}\n",
    "    next_obs, rew, done, info = env.step(env_act)\n",
    "    \n",
    "    # Prepare for next step and store scores\n",
    "    plt.imshow(env.render(mode='rgb_array'))     \n",
    "    display.display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "    print('compassAngle:',obs['compassAngle'])\n",
    "    print('next compassAngle:', next_obs['compassAngle'])\n",
    "    obs = next_obs\n",
    "    score += rew\n",
    "    current_score.append(score)\n",
    "    \n",
    "    net_reward += rew\n",
    "    actions.append((env_act, net_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
