{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Rainbow\n",
    "\n",
    "[M. Hessel et al., \"Rainbow: Combining Improvements in Deep Reinforcement Learning.\" arXiv preprint arXiv:1710.02298, 2017.](https://arxiv.org/pdf/1710.02298.pdf)\n",
    "\n",
    "We will integrate all the following seven components into a single integrated agent, which is called Rainbow!\n",
    "\n",
    "1. DQN\n",
    "2. Double DQN\n",
    "3. Prioritized Experience Replay\n",
    "4. Dueling Network\n",
    "5. Noisy Network\n",
    "6. Categorical DQN\n",
    "7. N-step Learning\n",
    "\n",
    "This method shows an impressive performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. \n",
    "\n",
    "![rainbow](https://user-images.githubusercontent.com/14961526/60591412-61748100-9dd9-11e9-84fb-076c7a61fbab.png)\n",
    "\n",
    "However, the integration is not so simple because some of components are not independent each other, so we will look into a number of points that people especailly feel confused.\n",
    "\n",
    "1. Noisy Network <-> Dueling Network\n",
    "2. Dueling Network <-> Categorical DQN\n",
    "3. Categorical DQN <-> Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import deque, OrderedDict\n",
    "from typing import Deque, Dict, List, Tuple\n",
    "\n",
    "import minerl\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from segment_tree import MinSegmentTree, SumSegmentTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_stats() missing 1 required positional argument: 'losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-74bdaabebf98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: plot_stats() missing 1 required positional argument: 'losses'"
     ]
    }
   ],
   "source": [
    "plot_stats(frame_idx, scores, current_score, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Same as the basic N-step buffer. \n",
    "\n",
    "(Please see *01.dqn.ipynb*, *07.n_step_learning.ipynb* for detailed description about the basic (n-step) replay buffer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim: int, \n",
    "        size: int, \n",
    "        batch_size: int = 32, \n",
    "        n_step: int = 1, \n",
    "        gamma: float = 0.99\n",
    "    ):\n",
    "        \n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "        \n",
    "        # for N-step Learning\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def store(\n",
    "        self, \n",
    "        obs: np.ndarray, \n",
    "        act: np.ndarray, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
    "        \n",
    "        transition = (obs, act, rew, next_obs, done)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "        # single step transition is not ready\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return ()\n",
    "        \n",
    "        # make a n-step transition\n",
    "        rew, next_obs, done = self._get_n_step_info(\n",
    "            self.n_step_buffer, self.gamma\n",
    "        )\n",
    "        obs, act = self.n_step_buffer[0][:2]\n",
    "        \n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "        return self.n_step_buffer[0]\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "            # for N-step Learning\n",
    "            indices=indices,\n",
    "        )\n",
    "    \n",
    "    def sample_batch_from_idxs(\n",
    "        self, idxs: np.ndarray\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        # for N-step Learning\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "        )\n",
    "    \n",
    "    def _get_n_step_info(\n",
    "        self, n_step_buffer: Deque, gamma: float\n",
    "    ) -> Tuple[np.int64, np.ndarray, bool]:\n",
    "        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n",
    "        # info of the last transition\n",
    "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
    "\n",
    "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
    "            r, n_o, d = transition[-3:]\n",
    "\n",
    "            rew = r + gamma * rew * (1 - d)\n",
    "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
    "\n",
    "        return rew, next_obs, done\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized replay Buffer\n",
    "\n",
    "`store` method returns boolean in order to inform if a N-step transition has been generated.\n",
    "\n",
    "(Please see *02.per.ipynb* for detailed description about PER.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    \"\"\"Prioritized Replay buffer.\n",
    "    \n",
    "    Attributes:\n",
    "        max_priority (float): max priority\n",
    "        tree_ptr (int): next index of tree\n",
    "        alpha (float): alpha parameter for prioritized replay buffer\n",
    "        sum_tree (SumSegmentTree): sum tree for prior\n",
    "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim: int, \n",
    "        size: int, \n",
    "        batch_size: int = 32, \n",
    "        alpha: float = 0.6,\n",
    "        n_step: int = 1, \n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        assert alpha >= 0\n",
    "        \n",
    "        super(PrioritizedReplayBuffer, self).__init__(\n",
    "            obs_dim, size, batch_size, n_step, gamma\n",
    "        )\n",
    "        self.max_priority, self.tree_ptr = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "        \n",
    "    def store(\n",
    "        self, \n",
    "        obs: np.ndarray, \n",
    "        act: int, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n",
    "        \"\"\"Store experience and priority.\"\"\"\n",
    "        transition = super().store(obs, act, rew, next_obs, done)\n",
    "        \n",
    "        if transition:\n",
    "            self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "            self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
    "        \n",
    "        return transition\n",
    "\n",
    "    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "        \n",
    "        indices = self._sample_proportional()\n",
    "        \n",
    "        obs = self.obs_buf[indices]\n",
    "        next_obs = self.next_obs_buf[indices]\n",
    "        acts = self.acts_buf[indices]\n",
    "        rews = self.rews_buf[indices]\n",
    "        done = self.done_buf[indices]\n",
    "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "        \n",
    "        return dict(\n",
    "            obs=obs,\n",
    "            next_obs=next_obs,\n",
    "            acts=acts,\n",
    "            rews=rews,\n",
    "            done=done,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "        \n",
    "    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n",
    "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority ** self.alpha\n",
    "            self.min_tree[idx] = priority ** self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "            \n",
    "    def _sample_proportional(self) -> List[int]:\n",
    "        \"\"\"Sample indices based on proportions.\"\"\"\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
    "        segment = p_total / self.batch_size\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def _calculate_weight(self, idx: int, beta: float):\n",
    "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "        \n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Layer\n",
    "\n",
    "Please see *05.noisy_net.ipynb* for detailed description.\n",
    "\n",
    "**References:**\n",
    "\n",
    "- https://github.com/higgsfield/RL-Adventure/blob/master/5.noisy%20dqn.ipynb\n",
    "- https://github.com/Kaixhin/Rainbow/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy linear module for NoisyNet.\n",
    "           \n",
    "    Attributes:\n",
    "        in_features (int): input size of linear module\n",
    "        out_features (int): output size of linear module\n",
    "        std_init (float): initial std value\n",
    "        weight_mu (nn.Parameter): mean value weight parameter\n",
    "        weight_sigma (nn.Parameter): std value weight parameter\n",
    "        bias_mu (nn.Parameter): mean value bias parameter\n",
    "        bias_sigma (nn.Parameter): std value bias parameter\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        std_init: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(\n",
    "            self.std_init / math.sqrt(self.in_features)\n",
    "        )\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(\n",
    "            self.std_init / math.sqrt(self.out_features)\n",
    "        )\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Make new noise.\"\"\"\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "\n",
    "        # outer product\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\n",
    "        \n",
    "        We don't use separate statements on train / eval mode.\n",
    "        It doesn't show remarkable difference of performance.\n",
    "        \"\"\"\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
    "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def scale_noise(size: int) -> torch.Tensor:\n",
    "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
    "        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=1.0, size=size))\n",
    "\n",
    "        return x.sign().mul(x.abs().sqrt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NoisyNet + DuelingNet + Categorical DQN\n",
    "\n",
    "#### NoisyNet + DuelingNet\n",
    "\n",
    "NoisyLinear is employed for the last two layers of advantage and value layers. The noise should be reset at evey update step.\n",
    "\n",
    "#### DuelingNet + Categorical DQN\n",
    "\n",
    "The dueling network architecture is adapted for use with return distributions. The network has a shared representation, which is then fed into a value stream with atom_size outputs, and into an advantage stream with atom_size × out_dim outputs. For each atom, the value and advantage streams are aggregated, as in dueling DQN, and then passed through a softmax layer to obtain the normalized parametric distributions used to estimate the returns’ distributions.\n",
    "\n",
    "```\n",
    "        advantage = self.advantage_layer(adv_hid).view(-1, self.out_dim, self.atom_size)\n",
    "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
    "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        dist = F.softmax(q_atoms, dim=-1)\n",
    "```\n",
    "\n",
    "(Please see *04.dueling.ipynb*, *05.noisy_net.ipynb*, *06.categorical_dqn.ipynb* for detailed description of each component's network architecture.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        out_dim: int, \n",
    "        atom_size: int,\n",
    "        non_visual_state_dim: int,\n",
    "        support: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.support = support\n",
    "        self.out_dim = out_dim\n",
    "        self.atom_size = atom_size\n",
    "\n",
    "        # set common feature layer\n",
    "        self.feature_layer_1 = nn.Sequential(\n",
    "#             nn.Linear(in_dim, 128), \n",
    "#             nn.ReLU(),\n",
    "              nn.Conv2d(3,32, kernel_size=8, stride=4),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(32,64, kernel_size=4, stride=2),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(64,64, kernel_size=3, stride=1),\n",
    "              nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.feature_layer_2 = nn.Sequential( \n",
    "              nn.Linear(1024+non_visual_state_dim, 512),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(512, 128),\n",
    "              nn.ReLU()\n",
    "        )\n",
    "\n",
    "        \n",
    "        # set advantage layer\n",
    "        self.advantage_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.advantage_layer = NoisyLinear(128, out_dim * atom_size)\n",
    "\n",
    "        # set value layer\n",
    "        self.value_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.value_layer = NoisyLinear(128, atom_size)\n",
    "\n",
    "    def forward(self, x_visual: torch.Tensor, x_not_visual: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        dist = self.dist(x_visual, x_not_visual)\n",
    "        q = torch.sum(dist * self.support, dim=2)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def dist(self,  x_visual: torch.Tensor, x_not_visual: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get distribution for atoms.\"\"\"\n",
    "        r = self.feature_layer_1(x_visual.permute(0,3,1,2))\n",
    "        feature = self.feature_layer_2(torch.cat([ r.view(r.shape[0], -1),  x_not_visual.view(-1,1) ], dim=1 ))\n",
    "        adv_hid = F.relu(self.advantage_hidden_layer(feature))\n",
    "        val_hid = F.relu(self.value_hidden_layer(feature))\n",
    "        \n",
    "        advantage = self.advantage_layer(adv_hid).view(\n",
    "            -1, self.out_dim, self.atom_size\n",
    "        )\n",
    "        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)\n",
    "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        dist = F.softmax(q_atoms, dim=-1)\n",
    "        \n",
    "        return dist\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset all noisy layers.\"\"\"\n",
    "        self.advantage_hidden_layer.reset_noise()\n",
    "        self.advantage_layer.reset_noise()\n",
    "        self.value_hidden_layer.reset_noise()\n",
    "        self.value_layer.reset_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rainbow Agent\n",
    "\n",
    "Here is a summary of DQNAgent class.\n",
    "\n",
    "| Method           | Note                                                 |\n",
    "| ---              | ---                                                  |\n",
    "|select_action     | select an action from the input state.               |\n",
    "|step              | take an action and return the response of the env.   |\n",
    "|compute_dqn_loss  | return dqn loss.                                     |\n",
    "|update_model      | update the model by gradient descent.                |\n",
    "|target_hard_update| hard update from the local model to the target model.|\n",
    "|train             | train the agent during num_frames.                   |\n",
    "|test              | test the agent (1 episode).                          |\n",
    "|plot              | plot the training progresses.                        |\n",
    "\n",
    "#### Categorical DQN + Double DQN\n",
    "\n",
    "The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Here, we use `self.dqn` instead of `self.dqn_target` to obtain the target actions.\n",
    "\n",
    "```\n",
    "        # Categorical DQN + Double DQN\n",
    "        # target_dqn is used when we don't employ double DQN\n",
    "        next_action = self.dqn(next_state).argmax(1)\n",
    "        next_dist = self.dqn_target.dist(next_state)\n",
    "        next_dist = next_dist[range(self.batch_size), next_action]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['', 'locals', 'locals()'],\n",
       " '_oh': {1: <function locals()>},\n",
       " '_dh': ['/app/code/minerl'],\n",
       " 'In': ['', 'locals', 'locals()'],\n",
       " 'Out': {1: <function locals()>},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f61d6068358>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f61d5e402e8>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f61d5e402e8>,\n",
       " '_': <function locals()>,\n",
       " '__': '',\n",
       " '___': '',\n",
       " '_i': 'locals',\n",
       " '_ii': '',\n",
       " '_iii': '',\n",
       " '_i1': 'locals',\n",
       " '_1': <function locals()>,\n",
       " '_i2': 'locals()'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "    \n",
    "    Attribute:\n",
    "        env (gym.Env): openAI Gym environment\n",
    "        memory (PrioritizedReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        target_update (int): period for target model's hard update\n",
    "        gamma (float): discount factor\n",
    "        dqn (Network): model to train and select actions\n",
    "        dqn_target (Network): target model to update\n",
    "        optimizer (torch.optim): optimizer for training dqn\n",
    "        transition (list): transition information including \n",
    "                           state, action, reward, next_state, done\n",
    "        v_min (float): min value of support\n",
    "        v_max (float): max value of support\n",
    "        atom_size (int): the unit number of support\n",
    "        support (torch.Tensor): support for categorical dqn\n",
    "        use_n_step (bool): whether to use n_step memory\n",
    "        n_step (int): step number to calculate n-step td error\n",
    "        memory_n (ReplayBuffer): n-step replay buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        obs_dim: int,\n",
    "        non_visual_state_dim: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "        # PER parameters\n",
    "        alpha: float = 0.6,\n",
    "        beta: float = 0.4,\n",
    "        prior_eps: float = 1e-6,\n",
    "        # Categorical DQN parameters\n",
    "        v_min: float = -30.0,\n",
    "        v_max: float = 180.0,\n",
    "        atom_size: int = 51,\n",
    "        # N-step Learning\n",
    "        n_step: int = 3,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "        \n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            target_update (int): period for target model's hard update\n",
    "            lr (float): learning rate\n",
    "            gamma (float): discount factor\n",
    "            alpha (float): determines how much prioritization is used\n",
    "            beta (float): determines how much importance sampling is used\n",
    "            prior_eps (float): guarantees every transition can be sampled\n",
    "            v_min (float): min value of support\n",
    "            v_max (float): max value of support\n",
    "            atom_size (int): the unit number of support\n",
    "            n_step (int): step number to calculate n-step td error\n",
    "        \"\"\"\n",
    "        obs_dim = obs_dim\n",
    "        # action_dim = env.action_space.n\n",
    "        \n",
    "        actions_dict = OrderedDict({#'attack', \n",
    "           #'back', \n",
    "           'camera_left' : {'camera': [ 0, -20]},\n",
    "           'camera_right' : {'camera': [ 0, 20]},\n",
    "           'camera_left_little' : {'camera': [ 0, -5]}, \n",
    "           'camera_right_little' : {'camera': [ 0, 5]},\n",
    "           'forward': {'forward': 1}, \n",
    "           #'jump', \n",
    "           'jump+forward' : {'forward': 1, 'jump':1},\n",
    "           'left' : {'left':1},\n",
    "           #'place',\n",
    "           'right' : {'right':1}, \n",
    "           'sneak+forward' : {'sneak': 1, 'jump':1},\n",
    "           'wait': {}})\n",
    "           #'sprint'\n",
    "            \n",
    "        self.actions_items = list(actions_dict.items())\n",
    "        action_dim = len(actions_dict) # we interpret camera yaw as discrete\n",
    "        \n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.gamma = gamma\n",
    "        # NoisyNet: All attributes related to epsilon are removed\n",
    "        \n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "        \n",
    "        # PER\n",
    "        # memory for 1-step Learning\n",
    "        self.beta = beta\n",
    "        self.prior_eps = prior_eps\n",
    "        self.memory = PrioritizedReplayBuffer(\n",
    "            obs_dim, memory_size, batch_size, alpha=alpha\n",
    "        )\n",
    "        \n",
    "        # memory for N-step Learning\n",
    "        self.use_n_step = True if n_step > 1 else False\n",
    "        if self.use_n_step:\n",
    "            self.n_step = n_step\n",
    "            self.memory_n = ReplayBuffer(\n",
    "                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma\n",
    "            )\n",
    "            \n",
    "        # Categorical DQN parameters\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.atom_size = atom_size\n",
    "        self.support = torch.linspace(\n",
    "            self.v_min, self.v_max, self.atom_size\n",
    "        ).to(self.device)\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network(\n",
    "            action_dim, self.atom_size,non_visual_state_dim, self.support\n",
    "        ).to(self.device)\n",
    "        self.dqn_target = Network(\n",
    "            action_dim, self.atom_size,non_visual_state_dim, self.support\n",
    "        ).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "        \n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "        self.scores = None\n",
    "        \n",
    "\n",
    "    def flatten_state(self, state):\n",
    "        return np.append(state['pov'].reshape(-1), state['compassAngle'])\n",
    "\n",
    "    def unflatten_state(self, flat_state):\n",
    "        return (flat_state[:,:-1].reshape(-1,64,64,3), flat_state[:,-1])\n",
    "    \n",
    "    def convert_action(self, action_number):\n",
    "        return self.actions_items[action_number][1]\n",
    "        \n",
    "    def select_action(self, raw_state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # NoisyNet: no epsilon greedy action selection\n",
    "        selected_action = self.dqn(\n",
    "            self.float_tensor( [raw_state['pov'].astype(float)] ),\n",
    "            self.float_tensor( [raw_state['compassAngle']] )\n",
    "        ).argmax()\n",
    "        selected_action = selected_action.detach().cpu().numpy()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition = [ self.flatten_state(raw_state), selected_action ]\n",
    "        \n",
    "        return selected_action\n",
    "    \n",
    "\n",
    "    \n",
    "    def step(self, action_number: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        action_dict = self.convert_action(action_number)\n",
    "        next_state, reward, done, _ = self.env.step(action_dict)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, self.flatten_state(next_state), done]\n",
    "            \n",
    "            # N-step transition\n",
    "            if self.use_n_step:\n",
    "                one_step_transition = self.memory_n.store(*self.transition)\n",
    "            # 1-step transition\n",
    "            else:\n",
    "                one_step_transition = self.transition\n",
    "\n",
    "            # add a single step transition\n",
    "            if one_step_transition:\n",
    "                self.memory.store(*one_step_transition)\n",
    "    \n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        # PER needs beta to calculate weights\n",
    "        samples = self.memory.sample_batch(self.beta)\n",
    "        weights = self.float_tensor( samples[\"weights\"].reshape(-1, 1) )\n",
    "        indices = samples[\"indices\"]\n",
    "        \n",
    "        # 1-step Learning loss\n",
    "        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n",
    "        \n",
    "        # PER: importance sampling before average\n",
    "        loss = torch.mean(elementwise_loss * weights)\n",
    "        \n",
    "        # N-step Learning loss\n",
    "        # we are gonna combine 1-step loss and n-step loss so as to\n",
    "        # prevent high-variance. The original rainbow employs n-step loss only.\n",
    "        if self.use_n_step:\n",
    "            gamma = self.gamma ** self.n_step\n",
    "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
    "            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n",
    "            elementwise_loss += elementwise_loss_n_loss\n",
    "            \n",
    "            # PER: importance sampling before average\n",
    "            loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        # https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_\n",
    "        clip_grad_norm_(self.dqn.parameters(), 1.0, norm_type=1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # PER: update priorities\n",
    "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
    "        new_priorities = loss_for_prior + self.prior_eps\n",
    "        self.memory.update_priorities(indices, new_priorities)\n",
    "        \n",
    "        # NoisyNet: reset noise\n",
    "        self.dqn.reset_noise()\n",
    "        self.dqn_target.reset_noise()\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        update_cnt = 0\n",
    "        losses = []\n",
    "        self.scores = []\n",
    "        self.current_score = []\n",
    "        score = 0\n",
    "\n",
    "        for frame_idx in range(1, num_frames + 1):\n",
    "            action = self.select_action(state)\n",
    "            \n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            self.current_score.append(score)\n",
    "            \n",
    "            # NoisyNet: removed decrease of epsilon\n",
    "            \n",
    "            # PER: increase beta\n",
    "            fraction = min(frame_idx / num_frames, 1.0)\n",
    "            self.beta = self.beta + fraction * (1.0 - self.beta)\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                self.current_score = []\n",
    "                self.scores.append(score)\n",
    "                score = 0\n",
    "\n",
    "            # if training is ready\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "                \n",
    "                # if hard update is needed\n",
    "                if update_cnt % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "\n",
    "            # plotting\n",
    "            if frame_idx % plotting_interval == 0:\n",
    "                self._plot(frame_idx, self.scores,self.current_score, losses)\n",
    "                \n",
    "        self.env.close()\n",
    "                \n",
    "    def test(self) -> List[np.ndarray]:\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "        \n",
    "        raw_state = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        frames = []\n",
    "        while not done:\n",
    "            frames.append(self.env.render(mode=\"rgb_array\"))\n",
    "            action = self.select_action(raw_state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            raw_state = next_state\n",
    "            score += reward\n",
    "        \n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def float_tensor(self, numpy_array):\n",
    "        return torch.FloatTensor(numpy_array).to(self.device)\n",
    "\n",
    "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray], gamma: float) -> torch.Tensor:\n",
    "        \"\"\"Return categorical dqn loss.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "        state_visual, state_not_visual = self.unflatten_state( samples[\"obs\"] )\n",
    "        next_state_visual, next_state_not_visual = self.unflatten_state( samples[\"next_obs\"] )\n",
    "        next_state_visual = self.float_tensor( next_state_visual )\n",
    "        next_state_not_visual = self.float_tensor( next_state_not_visual )\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(device)\n",
    "        reward = self.float_tensor( samples[\"rews\"].reshape(-1, 1) )\n",
    "        done = self.float_tensor( samples[\"done\"].reshape(-1, 1) )\n",
    "        \n",
    "        # Categorical DQN algorithm\n",
    "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_action = self.dqn(next_state_visual, \n",
    "                                   next_state_not_visual).argmax(1)\n",
    "            next_dist = self.dqn_target.dist(next_state_visual, \n",
    "                                   next_state_not_visual)\n",
    "            next_dist = next_dist[range(self.batch_size), \n",
    "                                  next_action]\n",
    "\n",
    "            t_z = reward + (1 - done) * gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    0, (batch_size - 1) * self.atom_size, self.batch_size\n",
    "                ).long()\n",
    "                .unsqueeze(1)\n",
    "                .expand(self.batch_size, self.atom_size)\n",
    "                .to(self.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.dqn.dist(self.float_tensor(state_visual),\n",
    "                             self.float_tensor(state_not_visual))\n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
    "\n",
    "        return elementwise_loss\n",
    "\n",
    "    def _target_hard_update(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "                \n",
    "    def _plot(\n",
    "        self, \n",
    "        frame_idx: int, \n",
    "        scores: List[float],\n",
    "        current_score: List[float],\n",
    "        losses: List[float],\n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores)\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        plt.subplot(133)\n",
    "        plt.title('current_score')\n",
    "        plt.plot(current_score)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "You can see the [code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) and [configurations](https://github.com/openai/gym/blob/master/gym/envs/__init__.py#L53) of CartPole-v0 from OpenAI's repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 160 ms, sys: 238 ms, total: 398 ms\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'MineRLNavigateDense-v0'\n",
    "%time env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(compassAngle:Box(), inventory:Dict(dirt:Box()), pov:Box(64, 64, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_dim = 64*64*3 + 1\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 777\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "seed_torch(seed)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "number of trainable parameters: 877698\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "num_frames = 10000000\n",
    "memory_size = 100000\n",
    "batch_size = 32\n",
    "target_update = 100\n",
    "\n",
    "# train\n",
    "agent = DQNAgent(env, memory_size, batch_size, target_update, obs_dim)\n",
    "\n",
    "# number of trainable parameters of model\n",
    "print('number of trainable parameters:', sum(p.numel() for p in agent.dqn.parameters()  if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the agent.\"\"\"\n",
    "\n",
    "state = env.reset()\n",
    "update_cnt = 0\n",
    "losses = []\n",
    "scores = []\n",
    "current_score = []\n",
    "current_scores = []\n",
    "max_parameters = []\n",
    "plotting_interval = 200\n",
    "score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6171/10000000 [11:19<284:29:30,  9.76it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pickle.dump([i[0] for i in agent.dqn.named_parameters()], open('param_names.pkl', 'wb'))\n",
    "\n",
    "for frame_idx in tqdm(range(1, num_frames + 1)):\n",
    "    action = agent.select_action(state)\n",
    "\n",
    "    next_state, reward, done = agent.step(action)\n",
    "\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    current_score.append(score)\n",
    "\n",
    "    # NoisyNet: removed decrease of epsilon\n",
    "\n",
    "    # PER: increase beta\n",
    "    fraction = min(frame_idx / num_frames, 1.0)\n",
    "    agent.beta = agent.beta + fraction * (1.0 - agent.beta)\n",
    "    \n",
    "    # monitor parameters\n",
    "    max_parameters.append( [float(i.max()) for i in agent.dqn.parameters()])\n",
    "    pickle.dump(max_parameters, open('max_parameters.pkl', 'wb'))\n",
    "\n",
    "    # if episode ends\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        current_scores.append(current_score)\n",
    "        scores.append(score)\n",
    "        pickle.dump(current_scores, open('current_score.pkl', 'wb'))\n",
    "        pickle.dump(scores, open('scores.pkl', 'wb'))\n",
    "        current_score = []\n",
    "        score = 0\n",
    "\n",
    "    # if training is ready\n",
    "    if len(agent.memory) >= agent.batch_size:\n",
    "        loss = agent.update_model()\n",
    "        losses.append(loss)\n",
    "        update_cnt += 1\n",
    "\n",
    "        # if hard update is needed\n",
    "        if update_cnt % agent.target_update == 0:\n",
    "            agent._target_hard_update()\n",
    "\n",
    "    # plotting\n",
    "    if frame_idx % plotting_interval == 0:\n",
    "        agent._plot(frame_idx, scores, current_score, losses)\n",
    "        torch.save(agent.dqn.state_dict(), 'rainbow_model.torch')\n",
    "        torch.save(agent.dqn_target.state_dict(), 'rainbow_model.torch')\n",
    "        \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-3-06b7969c0015>\u001b[0m(86)\u001b[0;36mupdate_priorities\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     84 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     85 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriority\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 86 \u001b[0;31m            \u001b[0;32massert\u001b[0m \u001b[0mpriority\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     87 \u001b[0;31m            \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     88 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-6-4d47cad9b32a>\u001b[0m(221)\u001b[0;36mupdate_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    219 \u001b[0;31m        \u001b[0mloss_for_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melementwise_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    220 \u001b[0;31m        \u001b[0mnew_priorities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_for_prior\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_eps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 221 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_priorities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_priorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    222 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    223 \u001b[0;31m        \u001b[0;31m# NoisyNet: reset noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m<ipython-input-12-b33fee442c3e>\u001b[0m(34)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     32 \u001b[0;31m    \u001b[0;31m# if training is ready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     33 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 34 \u001b[0;31m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     35 \u001b[0;31m        \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     36 \u001b[0;31m        \u001b[0mupdate_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "*** Oldest frame\n",
      "ipdb> list(agen.dqn.parameters())[0]\n",
      "*** Error in argument: '(agen.dqn.parameters())[0]'\n",
      "ipdb> list(agent.dqn.parameters())\n",
      "*** Error in argument: '(agent.dqn.parameters())'\n",
      "ipdb> agent.dqn\n",
      "Network(\n",
      "  (feature_layer_1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (feature_layer_2): Sequential(\n",
      "    (0): Linear(in_features=1025, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (advantage_hidden_layer): NoisyLinear()\n",
      "  (advantage_layer): NoisyLinear()\n",
      "  (value_hidden_layer): NoisyLinear()\n",
      "  (value_layer): NoisyLinear()\n",
      ")\n",
      "ipdb> agent.dqn.parameters()\n",
      "<generator object Module.parameters at 0x7f6df2aebc78>\n",
      "ipdb> [max(i).float() for i in agent.dqn.parameters()]\n",
      "*** RuntimeError: bool value of Tensor with more than one value is ambiguous\n",
      "ipdb> [i.max().float() for i in agent.dqn.parameters()]\n",
      "[tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>), tensor(nan, device='cuda:0', grad_fn=<MaxBackward1>)]\n",
      "ipdb> ll\n",
      "\u001b[1;32m      1 \u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'param_names.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      2 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      3 \u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      4 \u001b[0m    \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      5 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      6 \u001b[0m    \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      7 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      8 \u001b[0m    \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      9 \u001b[0m    \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     10 \u001b[0m    \u001b[0mcurrent_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     11 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     12 \u001b[0m    \u001b[0;31m# NoisyNet: removed decrease of epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     13 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     14 \u001b[0m    \u001b[0;31m# PER: increase beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     15 \u001b[0m    \u001b[0mfraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_idx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     16 \u001b[0m    \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfraction\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     17 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     18 \u001b[0m    \u001b[0;31m# monitor parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     19 \u001b[0m    \u001b[0mmax_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     20 \u001b[0m    \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_parameters.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     21 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     22 \u001b[0m    \u001b[0;31m# if episode ends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     23 \u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     24 \u001b[0m        \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     25 \u001b[0m        \u001b[0mcurrent_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     26 \u001b[0m        \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     27 \u001b[0m        \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'current_score.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     28 \u001b[0m        \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scores.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     29 \u001b[0m        \u001b[0mcurrent_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     30 \u001b[0m        \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     31 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     32 \u001b[0m    \u001b[0;31m# if training is ready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     33 \u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 34 \u001b[0;31m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     35 \u001b[0m        \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     36 \u001b[0m        \u001b[0mupdate_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     37 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     38 \u001b[0m        \u001b[0;31m# if hard update is needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     39 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mupdate_cnt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     40 \u001b[0m            \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_hard_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     41 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     42 \u001b[0m    \u001b[0;31m# plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     43 \u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mplotting_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     44 \u001b[0m        \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     45 \u001b[0m        \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rainbow_model.torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     46 \u001b[0m        \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rainbow_model.torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     47 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     48 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     49 \u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> d\n",
      "> \u001b[0;32m<ipython-input-6-4d47cad9b32a>\u001b[0m(221)\u001b[0;36mupdate_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    219 \u001b[0;31m        \u001b[0mloss_for_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melementwise_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    220 \u001b[0;31m        \u001b[0mnew_priorities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_for_prior\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_eps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 221 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_priorities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_priorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    222 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    223 \u001b[0;31m        \u001b[0;31m# NoisyNet: reset noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> ll\n",
      "\u001b[1;32m    186 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    187 \u001b[0m        \u001b[0;34m\"\"\"Update the model by gradient descent.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    188 \u001b[0m        \u001b[0;31m# PER needs beta to calculate weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    189 \u001b[0m        \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    190 \u001b[0m        \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_tensor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    191 \u001b[0m        \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"indices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    192 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    193 \u001b[0m        \u001b[0;31m# 1-step Learning loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    194 \u001b[0m        \u001b[0melementwise_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dqn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    195 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    196 \u001b[0m        \u001b[0;31m# PER: importance sampling before average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    197 \u001b[0m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melementwise_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    198 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    199 \u001b[0m        \u001b[0;31m# N-step Learning loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    200 \u001b[0m        \u001b[0;31m# we are gonna combine 1-step loss and n-step loss so as to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    201 \u001b[0m        \u001b[0;31m# prevent high-variance. The original rainbow employs n-step loss only.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    202 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_n_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    203 \u001b[0m            \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    204 \u001b[0m            \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch_from_idxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    205 \u001b[0m            \u001b[0melementwise_loss_n_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dqn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    206 \u001b[0m            \u001b[0melementwise_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0melementwise_loss_n_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    207 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    208 \u001b[0m            \u001b[0;31m# PER: importance sampling before average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    209 \u001b[0m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melementwise_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    210 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    211 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    212 \u001b[0m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    213 \u001b[0m        \u001b[0;31m# gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    214 \u001b[0m        \u001b[0;31m# https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    215 \u001b[0m        \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    216 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    217 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    218 \u001b[0m        \u001b[0;31m# PER: update priorities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    219 \u001b[0m        \u001b[0mloss_for_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melementwise_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    220 \u001b[0m        \u001b[0mnew_priorities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_for_prior\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_eps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 221 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_priorities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_priorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    222 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    223 \u001b[0m        \u001b[0;31m# NoisyNet: reset noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    224 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    225 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    226 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    227 \u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    228 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> new_priorities()\n",
      "*** TypeError: 'numpy.ndarray' object is not callable\n",
      "ipdb> new_priorities\n",
      "array([5.377179 , 5.386893 , 5.39185  , 5.375078 , 5.40729  , 5.424279 ,\n",
      "       5.382612 , 5.3835907, 5.387334 , 5.376042 , 5.383843 , 5.3972425,\n",
      "       5.3836985, 5.385297 , 5.3839464, 5.3886404, 5.3806667, 5.384219 ,\n",
      "       5.3869095, 5.3862166, 5.3818097, 5.386282 , 5.379757 , 5.3875265,\n",
      "             nan, 5.3835716, 5.3844604, 5.38536  , 5.3784018, 5.3839564,\n",
      "       5.3848886, 5.381221 ], dtype=float32)\n",
      "ipdb> [i.grad for i in self.dqn.parameters()]\n",
      "[tensor([[[[ 2.8985e-08,  2.8985e-08, -1.2107e-07,  ..., -1.7073e-07,\n",
      "           -3.5239e-07, -2.8064e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.8233e-07,\n",
      "           -1.2452e-07, -4.0048e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.4322e-08,\n",
      "           -1.2181e-07, -2.4582e-07],\n",
      "          ...,\n",
      "          [-9.7558e-09,  6.0869e-08,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00, -1.2129e-07],\n",
      "          [-8.0064e-08, -8.0064e-08, -7.0624e-08,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-1.6117e-07, -8.0064e-08, -1.2268e-07,  ..., -6.0472e-08,\n",
      "           -6.0472e-08, -5.0989e-09]],\n",
      "\n",
      "         [[ 5.7970e-08,  5.7970e-08, -2.4214e-07,  ..., -3.4146e-07,\n",
      "           -7.1431e-07, -5.7289e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.6253e-07,\n",
      "           -2.5050e-07, -8.1272e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.8644e-08,\n",
      "           -2.4293e-07, -4.8950e-07],\n",
      "          ...,\n",
      "          [-1.5916e-08,  1.2029e-07,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00, -2.5067e-07],\n",
      "          [-1.5653e-07, -1.5653e-07, -1.3620e-07,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-3.2379e-07, -1.5508e-07, -2.4031e-07,  ..., -1.1935e-07,\n",
      "           -1.2094e-07, -9.8579e-09]],\n",
      "\n",
      "         [[ 1.5942e-08,  1.5942e-08, -6.5580e-08,  ..., -9.1838e-08,\n",
      "           -1.9660e-07, -1.4971e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -9.8242e-08,\n",
      "           -6.7219e-08, -2.1440e-07],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.9568e-09,\n",
      "           -6.5666e-08, -1.3191e-07],\n",
      "          ...,\n",
      "          [-7.0240e-09,  3.3333e-08,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00, -6.4689e-08],\n",
      "          [-4.5374e-08, -4.5374e-08, -4.0357e-08,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00],\n",
      "          [-8.8790e-08, -4.5374e-08, -6.9009e-08,  ..., -3.3418e-08,\n",
      "           -3.3418e-08, -2.7194e-09]]],\n",
      "\n",
      "\n",
      "        [[[ 8.0391e-07,  9.9151e-07,  7.9838e-07,  ...,  1.7259e-08,\n",
      "            1.2936e-08,  6.6521e-07],\n",
      "          [ 5.5311e-07,  7.9838e-07,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  1.2855e-07],\n",
      "          [ 2.2970e-07,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00, -6.5498e-07],\n",
      "          ...,\n",
      "          [ 8.1075e-07, -1.2030e-07, -1.8859e-08,  ..., -1.2653e-06,\n",
      "            8.1277e-07,  1.7149e-06],\n",
      "          [ 1.0767e-06,  9.3964e-07, -7.6534e-07,  ...,  1.4508e-06,\n",
      "            2.0310e-06,  9.8118e-07],\n",
      "          [ 2.7479e-06,  4.2239e-07, -3.3982e-07,  ...,  2.1409e-06,\n",
      "            1.1732e-06, -1.7294e-06]],\n",
      "\n",
      "         [[ 1.6180e-06,  1.9958e-06,  1.5968e-06,  ...,  3.3757e-08,\n",
      "            2.4380e-08,  1.3639e-06],\n",
      "          [ 1.1505e-06,  1.6410e-06,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  2.5710e-07],\n",
      "          [ 5.0365e-07,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00, -7.6681e-07],\n",
      "          ...,\n",
      "          [ 1.6341e-06, -2.3148e-07, -2.8600e-08,  ..., -1.6614e-06,\n",
      "            5.3776e-07,  1.9775e-06],\n",
      "          [ 2.1525e-06,  1.8850e-06, -9.7780e-07,  ...,  1.8188e-06,\n",
      "            2.4522e-06,  1.1609e-06],\n",
      "          [ 2.4967e-06,  1.3719e-06,  2.2800e-07,  ...,  2.6819e-06,\n",
      "            1.4420e-06, -1.3196e-06]],\n",
      "\n",
      "         [[ 4.6068e-07,  5.6652e-07,  4.6070e-07,  ...,  9.1237e-09,\n",
      "            6.8487e-09,  3.8677e-07],\n",
      "          [ 3.2863e-07,  4.6070e-07,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00,  6.8989e-08],\n",
      "          [ 1.5963e-07,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "            0.0000e+00, -1.3306e-06],\n",
      "          ...,\n",
      "          [ 4.5025e-07, -6.3626e-08, -6.4814e-09,  ..., -2.2523e-06,\n",
      "            7.5565e-07,  2.3570e-06],\n",
      "          [ 5.8849e-07,  5.2161e-07, -1.3813e-06,  ...,  1.1069e-06,\n",
      "            2.3576e-06,  8.3934e-07],\n",
      "          [ 5.1731e-06, -7.3375e-07, -1.7341e-06,  ...,  2.4198e-06,\n",
      "            8.8898e-07, -4.3574e-06]]],\n",
      "\n",
      "\n",
      "        [[[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.1964e-07, -7.6223e-08, -8.3548e-08,  ..., -3.4041e-08,\n",
      "           -4.1395e-07, -8.0676e-07],\n",
      "          [-1.1964e-07, -1.9401e-07, -2.4285e-07,  ..., -3.4041e-08,\n",
      "           -2.6218e-07, -9.7377e-07],\n",
      "          [-3.4975e-08, -8.3813e-09, -8.3813e-09,  ...,  0.0000e+00,\n",
      "           -1.3046e-07, -7.1891e-07],\n",
      "          ...,\n",
      "          [ 2.6544e-09, -7.6968e-08, -1.2766e-07,  ..., -2.2627e-08,\n",
      "           -4.3233e-07, -6.8167e-07],\n",
      "          [ 0.0000e+00, -1.5894e-07,  4.4921e-09,  ..., -2.6711e-08,\n",
      "           -4.5616e-07, -5.7670e-07],\n",
      "          [-1.6763e-07, -1.6343e-07,  3.7806e-08,  ..., -8.6026e-08,\n",
      "           -5.1727e-07, -3.0026e-07]],\n",
      "\n",
      "         [[-2.4117e-07, -1.5977e-07, -1.5977e-07,  ..., -6.8082e-08,\n",
      "           -8.3331e-07, -1.6052e-06],\n",
      "          [-2.4117e-07, -3.7527e-07, -4.7837e-07,  ..., -6.8082e-08,\n",
      "           -5.2436e-07, -1.9602e-06],\n",
      "          [-6.9949e-08, -1.2572e-08, -1.6763e-08,  ...,  0.0000e+00,\n",
      "           -2.6093e-07, -1.4313e-06],\n",
      "          ...,\n",
      "          [ 5.1047e-09, -1.5414e-07, -2.5134e-07,  ..., -4.5254e-08,\n",
      "           -8.6085e-07, -1.3642e-06],\n",
      "          [ 0.0000e+00, -3.1369e-07,  8.9843e-09,  ..., -5.3421e-08,\n",
      "           -8.9970e-07, -1.1509e-06],\n",
      "          [-3.3525e-07, -3.2687e-07,  7.0446e-08,  ..., -1.6529e-07,\n",
      "           -1.0220e-06, -6.0671e-07]],\n",
      "\n",
      "         [[-6.8092e-08, -4.6386e-08, -4.6386e-08,  ..., -2.1276e-08,\n",
      "           -2.2652e-07, -4.3034e-07],\n",
      "          [-6.8092e-08, -1.0581e-07, -1.3294e-07,  ..., -1.7021e-08,\n",
      "           -1.3554e-07, -5.2559e-07],\n",
      "          [-1.8965e-08, -4.1906e-09, -4.1906e-09,  ...,  0.0000e+00,\n",
      "           -6.4260e-08, -3.8091e-07],\n",
      "          ...,\n",
      "          [ 1.4293e-09, -4.0477e-08, -7.0016e-08,  ..., -1.1812e-08,\n",
      "           -2.3114e-07, -3.5834e-07],\n",
      "          [ 0.0000e+00, -8.5553e-08,  2.4503e-09,  ..., -1.4058e-08,\n",
      "           -2.4475e-07, -3.0416e-07],\n",
      "          [-9.2194e-08, -9.2194e-08,  2.2907e-08,  ..., -4.7762e-08,\n",
      "           -2.7804e-07, -1.6057e-07]]],\n",
      "\n",
      "\n",
      "        [[[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          ...,\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan],\n",
      "          [        nan,         nan,         nan,  ...,         nan,\n",
      "                   nan,         nan]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4703e-06,  3.1513e-06,  2.1301e-06,  ...,  6.2454e-07,\n",
      "            1.7008e-08,  2.0373e-06],\n",
      "          [ 1.8729e-06,  1.1908e-06,  7.8037e-07,  ...,  3.3005e-07,\n",
      "           -1.1139e-07,  2.2498e-06],\n",
      "          [ 2.4909e-06,  6.7983e-07, -1.2309e-07,  ...,  5.8373e-07,\n",
      "            5.8483e-07,  1.8938e-06],\n",
      "          ...,\n",
      "          [ 5.7261e-06,  5.6389e-06,  5.7881e-06,  ...,  1.6694e-06,\n",
      "            3.8824e-06,  4.4714e-06],\n",
      "          [ 5.8203e-06,  4.7612e-06,  4.4531e-06,  ...,  5.4995e-06,\n",
      "            5.7201e-06,  4.9408e-06],\n",
      "          [ 7.3997e-06,  6.0105e-06,  4.5422e-06,  ...,  5.6827e-06,\n",
      "            6.2461e-06,  3.8920e-06]],\n",
      "\n",
      "         [[ 7.0724e-06,  6.4325e-06,  4.2068e-06,  ...,  9.5648e-07,\n",
      "           -1.2902e-07,  3.9926e-06],\n",
      "          [ 3.9282e-06,  2.4179e-06,  1.4918e-06,  ...,  4.2730e-07,\n",
      "           -2.7954e-07,  4.4230e-06],\n",
      "          [ 5.1981e-06,  1.3255e-06, -2.5659e-07,  ...,  1.1687e-06,\n",
      "            1.0786e-06,  3.8042e-06],\n",
      "          ...,\n",
      "          [ 1.1376e-05,  1.1210e-05,  1.1502e-05,  ...,  3.3154e-06,\n",
      "            5.6032e-06,  6.7407e-06],\n",
      "          [ 1.1538e-05,  9.4859e-06,  8.8220e-06,  ...,  9.0173e-06,\n",
      "            9.2459e-06,  7.9092e-06],\n",
      "          [ 1.2861e-05,  1.1989e-05,  9.0242e-06,  ...,  9.5432e-06,\n",
      "            1.0384e-05,  7.6278e-06]],\n",
      "\n",
      "         [[ 1.7342e-06,  1.5436e-06,  1.1839e-06,  ...,  3.6415e-07,\n",
      "            5.2319e-08,  1.1563e-06],\n",
      "          [ 8.2363e-07,  6.8365e-07,  4.3444e-07,  ...,  2.2377e-07,\n",
      "            7.5820e-09,  1.2751e-06],\n",
      "          [ 1.1650e-06,  3.9037e-07, -7.0501e-08,  ...,  3.3993e-07,\n",
      "            3.6487e-07,  1.1364e-06],\n",
      "          ...,\n",
      "          [ 3.0923e-06,  3.0790e-06,  3.1477e-06,  ...,  9.0748e-07,\n",
      "            4.5162e-06,  4.8753e-06],\n",
      "          [ 3.1533e-06,  2.6234e-06,  2.3879e-06,  ...,  5.3911e-06,\n",
      "            5.5129e-06,  5.1128e-06],\n",
      "          [ 6.3536e-06,  3.2237e-06,  2.5555e-06,  ...,  5.5038e-06,\n",
      "            5.8420e-06,  2.1711e-06]]]], device='cuda:0'), tensor([-2.5539e-08, -3.2176e-08,         nan, -6.7666e-09,  1.1823e-07,\n",
      "         1.6594e-08,  6.1940e-08,  6.6283e-08,  5.4967e-11, -6.9229e-08,\n",
      "         4.6606e-08,         nan,         nan, -3.2604e-08, -6.5738e-09,\n",
      "         6.8486e-09,  2.1662e-08,         nan,  3.2964e-08,  4.0756e-08,\n",
      "         6.8587e-09,  1.0273e-08, -4.3142e-08, -4.1702e-09,  5.3659e-09,\n",
      "        -6.6227e-08, -2.3471e-09,  5.1557e-08,  7.5394e-08, -2.2153e-08,\n",
      "                nan,  1.8033e-07], device='cuda:0'), tensor([[[[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan],\n",
      "          [nan, nan, nan, nan]]]], device='cuda:0'), tensor([        nan,         nan,         nan, -8.9334e-07,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,  1.2640e-06,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "        -8.1139e-08,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "        -5.6766e-07,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan,         nan,\n",
      "                nan,         nan,  4.8530e-08,         nan,         nan,\n",
      "                nan,         nan,         nan,         nan], device='cuda:0'), tensor([[[[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00, -3.7968e-07,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00, -1.9094e-08,  0.0000e+00],\n",
      "          [-1.5941e-08, -6.5139e-08,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00, -8.2376e-08]],\n",
      "\n",
      "         [[ 0.0000e+00, -3.2911e-07,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000e+00,  1.9366e-08,  0.0000e+00],\n",
      "          [ 6.5952e-08,  0.0000e+00,  0.0000e+00],\n",
      "          [ 6.8269e-08,  2.8064e-08,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 2.8296e-08,  0.0000e+00,  0.0000e+00],\n",
      "          [-3.6784e-09,  2.1059e-08,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 2.9053e-08,  0.0000e+00,  0.0000e+00],\n",
      "          [ 4.6120e-09,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [ 1.3773e-09,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  7.6412e-09,  0.0000e+00]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "          [-8.3544e-09,  0.0000e+00,  0.0000e+00],\n",
      "          [ 0.0000e+00,  2.2667e-08,  0.0000e+00]]],\n",
      "\n",
      "\n",
      "        [[[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]],\n",
      "\n",
      "         [[        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan],\n",
      "          [        nan,         nan,         nan]]]], device='cuda:0'), tensor([        nan, -8.1988e-08,         nan,         nan,         nan,\n",
      "        -1.9033e-07,         nan,         nan, -1.5979e-07,  0.0000e+00,\n",
      "                nan,         nan,         nan,  0.0000e+00, -1.4586e-07,\n",
      "                nan,         nan,         nan,  2.0003e-07,  2.3203e-07,\n",
      "         0.0000e+00,         nan,         nan,  0.0000e+00,         nan,\n",
      "                nan,         nan,         nan,         nan,  3.5672e-07,\n",
      "         3.2264e-07, -1.5839e-07,         nan,         nan,         nan,\n",
      "                nan,         nan,         nan,  0.0000e+00,  0.0000e+00,\n",
      "                nan,         nan, -1.7878e-07,         nan,         nan,\n",
      "                nan,         nan, -2.2725e-06,  0.0000e+00,  0.0000e+00,\n",
      "                nan,  0.0000e+00,         nan, -7.4340e-08,         nan,\n",
      "                nan,         nan,         nan,         nan, -5.0479e-07,\n",
      "                nan,         nan,  5.3240e-08,         nan], device='cuda:0'), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "                nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,         nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "                nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  2.5258e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -5.3020e-06,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0'), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.6632e-05,\n",
      "         1.8635e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.6264e-06,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  8.2116e-04,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.1247e-03,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.8018e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0467e-04,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6046e-06,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  3.2262e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  9.1966e-05,  1.4562e-03,  2.5774e-04,  0.0000e+00,\n",
      "         3.4215e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.4604e-06,\n",
      "         1.2308e-05,  0.0000e+00,  3.4501e-04,  0.0000e+00,  0.0000e+00,\n",
      "         5.9006e-05,  0.0000e+00,  0.0000e+00, -2.2459e-04,  0.0000e+00,\n",
      "         0.0000e+00,         nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  4.3286e-04,  0.0000e+00,\n",
      "                nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.6643e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1221e-03,\n",
      "         0.0000e+00,  0.0000e+00,  9.9289e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6976e-05,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2948e-05,\n",
      "         0.0000e+00,  1.8684e-05,  0.0000e+00,  3.3178e-05,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00], device='cuda:0'), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -1.5930e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,         nan,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.8882e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00], device='cuda:0'), tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -1.3511e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,         nan,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         2.7304e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00], device='cuda:0'), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan], device='cuda:0'), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'), tensor([ 0.0000e+00,  1.1921e-04,  7.3120e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,         nan,  2.1502e-04,  0.0000e+00,\n",
      "                nan,         nan,  2.9057e-04,  0.0000e+00,  0.0000e+00,\n",
      "                nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,         nan,\n",
      "         3.6093e-05,  0.0000e+00,  0.0000e+00,  1.3812e-04,  0.0000e+00,\n",
      "         0.0000e+00,         nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,         nan,  0.0000e+00,  1.5349e-04,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.5745e-04,  0.0000e+00,  0.0000e+00,         nan,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.5142e-04,\n",
      "         0.0000e+00,         nan,  0.0000e+00,         nan,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,         nan,  1.2726e-04,  0.0000e+00,\n",
      "         2.0809e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,\n",
      "         3.2541e-05,  5.3451e-04,  7.7308e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  5.6491e-04,  2.5577e-04,  0.0000e+00,         nan,\n",
      "         0.0000e+00,  6.2474e-05,         nan,  0.0000e+00,  0.0000e+00,\n",
      "                nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.0271e-04,\n",
      "         5.3196e-04,  0.0000e+00,         nan,  0.0000e+00,  4.9175e-05,\n",
      "         0.0000e+00,         nan,  4.6512e-04,  0.0000e+00,         nan,\n",
      "                nan,         nan,  0.0000e+00,  0.0000e+00,         nan,\n",
      "                nan,  0.0000e+00,  0.0000e+00,         nan,  4.6311e-04,\n",
      "         2.7885e-05,         nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -2.0651e-06,         nan,\n",
      "         0.0000e+00,  5.3029e-05,  0.0000e+00], device='cuda:0'), tensor([ 0.0000e+00, -1.0144e-04, -8.7042e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,         nan, -2.5582e-04,  0.0000e+00,\n",
      "                nan,         nan,  1.5747e-04,  0.0000e+00,  0.0000e+00,\n",
      "                nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,         nan,\n",
      "         3.6697e-05,  0.0000e+00,  0.0000e+00,  9.8703e-05,  0.0000e+00,\n",
      "         0.0000e+00,         nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,         nan,  0.0000e+00,  1.2141e-04,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.8456e-04,  0.0000e+00,  0.0000e+00,         nan,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.1143e-04,\n",
      "         0.0000e+00,         nan,  0.0000e+00,         nan,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,         nan,  1.2150e-04,  0.0000e+00,\n",
      "         2.7330e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,         nan,\n",
      "        -1.2054e-05, -3.6951e-04,  6.6297e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -1.8747e-04, -2.3571e-04,  0.0000e+00,         nan,\n",
      "         0.0000e+00,  2.2654e-05,         nan,  0.0000e+00,  0.0000e+00,\n",
      "                nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.5903e-04,\n",
      "        -7.0302e-04,  0.0000e+00,         nan,  0.0000e+00,  8.0334e-06,\n",
      "         0.0000e+00,         nan,  5.3173e-04,  0.0000e+00,         nan,\n",
      "                nan,         nan,  0.0000e+00,  0.0000e+00,         nan,\n",
      "                nan,  0.0000e+00,  0.0000e+00,         nan,  6.1593e-04,\n",
      "        -1.7458e-05,         nan,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0391e-06,         nan,\n",
      "         0.0000e+00, -5.3731e-05,  0.0000e+00], device='cuda:0'), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan], device='cuda:0')]\n",
      "ipdb> ll\n",
      "\u001b[1;32m    186 \u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    187 \u001b[0m        \u001b[0;34m\"\"\"Update the model by gradient descent.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    188 \u001b[0m        \u001b[0;31m# PER needs beta to calculate weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    189 \u001b[0m        \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    190 \u001b[0m        \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_tensor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    191 \u001b[0m        \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"indices\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    192 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    193 \u001b[0m        \u001b[0;31m# 1-step Learning loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    194 \u001b[0m        \u001b[0melementwise_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dqn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    195 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    196 \u001b[0m        \u001b[0;31m# PER: importance sampling before average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    197 \u001b[0m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melementwise_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    198 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    199 \u001b[0m        \u001b[0;31m# N-step Learning loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    200 \u001b[0m        \u001b[0;31m# we are gonna combine 1-step loss and n-step loss so as to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    201 \u001b[0m        \u001b[0;31m# prevent high-variance. The original rainbow employs n-step loss only.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    202 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_n_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    203 \u001b[0m            \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    204 \u001b[0m            \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch_from_idxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    205 \u001b[0m            \u001b[0melementwise_loss_n_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dqn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    206 \u001b[0m            \u001b[0melementwise_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0melementwise_loss_n_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    207 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    208 \u001b[0m            \u001b[0;31m# PER: importance sampling before average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    209 \u001b[0m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melementwise_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    210 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    211 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    212 \u001b[0m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    213 \u001b[0m        \u001b[0;31m# gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    214 \u001b[0m        \u001b[0;31m# https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    215 \u001b[0m        \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    216 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    217 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    218 \u001b[0m        \u001b[0;31m# PER: update priorities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    219 \u001b[0m        \u001b[0mloss_for_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melementwise_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    220 \u001b[0m        \u001b[0mnew_priorities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_for_prior\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_eps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 221 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_priorities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_priorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    222 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    223 \u001b[0m        \u001b[0;31m# NoisyNet: reset noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    224 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    225 \u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    226 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    227 \u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    228 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> self.opttimizer\n",
      "*** AttributeError: 'DQNAgent' object has no attribute 'opttimizer'\n",
      "ipdb> self.optimizer\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# start_time: 0:02\n",
    "# agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Run the trained agent (1 episode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = agent.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imports specifically so we can render outputs in Colab.\n",
    "# from matplotlib import animation\n",
    "# from JSAnimation.IPython_display import display_animation\n",
    "# from IPython.display import display\n",
    "\n",
    "\n",
    "# def display_frames_as_gif(frames):\n",
    "#     \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
    "#     patch = plt.imshow(frames[0])\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     def animate(i \n",
    "#         patch.set_data(frames[i])\n",
    "\n",
    "#     anim = animation.FuncAnimation(\n",
    "#         plt.gcf(), animate, frames = len(frames), interval=50\n",
    "#     )\n",
    "#     display(display_animation(anim, default_mode='loop'))\n",
    "    \n",
    "        \n",
    "# # display \n",
    "# display_frames_as_gif(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
