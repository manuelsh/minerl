{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'noise=0.3,new_bn,one_noise,lr=1E-4,cam_fact=0.001,tau=0.005,change_order_step'\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "from collections import deque, defaultdict\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import minerl\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from typing import Deque, Dict, List, Tuple\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "logging.basicConfig(filename='logs/'+VERSION+'-'+time.strftime(\"%Y%m%d-%H%M%S\")+'.log', \n",
    "                    filemode='w', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = ['attack', 'jump', 'forward', 'back', 'left', 'right', 'sprint', 'sneak', 'camera_hor', 'camera_ver']\n",
    "\n",
    "SEED = 230\n",
    "OBS_DIM = int(64*64*3+1) # pov + compassAngle\n",
    "BUFFER_SIZE = int(1E5)\n",
    "\n",
    "MAX_NUM_FRAMES = int(1E9)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "POV_SCALING = 255\n",
    "COMPASS_SCALING = 180\n",
    "TANH_FACTOR = 1\n",
    "CAMERA_FACTOR = 0.01\n",
    "NUM_FILTERS = 64\n",
    "\n",
    "NOISY_LINEAR_SIGMA = 0.3\n",
    "ACTOR_LR = 1E-4\n",
    "CRITIC_LR = 1E-4\n",
    "GAMMA = 0.99\n",
    "\n",
    "TAU = 0.005\n",
    "USE_BN = True\n",
    "\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffer and random process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy linear module for NoisyNet.\n",
    "           \n",
    "    Attributes:\n",
    "        in_features (int): input size of linear module\n",
    "        out_features (int): output size of linear module\n",
    "        std_init (float): initial std value\n",
    "        weight_mu (nn.Parameter): mean value weight parameter\n",
    "        weight_sigma (nn.Parameter): std value weight parameter\n",
    "        bias_mu (nn.Parameter): mean value bias parameter\n",
    "        bias_sigma (nn.Parameter): std value bias parameter\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        std_init: float = 0.5,   # this is to restart the parameters of the network\n",
    "        sigma: float = NOISY_LINEAR_SIGMA\n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter( torch.Tensor(out_features, in_features) )\n",
    "        self.register_buffer(\n",
    "            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(\n",
    "            self.std_init / math.sqrt(self.in_features)\n",
    "        )\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(\n",
    "            self.std_init / math.sqrt(self.out_features)\n",
    "        )\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Make new noise.\"\"\"\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "\n",
    "        # outer product\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\n",
    "        \n",
    "        We don't use separate statements on train / eval mode.\n",
    "        It doesn't show remarkable difference of performance.\n",
    "        \"\"\"\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
    "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
    "        )\n",
    "    \n",
    "    def scale_noise(self, size: int) -> torch.Tensor:\n",
    "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
    "        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=self.sigma, size=size))\n",
    "\n",
    "        return x.sign().mul(x.abs().sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim: int = OBS_DIM, \n",
    "        size: int = BUFFER_SIZE,\n",
    "        act_dim: int = len(ACTIONS),\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "#         n_step: int = 1, \n",
    "#         gamma: float = 0.99\n",
    "    ):\n",
    "        \n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "        \n",
    "#         # for N-step Learning\n",
    "#         self.n_step_buffer = deque(maxlen=n_step)\n",
    "#         self.n_step = n_step\n",
    "#         self.gamma = gamma\n",
    "\n",
    "    def store(\n",
    "        self, \n",
    "        obs: np.ndarray, \n",
    "        act: np.ndarray, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ) -> None:\n",
    "        \n",
    "        transition = (obs, act, rew, next_obs, done)\n",
    "#         self.n_step_buffer.append(transition)\n",
    "\n",
    "#         # single step transition is not ready\n",
    "#         if len(self.n_step_buffer) < self.n_step:\n",
    "#             return ()\n",
    "        \n",
    "#         # make a n-step transition\n",
    "#         rew, next_obs, done = self._get_n_step_info(\n",
    "#             self.n_step_buffer, self.gamma\n",
    "#         )\n",
    "#         obs, act = self.n_step_buffer[0][:2]\n",
    "        \n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "#         return self.n_step_buffer[0]\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "\n",
    "        return dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            dones=self.done_buf[idxs],\n",
    "#             # for N-step Learning\n",
    "#             indices=indices,\n",
    "        )\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_acts=len(ACTIONS),\n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 gamma=GAMMA,\n",
    "                 actor_learning_rate=ACTOR_LR, \n",
    "                 critic_learning_rate=CRITIC_LR,\n",
    "                 tau=TAU,\n",
    "                 buffer_capacity=BUFFER_SIZE):\n",
    "\n",
    "        self.num_acts = num_acts\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(\"Device is: \"+str(self.device))\n",
    "        print(\"Device is: \", self.device)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.actor_lr = actor_learning_rate\n",
    "        self.critic_lr = critic_learning_rate\n",
    "        self.tau = tau\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        self.target_actor = ActorNetwork().to(self.device)\n",
    "        self.actor = ActorNetwork().to(self.device)\n",
    "        self.target_critic = CriticNetwork().to(self.device)\n",
    "        self.critic = CriticNetwork().to(self.device)\n",
    "\n",
    "        # Initializing the target networks with the standard network weights\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # Create the optimizers for the actor and critic using the corresponding learning rate\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "\n",
    "    def get_act(self, obs_pov, obs_rest):\n",
    "        \n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            act = self.actor(obs_pov, obs_rest)\n",
    "        self.actor.train()\n",
    "        \n",
    "        return act.cpu().numpy()\n",
    "        \n",
    "    def unflatten_obs(self, flat_obs):\n",
    "        return (flat_obs[:,:-1].reshape(-1,64,64,3), flat_obs[:,-1].reshape(-1,1))\n",
    "        \n",
    "    def flatten_obs(self, obs):\n",
    "        return np.append(obs['pov'].reshape(-1), obs['compassAngle'])\n",
    "\n",
    "    # Store the transition into the replay buffer\n",
    "    def store_transition(self, obs, next_obs, act, rew, done):\n",
    "        obs = self.flatten_obs(obs)\n",
    "        next_obs = self.flatten_obs(next_obs)\n",
    "        self.buffer.store(obs=obs, act=act, rew=rew, \n",
    "                          next_obs=next_obs, done=done)\n",
    "\n",
    "    # Update the target networks using polyak averaging\n",
    "    def update_target_networks(self):\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + target_param.data * (1.0 - self.tau))\n",
    "\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + target_param.data * (1.0 - self.tau))\n",
    "\n",
    "    def float_tensor(self, numpy_array):\n",
    "        return torch.FloatTensor(numpy_array).to(self.device)\n",
    "    \n",
    "   \n",
    "    def reset_noise_networks(self):\n",
    "        self.actor.reset_noise()\n",
    "#         self.critic.reset_noise()\n",
    "        self.target_actor.reset_noise()\n",
    "#         self.target_critic.reset_noise()\n",
    "        \n",
    "     # Train the networks\n",
    "    def fit_batch(self):\n",
    "        # Sample mini-batch from the buffer uniformly or using prioritized experience replay\n",
    "        transitions = self.buffer.sample_batch()\n",
    "        next_obss_pov, next_obss_rest = self.unflatten_obs(self.float_tensor(transitions['next_obs']))\n",
    "        rews = self.float_tensor(transitions['rews'])\n",
    "        obss_pov, obss_rest = self.unflatten_obs(self.float_tensor(transitions['obs']))\n",
    "        dones = self.float_tensor(transitions['dones'])\n",
    "        acts = self.float_tensor(transitions['acts'])\n",
    "\n",
    "        # Step 2: Compute the target values using the target actor network and target critic network\n",
    "        # Compute the Q-values given the current obs ( in this case it is the next_obss)\n",
    "        with torch.no_grad():\n",
    "            next_act = self.target_actor(next_obss_pov, next_obss_rest)\n",
    "            next_Q_values = self.target_critic(next_obss_pov, next_obss_rest, next_act)\n",
    "            next_Q_values = torch.squeeze(next_Q_values, dim=1)\n",
    "            next_Q_values = next_Q_values * (1 - dones)\n",
    "            \n",
    "        # Train critic\n",
    "        y = rews + self.gamma*next_Q_values\n",
    "        q_values = self.critic(obss_pov, obss_rest, acts).view(-1)\n",
    "        loss_critic = self.mse_loss(q_values, y)\n",
    "        self.critic_optim.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0, norm_type=1)\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # Train agent\n",
    "        loss_agent = - self.critic(obss_pov, obss_rest, self.actor(obss_pov, obss_rest)).mean()\n",
    "        self.actor_optim.zero_grad()\n",
    "        loss_agent.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0, norm_type=1)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        return loss_critic, loss_agent, q_values, next_Q_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__( self, acts_dim=len(ACTIONS), num_filters=NUM_FILTERS):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=num_filters, \n",
    "                               padding=0, kernel_size=9, stride=1, bias=not USE_BN)   # output dim: 56\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)         # output dim: 28\n",
    "        if USE_BN: self.bn1 = nn.BatchNorm2d(num_features=num_filters)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=num_filters, out_channels=num_filters, \n",
    "                               padding=1,stride=2, kernel_size=4, bias=not USE_BN)    # output dim: 14\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output dim: 7\n",
    "        if USE_BN: self.bn2 = nn.BatchNorm2d(num_features=num_filters)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.linear1 = NoisyLinear(num_filters*7*7+1, 32) #todo: automatically calculate this\n",
    "        if USE_BN: self.bn4 = nn.BatchNorm1d(num_features=32)\n",
    "        self.linear2= nn.Linear(32, acts_dim)\n",
    "            \n",
    "        self.non_lin_1 = self.non_lin_2 = self.non_lin_3 = None\n",
    "\n",
    "    def forward(self, obs_pov, obs_rest):\n",
    "        \n",
    "        x = self.conv1(obs_pov.permute(0,3,1,2) / POV_SCALING)\n",
    "        x = self.max_pool1(x)\n",
    "        if USE_BN: x = self.bn1(x)\n",
    "        self.non_lin_1 = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(self.non_lin_1)\n",
    "        x = self.max_pool2(x)\n",
    "        if USE_BN: x = self.bn2(x)\n",
    "        self.non_lin_2 = F.elu(x)\n",
    "        \n",
    "        x = x.view(self.non_lin_2.size(0), -1)\n",
    "        x = self.linear1( torch.cat([x, obs_rest / COMPASS_SCALING], dim=1) )\n",
    "        if USE_BN: x = self.bn4(x)\n",
    "        self.non_lin_3 = F.elu(x)\n",
    "        x = self.linear2(self.non_lin_3)\n",
    "        x[:,0:8] = x[:,0:8]*TANH_FACTOR  # for \"discrete actions\"\n",
    "        x[:,8:10] = x[:,8:10]*CAMERA_FACTOR\n",
    "        acts = torch.tanh(x)        # we assume discrete acts threshold at 0\n",
    "        return acts\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        self.linear1.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):   \n",
    "    def __init__(self, acts_dim=len(ACTIONS), num_filters=NUM_FILTERS):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=NUM_FILTERS, padding=0,\n",
    "                               kernel_size=9, stride=1, bias=not USE_BN)   # output dim: 56\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)         # output dim: 28\n",
    "        if USE_BN: self.bn1 = nn.BatchNorm2d(num_features=NUM_FILTERS)\n",
    "        self.conv2 = nn.Conv2d(in_channels=NUM_FILTERS, out_channels=NUM_FILTERS, padding=1,stride=2,\n",
    "                               kernel_size=4, bias=not USE_BN)    # output dim: 14\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output dim: 7\n",
    "        if USE_BN: self.bn2 = nn.BatchNorm2d(num_features=NUM_FILTERS)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.linear1 = nn.Linear(NUM_FILTERS*7*7+acts_dim+1, 32) #todo: automatically calculate this\n",
    "        self.linear2 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.non_lin_1 = self.non_lin_2 = self.non_lin_3 = None\n",
    "\n",
    "    def forward(self, obs_pov, obs_rest, acts):\n",
    "        \n",
    "        x = self.conv1(obs_pov.permute(0,3,1,2) / POV_SCALING)\n",
    "        x = self.max_pool1(x)\n",
    "        if USE_BN: x = self.bn1(x)\n",
    "        self.non_lin_1 = F.elu(x)\n",
    "        \n",
    "        x = self.conv2(self.non_lin_1)\n",
    "        x = self.max_pool2(x)\n",
    "        if USE_BN: x = self.bn2(x)\n",
    "        self.non_lin_2 = F.elu(x)\n",
    "        \n",
    "        x = x.view(self.non_lin_2.size(0), -1)\n",
    "        x = self.linear1(torch.cat([x, obs_rest / COMPASS_SCALING, acts], dim=1))\n",
    "        self.non_lin_3 = F.elu(x)\n",
    "        out = self.linear2(self.non_lin_3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'environment_name' not in locals():\n",
    "    environment_name = 'MineRLNavigateDense-v0'\n",
    "    %time env = gym.make(environment_name)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "\n",
    "# number of trainable parameters of model\n",
    "message_1 = 'Number of trainable parameters actor:' + str(sum(p.numel() for p in agent.actor.parameters()  if p.requires_grad))\n",
    "message_2 = 'Number of trainable parameters critic:'+str(sum(p.numel() for p in agent.critic.parameters()  if p.requires_grad))\n",
    "print(message_1)\n",
    "print(message_2)\n",
    "\n",
    "logging.info(message_1)\n",
    "logging.info(message_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agent\n",
    "# agent.actor.load_state_dict(torch.load('trained_models/ddpg_actor_'+VERSION+'.pkl'))\n",
    "# agent.critic.load_state_dict(torch.load('trained_models/ddpg_critic_'+VERSION+'.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_act(model_act):\n",
    "    '''\n",
    "    Gets environment act from model act\n",
    "    '''\n",
    "    env_act = {act: int(value>0) for act, value in zip(ACTIONS[0:8], model_act[0:8])}\n",
    "    env_act['camera'] = [model_act[8]*180, model_act[9]*180]\n",
    "    return env_act\n",
    "\n",
    "def plot_stats(\n",
    "        frame_idx: int, \n",
    "        scores: List[float],\n",
    "        current_score: List[float],\n",
    "        losses: List[float],\n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('q_values')\n",
    "        plt.plot(scores[-10000:])\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        plt.yscale('log')\n",
    "        plt.subplot(133)\n",
    "        plt.title('current_score')\n",
    "        plt.plot(current_score)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time obs = env.reset()\n",
    "assert (agent.unflatten_obs( np.array([agent.flatten_obs(obs)]) )[0][0].astype(int) == obs['pov']).sum() == 3*64*64\n",
    "update_cnt = 0\n",
    "losses_agent = []\n",
    "losses_critic = []\n",
    "scores = []\n",
    "current_score = []\n",
    "current_scores = []\n",
    "max_parameters = []\n",
    "plotting_interval = 100\n",
    "score = 0\n",
    "logging.info('Environment reset')\n",
    "agent_weight_sigmas_1 = []\n",
    "agent_weight_sigmas_2 = []\n",
    "\n",
    "q_values_mean = []\n",
    "q_values_max = []\n",
    "q_values_min = []\n",
    "next_q_values_mean = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relus_are_alive():\n",
    "    for network in [agent.actor, agent.critic, agent.target_actor, agent.target_critic]:\n",
    "        if float(network.non_lin_1.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_1 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "        if float(network.non_lin_2.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_2 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "        if float(network.non_lin_3.sum()) == 0:\n",
    "            text = str(network) + ' has non_lin_3 died'\n",
    "            logging.info(text)\n",
    "            print(text)\n",
    "            \n",
    "# test\n",
    "# agent.actor.x_relu_1[agent.actor.x_relu_1!=0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_idx in tqdm(range(MAX_NUM_FRAMES), desc='frame'):\n",
    "    obs_pov = agent.float_tensor([obs['pov'].astype(float)])\n",
    "    obs_rest = agent.float_tensor([[obs['compassAngle']]])\n",
    "    model_act = agent.get_act(obs_pov, obs_rest)\n",
    "    env_act = get_env_act(model_act[0])\n",
    "    next_obs, rew, done, info = env.step(env_act)\n",
    "\n",
    "    # Store the transition in the replay buffer of the agent\n",
    "    agent.store_transition(obs=obs, next_obs=next_obs,\n",
    "                               act=model_act, done=done, rew=rew)\n",
    "    \n",
    "    # Prepare for next step and store scores\n",
    "    obs = next_obs\n",
    "    score += rew\n",
    "    current_score.append(score)\n",
    "    \n",
    "    # reset noise\n",
    "    agent.reset_noise_networks()\n",
    "\n",
    "    # if episode ends\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        current_scores.append(current_score)\n",
    "        scores.append(score)\n",
    "        pickle.dump(current_scores, open('scores/current_score_'+VERSION+'.pkl', 'wb'))\n",
    "        pickle.dump(scores, open('scores/scores_'+VERSION+'.pkl', 'wb'))\n",
    "        pickle.dump([losses_agent, losses_critic], open('monitoring/losses_'+VERSION+'.pkl', 'wb'))\n",
    "        current_score = []\n",
    "        score = 0\n",
    "        torch.save(agent.actor.state_dict(), 'trained_models/ddpg_actor_'+VERSION+'.pkl')\n",
    "        torch.save(agent.critic.state_dict(), 'trained_models/ddpg_critic_'+VERSION+'.pkl')\n",
    "        logging.info(f'Trajectory {len(current_scores)} done, with final score {current_scores[-1][-1]}')\n",
    "\n",
    "    # If the size of the buffer is less than batch size then return\n",
    "    if len(agent.buffer) >= agent.batch_size:\n",
    "        loss_critic, loss_agent, q_values, next_Q_values= agent.fit_batch()\n",
    "        agent.update_target_networks()\n",
    "        q_values_mean.append(float(q_values.mean()))\n",
    "        next_q_values_mean.append(float(next_Q_values.mean()))\n",
    "#         q_values_max.append(float(q_values.max()))\n",
    "#         q_values_min.append(float(q_values.min()))\n",
    "        \n",
    "        losses_agent.append(float(loss_agent))\n",
    "        losses_critic.append(float(loss_critic))\n",
    "\n",
    "    if (frame_idx+1) % plotting_interval == 0:\n",
    "        agent_weight_sigmas_1.append(float(agent.actor.linear1.weight_sigma.abs().mean()))\n",
    "#        agent_weight_sigmas_2.append(float(agent.actor.linear2.weight_sigma.abs().mean()))\n",
    "#         critic_weight_sigmas_1.append(float(agent.critic.linear1.weight_sigma.abs().mean()))\n",
    "#         critic_weight_sigmas_2.append(float(agent.critic.linear2.weight_sigma.abs().mean()))\n",
    "        \n",
    "        pickle.dump([agent_weight_sigmas_1, agent_weight_sigmas_2], open('monitoring/weight_sigmas_'+VERSION+'.pkl','wb'))\n",
    "        check_relus_are_alive()\n",
    "        \n",
    "        plot_stats(frame_idx, \n",
    "                   np.array([q_values_mean, next_q_values_mean]).T, \n",
    "                   current_score,\n",
    "                   np.array([losses_agent, losses_critic]).T)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "net_reward = 0\n",
    "actions = []\n",
    "score = 0\n",
    "current_score = []\n",
    "current_scores = []\n",
    "max_parameters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    import pdb; pdb.set_trace()\n",
    "    obs_pov = agent.float_tensor([obs['pov'].astype(float)])\n",
    "    obs_rest = agent.float_tensor([[obs['compassAngle']]])\n",
    "    model_act = agent.get_act(obs_pov,obs_rest)\n",
    "    env_act = get_env_act(model_act[0])\n",
    "    next_obs, rew, done, info = env.step(env_act)\n",
    "\n",
    "    # Prepare for next step and store scores\n",
    "    obs = next_obs\n",
    "    score += rew\n",
    "    current_score.append(score)\n",
    "    \n",
    "#     if i%10==0:\n",
    "    plt.imshow(env.render(mode='rgb_array')) \n",
    "    display.display(plt.gcf())\n",
    "    clear_output(wait=True)\n",
    "    net_reward += rew\n",
    "    actions.append((env_act, net_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(agent.actor.conv2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class for a generic trainer used for training all the different reinforcement learning models\n",
    "\"\"\"\n",
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, agent, num_epochs,\n",
    "                 num_rollouts, num_eval_rollouts, env, eval_env, nb_train_steps,\n",
    "                 max_episodes_per_epoch, \n",
    "                 output_folder=None, her_training=False,\n",
    "                 save_model=False, future=None):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        :param ddpg: The ddpg network\n",
    "        :param num_rollouts: number of experience gathering rollouts per episode\n",
    "        :param num_eval_rollouts: number of evaluation rollouts\n",
    "        :param num_episodes: number of episodes per epoch\n",
    "        :param env: Gym environment to train on\n",
    "        :param eval_env: Gym environment to evaluate on\n",
    "        :param nb_train_steps: training steps to take\n",
    "        :param max_episodes_per_epoch: maximum number of episodes per epoch\n",
    "        :param her_training: use hindsight experience replay\n",
    "        :param multi_gpu_training: train on multiple gpus\n",
    "        \"\"\"\n",
    "\n",
    "        self.ddpg = agent\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_rollouts = num_rollouts\n",
    "        self.num_eval_rollouts = num_eval_rollouts\n",
    "        self.env = env\n",
    "        self.eval_env = eval_env\n",
    "        self.nb_train_steps = nb_train_steps\n",
    "        self.max_episodes = max_episodes_per_epoch\n",
    "        self.her = her_training\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Device is: \", self.device)\n",
    "        self.output_folder = output_folder\n",
    "        self.future = future\n",
    "\n",
    "        self.all_rews = []\n",
    "        self.successes = []\n",
    "\n",
    "        # Get the target  and standard networks\n",
    "        self.target_actor = self.ddpg.get_actors()['target']\n",
    "        self.actor = self.ddpg.get_actors()['actor']\n",
    "        self.target_critic  = self.ddpg.get_critics()['target']\n",
    "        self.critic = self.ddpg.get_critics()['critic']\n",
    "        self.statistics = defaultdict(float)\n",
    "        self.combined_statistics = defaultdict(list)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Starting time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Initialize the statistics dictionary\n",
    "        statistics = self.statistics\n",
    "\n",
    "        episode_rews_history = deque(maxlen=100)\n",
    "        eval_episode_rews_history = deque(maxlen=100)\n",
    "        episode_success_history = deque(maxlen=100)\n",
    "        eval_episode_success_history = deque(maxlen=100)\n",
    "\n",
    "        epoch_episode_rews = []\n",
    "        epoch_episode_success = []\n",
    "        epoch_episode_steps = []\n",
    "\n",
    "        # Epoch rews and success\n",
    "        epoch_rews = []\n",
    "        epoch_success = []\n",
    "\n",
    "        # Initialize the training with an initial obs\n",
    "        obs = self.env.reset()\n",
    "        # If eval, initialize the evaluation with an initial obs\n",
    "        if self.eval_env is not None:\n",
    "            eval_obs = self.eval_env.reset()\n",
    "            eval_obs = to_tensor(eval_obs, use_cuda=self.cuda)\n",
    "            eval_obs = torch.unsqueeze(eval_obs, dim=0)\n",
    "\n",
    "        # Initialize the losses\n",
    "        loss = 0\n",
    "        episode_rew =  0\n",
    "        episode_success = 0\n",
    "        episode_step = 0\n",
    "        epoch_acts = []\n",
    "        t = 0\n",
    "\n",
    "        # Check whether to use cuda or not\n",
    "        obs = to_tensor(obs, use_cuda=self.cuda)\n",
    "        obs = torch.unsqueeze(obs, dim=0)\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_actor_losses = []\n",
    "            epoch_critic_losses = []\n",
    "            for episode in range(self.max_episodes):\n",
    "\n",
    "                # Rollout of trajectory to fill the replay buffer before training\n",
    "                for rollout in range(self.num_rollouts):\n",
    "                    # Sample an act from behavioural policy pi\n",
    "                    act = self.ddpg.get_act(obs=obs, noise=True)\n",
    "                    assert act.shape == self.env.get_act_shape\n",
    "\n",
    "                    # Execute next act\n",
    "                    new_obs, rew, done, success = self.env.step(act)\n",
    "                    success = success['is_success']\n",
    "                    done_bool = done * 1\n",
    "\n",
    "                    t+=1\n",
    "                    episode_rew += rew\n",
    "                    episode_step += 1\n",
    "                    episode_success += success\n",
    "\n",
    "                    # Book keeping\n",
    "                    epoch_acts.append(act)\n",
    "                    # Store the transition in the replay buffer of the agent\n",
    "                    self.ddpg.store_transition(obs=obs, new_obs=new_obs,\n",
    "                                               act=act, done=done_bool, rew=rew,\n",
    "                                               success=success)\n",
    "                    # Set the current obs as the next obs\n",
    "                    obs = to_tensor(new_obs, use_cuda=self.cuda)\n",
    "                    obs = torch.unsqueeze(obs, dim=0)\n",
    "\n",
    "                    # End of the episode\n",
    "                    if done:\n",
    "                        epoch_episode_rews.append(episode_rew)\n",
    "                        episode_rews_history.append(episode_rew)\n",
    "                        episode_success_history.append(episode_success)\n",
    "                        epoch_episode_success.append(episode_success)\n",
    "                        epoch_episode_steps.append(episode_step)\n",
    "                        episode_rew = 0\n",
    "                        episode_step = 0\n",
    "                        episode_success = 0\n",
    "\n",
    "                        # Reset the agent\n",
    "                        self.ddpg.reset()\n",
    "                        # Get a new initial obs to start from\n",
    "                        obs = self.env.reset()\n",
    "                        obs = to_tensor(obs, use_cuda=self.cuda)\n",
    "\n",
    "                # Train\n",
    "                for train_steps in range(self.nb_train_steps):\n",
    "                    critic_loss, actor_loss = self.ddpg.fit_batch()\n",
    "                    if critic_loss is not None and actor_loss is not None:\n",
    "                        epoch_critic_losses.append(critic_loss)\n",
    "                        epoch_actor_losses.append(actor_loss)\n",
    "\n",
    "                    # Update the target networks using polyak averaging\n",
    "                    self.ddpg.update_target_networks()\n",
    "\n",
    "                eval_episode_rews = []\n",
    "                eval_episode_successes = []\n",
    "                if self.eval_env is not None:\n",
    "                    eval_episode_rew = 0\n",
    "                    eval_episode_success = 0\n",
    "                    for t_rollout in range(self.num_eval_rollouts):\n",
    "                        if eval_obs is not None:\n",
    "                            eval_act = self.ddpg.get_act(obs=eval_obs, noise=False)\n",
    "                        eval_new_obs, eval_rew, eval_done, eval_success = self.eval_env.step(eval_act)\n",
    "                        eval_episode_rew += eval_rew\n",
    "                        eval_episode_success += eval_success\n",
    "\n",
    "                        if eval_done:\n",
    "                            eval_obs = self.eval_env.reset()\n",
    "                            eval_obs = to_tensor(eval_obs, use_cuda=self.cuda)\n",
    "                            eval_obs = torch.unsqueeze(eval_obs, dim=0)\n",
    "                            eval_episode_rews.append(eval_episode_rew)\n",
    "                            eval_episode_rews_history.append(eval_episode_rew)\n",
    "                            eval_episode_successes.append(eval_episode_success)\n",
    "                            eval_episode_success_history.append(eval_episode_success)\n",
    "                            eval_episode_rew = 0\n",
    "                            eval_episode_success = 0\n",
    "\n",
    "            # Log stats\n",
    "            duration = time.time() - start_time\n",
    "            statistics['rollout/rews'] = np.mean(epoch_episode_rews)\n",
    "            statistics['rollout/rews_history'] = np.mean(episode_rews_history)\n",
    "            statistics['rollout/successes'] = np.mean(epoch_episode_success)\n",
    "            statistics['rollout/successes_history'] = np.mean(episode_success_history)\n",
    "            statistics['rollout/acts_mean'] = np.mean(epoch_acts)\n",
    "            statistics['train/loss_actor'] = np.mean(epoch_actor_losses)\n",
    "            statistics['train/loss_critic'] = np.mean(epoch_critic_losses)\n",
    "            statistics['total/duration'] = duration\n",
    "\n",
    "            # Evaluation statistics\n",
    "            if self.eval_env is not None:\n",
    "                statistics['eval/rews'] = np.mean(eval_episode_rews)\n",
    "                statistics['eval/rews_history'] = np.mean(eval_episode_rews_history)\n",
    "                statistics['eval/successes'] = np.mean(eval_episode_successes)\n",
    "                statistics['eval/success_history'] = np.mean(eval_episode_success_history)\n",
    "\n",
    "            # Print the statistics\n",
    "            if self.verbose:\n",
    "                if epoch % 5 == 0:\n",
    "                    print(\"Actor Loss: \", statistics['train/loss_actor'])\n",
    "                    print(\"Critic Loss: \", statistics['train/loss_critic'])\n",
    "                    print(\"rew \", statistics['rollout/rews'])\n",
    "                    print(\"Successes \", statistics['rollout/successes'])\n",
    "\n",
    "                    if self.eval_env is not None:\n",
    "                        print(\"Evaluation rew \", statistics['eval/rews'])\n",
    "                        print(\"Evaluation Successes \", statistics['eval/successes'])\n",
    "\n",
    "            # Log the combined statistics for all epochs\n",
    "            for key in sorted(statistics.keys()):\n",
    "                self.combined_statistics[key].append(statistics[key])\n",
    "\n",
    "            # Log the epoch rews and successes\n",
    "            epoch_rews.append(np.mean(epoch_episode_rews))\n",
    "            epoch_success.append(np.mean(epoch_episode_success))\n",
    "\n",
    "        # Plot the statistics calculated\n",
    "        if self.plot_stats:\n",
    "            # Plot the rews and successes\n",
    "            rews_fname = self.output_folder + '/rews.jpg'\n",
    "            success_fname = self.output_folder + '/success.jpg'\n",
    "            plot(epoch_rews, f_name=rews_fname, save_fig=True, show_fig=False)\n",
    "            plot(epoch_success, f_name=success_fname, save_fig=True, show_fig=False)\n",
    "\n",
    "        # Save the models on the disk\n",
    "        if self.save_model:\n",
    "            self.ddpg.save_model(self.output_folder)\n",
    "\n",
    "        return self.combined_statistics\n",
    "\n",
    "    def get_frames(self, transition, sample_experience, k):\n",
    "        \"\"\"\n",
    "\n",
    "        :param transition: Current transition -> Goal substitution\n",
    "        :param sample_experience: The Future episode experiences\n",
    "        :param k: The number of transitions to consider\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Get the frames predicted by our self attention network\n",
    "        seq_length = len(sample_experience)\n",
    "        obss = []\n",
    "        new_obss= []\n",
    "        rews = []\n",
    "        successes = []\n",
    "        acts = []\n",
    "        dones = []\n",
    "        for t in sample_experience:\n",
    "            obs, new_obs, rew, success, act, done_bool = t\n",
    "            obs = np.concatenate(obs[:self.ddpg.obs_dim], obs['achieved_goal'])\n",
    "            new_obs = np.concatenate(new_obs[:self.ddpg.obs_dim], new_obs['achieved_goal'])\n",
    "            obss.append(obs)\n",
    "            new_obss.append(new_obs[:self.ddpg.obs_dim])\n",
    "            rews.append(rew)\n",
    "            successes.append(success)\n",
    "            acts.append(act)\n",
    "            dones.append(done_bool)\n",
    "\n",
    "        # Input Sequence consists of n embeddings of obss||achieved_goals\n",
    "        input_sequence = Variable(torch.cat(obss))\n",
    "        # The Query vector is the current obs || desired goal\n",
    "        obs, new_obs, rew, success, act, done_bool = transition\n",
    "        query = Variable(obs)\n",
    "\n",
    "        # The Goal Network\n",
    "        gn = GoalNetwork(input_dim=seq_length, embedding_dim=self.ddpg.input_dim,\n",
    "                         query_dim=self.ddpg.input_dim, num_hidden=self.ddpg.num_hidden_units,\n",
    "                         output_features=1, use_additive=True, use_self_attn=True, use_token2token=True,\n",
    "                         activation=nn.ReLU)\n",
    "\n",
    "        if self.cuda:\n",
    "            input_sequence = input_sequence.cuda()\n",
    "            query = query.cuda()\n",
    "            gn = gn.cuda()\n",
    "\n",
    "        scores = gn(input_sequence, query)\n",
    "        optimizer_gn = optim.Adam(gn.parameters(), lr=self.ddpg.actor_lr)\n",
    "        optimizer_gn.zero_grad()\n",
    "        # Dimension of the scores vector is 1 x n\n",
    "        # Find the top 5 maximum values from the scores vector and their indexes\n",
    "        values, indices = torch.topk(scores, k, largest=True)\n",
    "        # Now we have the indices -> Get the corresponding experiences\n",
    "        top_experiences = []\n",
    "        for m in indices:\n",
    "            top_experiences.append(sample_experience[m])\n",
    "\n",
    "        # Training Step\n",
    "        TD_error = 0\n",
    "        for t in top_experiences:\n",
    "            TD_error += self.ddpg.calc_td_error(t)\n",
    "        loss = -1 * (TD_error.mean())\n",
    "        loss.backward()\n",
    "        # Clamp the gradients to avoid the vanishing gradient problem\n",
    "        for param in gn.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer_gn.step()\n",
    "\n",
    "        return top_experiences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for the \"her\" part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def sample_goals(self,sampling_strategy, experience, future=None, transition=None):\n",
    "#         g = []\n",
    "#         if sampling_strategy == 'final':\n",
    "#             n_s = experience[len(experience)-1]\n",
    "#             g.append(n_s['achieved_goal'])\n",
    "\n",
    "#         elif sampling_strategy == 'self_attention':\n",
    "#             if transition is not None:\n",
    "#                 index_of_t = experience.index(transition)\n",
    "#                 sample_experience = experience[index_of_t:]\n",
    "#                 if future is None:\n",
    "#                     future = 5\n",
    "#                 frames = self.get_frames(transition, sample_experience, k=future)\n",
    "#                 for f in frames:\n",
    "#                     g.append(f['achieved_goal'])\n",
    "\n",
    "#         elif sampling_strategy == 'future':\n",
    "#             if transition is not None and future is not None:\n",
    "#                 index_of_t = transition\n",
    "#                 if index_of_t < len(experience)-2:\n",
    "#                     sample_experience = experience[index_of_t+1:]\n",
    "#                     random_transitions = random.sample(population=sample_experience,\n",
    "#                                               k=future)\n",
    "#                     for f in random_transitions:\n",
    "#                         observation, new_observation, obs, new_obs, rew, success, act, done_bool, achieved_goal, desired_goal = f\n",
    "#                         g.append(achieved_goal)\n",
    "\n",
    "#         elif sampling_strategy == 'prioritized':\n",
    "#             pass\n",
    "\n",
    "#         return g\n",
    "\n",
    "#     def her_training(self):\n",
    "\n",
    "#         # Starting Time\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         # Initialize the statistics dictionary\n",
    "#         statistics = self.statistics\n",
    "\n",
    "#         episode_rews_history = deque(maxlen=100)\n",
    "#         episode_revised_rews_history  =  deque(maxlen=100)\n",
    "#         eval_episode_rews_history = deque(maxlen=100)\n",
    "#         episode_success_history = deque(maxlen=100)\n",
    "#         eval_episode_success_history = deque(maxlen=100)\n",
    "#         episode_goals_history = deque(maxlen=100)\n",
    "#         eval_episode_goals_history = deque(maxlen=100)\n",
    "#         all_goals_history = deque(maxlen=100)\n",
    "\n",
    "#         epoch_episode_rews = []\n",
    "#         epoch_episode_success = []\n",
    "#         epoch_episode_steps = []\n",
    "\n",
    "\n",
    "#         episode_obss_history = deque(maxlen=100)\n",
    "#         episode_new_obss_history = deque(maxlen=100)\n",
    "\n",
    "#         # rews and success for each epoch\n",
    "#         epoch_rews = []\n",
    "#         epoch_success = []\n",
    "\n",
    "#         # Sample a goal g and an initial obs s0\n",
    "#         obs = self.env.reset() # The obs space includes the observation, achieved_goal and the desired_goal\n",
    "#         observation = obs['observation']\n",
    "#         achieved_goal = obs['achieved_goal']\n",
    "#         desired_goal = obs['desired_goal']\n",
    "#         obs = np.concatenate((observation, desired_goal))\n",
    "\n",
    "#         # If eval, initialize the evaluation with an initial obs\n",
    "#         if self.eval_env is not None:\n",
    "#             eval_obs = self.eval_env.reset()\n",
    "#             eval_observation = eval_obs['observation']\n",
    "#             eval_achieved_goal = eval_obs['achieved_goal']\n",
    "#             eval_desired_goal = eval_obs['desired_goal']\n",
    "#             eval_obs = np.concatenate((eval_observation, eval_desired_goal))\n",
    "#             eval_obs = to_tensor(eval_obs, use_cuda=self.cuda)\n",
    "#             eval_obs = torch.unsqueeze(eval_obs, dim=0)\n",
    "\n",
    "#         # Initialize the losses\n",
    "#         loss = 0\n",
    "#         episode_rew = 0\n",
    "#         episode_success = 0\n",
    "#         episode_step = 0\n",
    "#         epoch_acts = []\n",
    "#         t = 0\n",
    "\n",
    "#         # Check whether to use cuda or not\n",
    "#         obs = to_tensor(obs, use_cuda=self.cuda)\n",
    "#         obs = torch.unsqueeze(obs, dim=0)\n",
    "\n",
    "#         for epoch in range(self.num_epochs):\n",
    "#             epoch_actor_losses = []\n",
    "#             epoch_critic_losses = []\n",
    "\n",
    "#             for cycle in range(self.max_episodes):\n",
    "\n",
    "#                 # States and new obss for the hindsight experience replay\n",
    "#                 episode_obss = []\n",
    "#                 episode_achieved_goals = []\n",
    "#                 episode_desired_goals = []\n",
    "#                 episode_new_obss = []\n",
    "#                 episode_rews = []\n",
    "#                 episode_successes = []\n",
    "#                 episode_acts = []\n",
    "#                 episode_dones = []\n",
    "#                 episode_experience = []\n",
    "#                 episode_observations = []\n",
    "#                 episode_new_observations = []\n",
    "\n",
    "#                 # Rollout of trajectory to fill the replay buffer before the training\n",
    "#                 for rollout in range(self.num_rollouts):\n",
    "#                     # Sample an act from behavioural policy pi\n",
    "#                     act = self.ddpg.get_act(obs=obs, noise=True)\n",
    "#                     #assert act.shape == self.env.get_act_shape\n",
    "\n",
    "#                     # Execute the act and observe the new obs\n",
    "#                     new_obs, rew, done, success = self.env.step(act)\n",
    "\n",
    "#                     # The following has to hold\n",
    "#                     assert rew == self.env.compute_rew(\n",
    "#                         new_obs['achieved_goal'], new_obs['desired_goal'],\n",
    "#                         info=success\n",
    "#                     )\n",
    "\n",
    "#                     new_observation = new_obs['observation']\n",
    "#                     new_achieved_goal = new_obs['achieved_goal']\n",
    "#                     new_desired_goal = new_obs['desired_goal']\n",
    "#                     new_obs = np.concatenate((new_observation, new_desired_goal))\n",
    "#                     new_obs = to_tensor(new_obs, self.cuda)\n",
    "#                     #new_obs = torch.unsqueeze(new_obs, dim=0)\n",
    "#                     success = success['is_success']\n",
    "#                     done_bool = done * 1\n",
    "\n",
    "#                     episode_obss.append(obs)\n",
    "#                     episode_new_obss.append(new_obs)\n",
    "#                     episode_rews.append(rew)\n",
    "#                     episode_successes.append(success)\n",
    "#                     episode_acts.append(act)\n",
    "#                     episode_dones.append(done_bool)\n",
    "#                     episode_achieved_goals.append(new_achieved_goal)\n",
    "#                     episode_desired_goals.append(new_desired_goal)\n",
    "#                     episode_observations.append(observation)\n",
    "#                     episode_new_observations.append(new_observation)\n",
    "#                     episode_experience.append(\n",
    "#                         (observation, new_observation, obs, new_obs, rew, success, act, done_bool, new_achieved_goal, desired_goal)\n",
    "#                     )\n",
    "\n",
    "#                     t += 1\n",
    "#                     episode_rew += rew\n",
    "#                     episode_step += 1\n",
    "#                     episode_success += success\n",
    "\n",
    "#                     # Set the current obs as the next obs\n",
    "#                     obs = to_tensor(new_obs, use_cuda=self.cuda)\n",
    "#                     obs = torch.unsqueeze(obs, dim=0)\n",
    "#                     observation = new_observation\n",
    "\n",
    "#                     # End of the episode\n",
    "#                     if done:\n",
    "#                         # Get the episode goal\n",
    "#                         #episode_goal = new_obs[:self.ddpg.obs_dim]\n",
    "#                         #episode_goals_history.append(episode_goal)\n",
    "#                         epoch_episode_rews.append(episode_rew)\n",
    "#                         episode_goals_history.append(achieved_goal)\n",
    "#                         episode_rews_history.append(episode_rew)\n",
    "#                         episode_success_history.append(episode_success)\n",
    "#                         epoch_episode_success.append(episode_success)\n",
    "#                         epoch_episode_steps.append(episode_step)\n",
    "#                         episode_rew = 0\n",
    "#                         episode_step = 0\n",
    "#                         episode_success = 0\n",
    "\n",
    "#                         # Reset the agent\n",
    "#                         self.ddpg.reset()\n",
    "#                         # Get a new initial obs to start from\n",
    "#                         obs = self.env.reset()\n",
    "#                         observation = obs['observation']\n",
    "#                         achieved_goal = obs['achieved_goal']\n",
    "#                         desired_goal = obs['desired_goal']\n",
    "#                         obs = np.concatenate((observation, desired_goal))\n",
    "#                         obs = to_tensor(obs, use_cuda=self.cuda)\n",
    "#                         obs = torch.unsqueeze(obs, dim=0)\n",
    "\n",
    "#                 # Standard Experience Replay\n",
    "#                 i = 0\n",
    "#                 for tr in episode_experience:\n",
    "#                     observation, new_observation, obs, new_obs, rew, success, act, done_bool, achieved_goal, desired_goal = tr\n",
    "#                     new_obs = torch.unsqueeze(new_obs, dim=0)\n",
    "#                     act = to_tensor(act, use_cuda=self.cuda)\n",
    "#                     act = torch.unsqueeze(act, dim=0)\n",
    "#                     rew = to_tensor([np.asscalar(rew)], use_cuda=self.cuda)\n",
    "#                     done_bool = to_tensor([done_bool], use_cuda=self.cuda)\n",
    "#                     #success = to_tensor([np.asscalar(success)], use_cuda=self.cuda)\n",
    "\n",
    "#                     # Store the transition in the experience replay\n",
    "#                     self.ddpg.store_transition(\n",
    "#                         obs=obs, new_obs=new_obs, rew=rew,\n",
    "#                         success=success, act=act, done=done_bool\n",
    "#                     )\n",
    "\n",
    "#                     # Hindsight Experience Replay\n",
    "#                     # Sample a set of additional goals for replay G: S\n",
    "#                     additional_goals = self.sample_goals(sampling_strategy='future',\n",
    "#                                                          experience=episode_experience,\n",
    "#                                                          future=self.future, transition=i)\n",
    "\n",
    "#                     for g in additional_goals:\n",
    "#                         # Recalculate the rew\n",
    "#                         substitute_goal = g\n",
    "\n",
    "#                         # Recalculate the rew now when the desired goal is the substituted goal\n",
    "#                         # which is the achieved goal sampled using the sampling strategy\n",
    "#                         rew_revised = self.env.compute_rew(\n",
    "#                             achieved_goal, substitute_goal, info=success\n",
    "#                         )\n",
    "#                         # Book Keeping\n",
    "#                         #episode_revised_rews_history.append(rew_revised)\n",
    "#                         # Store the transition with the new goal and rew in the replay buffer\n",
    "#                         # Get the observation and new observation from the concatenated value\n",
    "\n",
    "#                         # Currently, the env on resetting returns a concatenated vector of\n",
    "#                         # Observation and the desired goal. Therefore, we need to extract the\n",
    "#                         # Observation for this step.\n",
    "#                         observation = to_tensor(observation, use_cuda=self.cuda)\n",
    "#                         new_observation = to_tensor(new_observation, use_cuda=self.cuda)\n",
    "\n",
    "#                         g = to_tensor(g, use_cuda=self.cuda)\n",
    "#                         #rew_revised = to_tensor(rew_revised, use_cuda=self.cuda)\n",
    "#                         #print(observation)\n",
    "#                         augmented_obs = torch.cat([observation, g])\n",
    "#                         augmented_new_obs = torch.cat([new_observation, g])\n",
    "#                         augmented_obs = torch.unsqueeze(augmented_obs, dim=0)\n",
    "#                         augmented_new_obs = torch.unsqueeze(augmented_new_obs, dim=0)\n",
    "#                         rew_revised = to_tensor([np.asscalar(rew_revised)], use_cuda=self.cuda)\n",
    "\n",
    "#                         # Store the transition in the buffer\n",
    "#                         self.ddpg.store_transition(obs=augmented_obs, new_obs=augmented_new_obs,\n",
    "#                                                    act=act, done=done_bool, rew=rew_revised,\n",
    "#                                                    success=success)\n",
    "\n",
    "#                 # Train the network\n",
    "#                 for train_steps in range(self.nb_train_steps):\n",
    "#                     critic_loss, actor_loss = self.ddpg.fit_batch()\n",
    "#                     if critic_loss is not None and actor_loss is not None:\n",
    "#                         epoch_critic_losses.append(critic_loss)\n",
    "#                         epoch_actor_losses.append(actor_loss)\n",
    "\n",
    "#                     # Update the target networks using polyak averaging\n",
    "#                     self.ddpg.update_target_networks()\n",
    "\n",
    "#                 eval_episode_rews = []\n",
    "#                 eval_episode_successes = []\n",
    "#                 if self.eval_env is not None:\n",
    "#                     eval_episode_rew = 0\n",
    "#                     eval_episode_success = 0\n",
    "#                     for t_rollout in range(self.num_eval_rollouts):\n",
    "#                         if eval_obs is not None:\n",
    "#                             eval_act = self.ddpg.get_act(obs=eval_obs, noise=False)\n",
    "#                         eval_new_obs, eval_rew, eval_done, eval_success = self.eval_env.step(eval_act)\n",
    "#                         eval_episode_rew += eval_rew\n",
    "#                         eval_episode_success += eval_success['is_success']\n",
    "\n",
    "#                         if eval_done:\n",
    "#                             # Get the episode goal\n",
    "#                             #eval_episode_goal = eval_new_obs[:self.ddpg.obs_dim]\n",
    "#                             #eval_episode_goals_history.append(eval_episode_goal)\n",
    "#                             eval_obs = self.eval_env.reset()\n",
    "#                             eval_obs = to_tensor(eval_obs, use_cuda=self.cuda)\n",
    "#                             eval_obs = torch.unsqueeze(eval_obs, dim=0)\n",
    "#                             eval_episode_rews.append(eval_episode_rew)\n",
    "#                             eval_episode_rews_history.append(eval_episode_rew)\n",
    "#                             eval_episode_successes.append(eval_episode_success)\n",
    "#                             eval_episode_success_history.append(eval_episode_success)\n",
    "#                             eval_episode_rew = 0\n",
    "#                             eval_episode_success = 0\n",
    "\n",
    "#                 # Log stats\n",
    "#                 duration = time.time() - start_time\n",
    "#                 statistics['rollout/rews'] = np.mean(epoch_episode_rews)\n",
    "#                 statistics['rollout/rews_history'] = np.mean(episode_rews_history)\n",
    "#                 statistics['rollout/successes'] = np.mean(epoch_episode_success)\n",
    "#                 statistics['rollout/successes_history'] = np.mean(episode_success_history)\n",
    "#                 statistics['rollout/acts_mean'] = np.mean(epoch_acts)\n",
    "#                 statistics['rollout/goals_mean'] = np.mean(episode_goals_history)\n",
    "#                 statistics['train/loss_actor'] = np.mean(epoch_actor_losses)\n",
    "#                 statistics['train/loss_critic'] = np.mean(epoch_critic_losses)\n",
    "#                 statistics['total/duration'] = duration\n",
    "\n",
    "#                 # Evaluation statistics\n",
    "#                 if self.eval_env is not None:\n",
    "#                     statistics['eval/rews'] = np.mean(eval_episode_rews)\n",
    "#                     statistics['eval/rews_history'] = np.mean(eval_episode_rews_history)\n",
    "#                     statistics['eval/successes'] = np.mean(eval_episode_successes)\n",
    "#                     statistics['eval/success_history'] = np.mean(eval_episode_success_history)\n",
    "#                     statistics['eval/goals_history'] = np.mean(eval_episode_goals_history)\n",
    "\n",
    "#             # Print the statistics\n",
    "#             if self.verbose:\n",
    "#                 if epoch % 5 == 0:\n",
    "#                     print(epoch)\n",
    "#                     print(\"rew \", statistics['rollout/rews'])\n",
    "#                     print(\"Successes \", statistics['rollout/successes'])\n",
    "\n",
    "#                     if self.eval_env is not None:\n",
    "#                         print(\"Evaluation rew \", statistics['eval/rews'])\n",
    "#                         print(\"Evaluation Successes \", statistics['eval/successes'])\n",
    "\n",
    "#             # Log the combined statistics for all epochs\n",
    "#             for key in sorted(statistics.keys()):\n",
    "#                 self.combined_statistics[key].append(statistics[key])\n",
    "\n",
    "#             # Log the epoch rews and successes\n",
    "#             epoch_rews.append(np.mean(epoch_episode_rews))\n",
    "#             epoch_success.append(np.mean(epoch_episode_success))\n",
    "\n",
    "#         # Plot the statistics calculated\n",
    "#         if self.plot_stats:\n",
    "#             # Plot the rews and successes\n",
    "#             rews_fname = self.output_folder + '/rews.jpg'\n",
    "#             success_fname = self.output_folder + '/success.jpg'\n",
    "#             plot(epoch_rews, f_name=rews_fname, save_fig=True, show_fig=False)\n",
    "#             plot(epoch_success, f_name=success_fname, save_fig=True, show_fig=False)\n",
    "\n",
    "#         # Save the models on the disk\n",
    "#         if self.save_model:\n",
    "#             self.ddpg.save_model(self.output_folder)\n",
    "\n",
    "#         return self.combined_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
